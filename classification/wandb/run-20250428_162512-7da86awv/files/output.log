Not using distributed mode
Namespace(mode='train', batch_size=128, epochs=50, update_freq=1, save_ckpt_freq=5, model='PanDerm_Large_FT', rel_pos_bias=True, sin_pos_emb=True, layer_scale_init_value=0.1, ood_eval=False, input_size=224, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, weights=True, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, percent_data=1.0, TTA=False, monitor='recall', opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.65, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=10, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', pretrained_checkpoint='/home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth', model_key='model|module|state_dict', model_prefix='', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, data_path='/datasets01/imagenet_full_size/061417/', eval_data_path=None, test_csv_path=None, image_key='image', nb_classes=7, imagenet_default_mean_and_std=True, data_set='IMNET', csv_path='/home/syyan/XJ/PanDerm-open_source/data/linear_probing/HAM_clean.csv', root_path='/home/share/Uni_Eval/ISIC2018_reader/images/', output_dir='/home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large', log_dir=None, device='cuda', seed=0, resume='', auto_resume=False, wandb_name='weight_sampler_max_recall_mask_B128_5e-4_0', save_ckpt=True, start_epoch=0, eval=False, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, enable_linear_eval=False, enable_multi_print=False, exp_name='ham finetune and eval', distributed=False)
Label distribution:
Label 0: 273
Label 1: 448
Label 2: 941
Label 3: 102
Label 4: 1021
Label 5: 5304
Label 6: 118
Using WeightedRandomSampler
train size: 8207 ,val size: 575 ,test size: 1232
Mixup is activated!
/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=7, bias=True)
)
Patch size = (16, 16)
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained_checkpoint, map_location='cpu')
Load ckpt from /home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth
Load state_dict by model_key = model
all keys: ['encoder.cls_token', 'encoder.pos_embed', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.blocks.0.gamma_1', 'encoder.blocks.0.gamma_2', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.q_bias', 'encoder.blocks.0.attn.v_bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.gamma_1', 'encoder.blocks.1.gamma_2', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.q_bias', 'encoder.blocks.1.attn.v_bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.gamma_1', 'encoder.blocks.2.gamma_2', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.q_bias', 'encoder.blocks.2.attn.v_bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.gamma_1', 'encoder.blocks.3.gamma_2', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.q_bias', 'encoder.blocks.3.attn.v_bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.gamma_1', 'encoder.blocks.4.gamma_2', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.q_bias', 'encoder.blocks.4.attn.v_bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.gamma_1', 'encoder.blocks.5.gamma_2', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.q_bias', 'encoder.blocks.5.attn.v_bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.gamma_1', 'encoder.blocks.6.gamma_2', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.q_bias', 'encoder.blocks.6.attn.v_bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.gamma_1', 'encoder.blocks.7.gamma_2', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.q_bias', 'encoder.blocks.7.attn.v_bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.blocks.8.gamma_1', 'encoder.bl
##############new keys: 454 odict_keys(['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight', 'regresser.regressor_
Weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_bias_table', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.3.attn.relative_position_bias_table', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.6.attn.relative_position_bias_table', 'blocks.7.attn.relative_position_bias_table', 'blocks.8.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.10.attn.relative_position_bias_table', 'blocks.11.attn.relative_position_bias_table', 'blocks.12.attn.relative_position_bias_table', 'blocks.13.attn.relative_position_bias_table', 'blocks.14.attn.relative_position_bias_table', 'blocks.15.attn.relative_position_bias_table', 'blocks.16.attn.relative_position_bias_table', 'blocks.17.attn.relative_position_bias_table', 'blocks.18.attn.relative_position_bias_table', 'blocks.19.attn.relative_position_bias_table', 'blocks.20.attn.relative_position_bias_table', 'blocks.21.attn.relative_position_bias_table', 'blocks.22.attn.relative_position_bias_table', 'blocks.23.attn.relative_position_bias_table', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight',
Ignored weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_index', 'blocks.1.attn.relative_position_index', 'blocks.2.attn.relative_position_index', 'blocks.3.attn.relative_position_index', 'blocks.4.attn.relative_position_index', 'blocks.5.attn.relative_position_index', 'blocks.6.attn.relative_position_index', 'blocks.7.attn.relative_position_index', 'blocks.8.attn.relative_position_index', 'blocks.9.attn.relative_position_index', 'blocks.10.attn.relative_position_index', 'blocks.11.attn.relative_position_index', 'blocks.12.attn.relative_position_index', 'blocks.13.attn.relative_position_index', 'blocks.14.attn.relative_position_index', 'blocks.15.attn.relative_position_index', 'blocks.16.attn.relative_position_index', 'blocks.17.attn.relative_position_index', 'blocks.18.attn.relative_position_index', 'blocks.19.attn.relative_position_index', 'blocks.20.attn.relative_position_index', 'blocks.21.attn.relative_position_index', 'blocks.22.attn.relative_position_index', 'blocks.23.attn.relative_position_index']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=7, bias=True)
)
number of params: 303412743
LR = 0.00050000
Batch size = 128
Update frequent = 1
Number of training examples = 8207
Number of training training per epoch = 64
Assigned values = [2.1029740616282293e-05, 3.2353447101972754e-05, 4.977453400303501e-05, 7.65762061585154e-05, 0.00011780954793617752, 0.00018124545836335003, 0.0002788391667128462, 0.0004289833334043787, 0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "patch_embed.proj.bias"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.gamma_1",
      "blocks.0.gamma_2",
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.relative_position_bias_table",
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.gamma_1",
      "blocks.1.gamma_2",
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.relative_position_bias_table",
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.gamma_1",
      "blocks.2.gamma_2",
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.relative_position_bias_table",
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.gamma_1",
      "blocks.3.gamma_2",
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.relative_position_bias_table",
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.gamma_1",
      "blocks.4.gamma_2",
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.relative_position_bias_table",
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.gamma_1",
      "blocks.5.gamma_2",
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.relative_position_bias_table",
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.gamma_1",
      "blocks.6.gamma_2",
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.relative_position_bias_table",
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.gamma_1",
      "blocks.7.gamma_2",
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.relative_position_bias_table",
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.gamma_1",
      "blocks.8.gamma_2",
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.relative_position_bias_table",
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.gamma_1",
      "blocks.9.gamma_2",
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.relative_position_bias_table",
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.gamma_1",
      "blocks.10.gamma_2",
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.relative_position_bias_table",
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.gamma_1",
      "blocks.11.gamma_2",
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.relative_position_bias_table",
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.12.gamma_1",
      "blocks.12.gamma_2",
      "blocks.12.norm1.weight",
      "blocks.12.norm1.bias",
      "blocks.12.attn.q_bias",
      "blocks.12.attn.v_bias",
      "blocks.12.attn.proj.bias",
      "blocks.12.norm2.weight",
      "blocks.12.norm2.bias",
      "blocks.12.mlp.fc1.bias",
      "blocks.12.mlp.fc2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.12.attn.relative_position_bias_table",
      "blocks.12.attn.qkv.weight",
      "blocks.12.attn.proj.weight",
      "blocks.12.mlp.fc1.weight",
      "blocks.12.mlp.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_14_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.13.gamma_1",
      "blocks.13.gamma_2",
      "blocks.13.norm1.weight",
      "blocks.13.norm1.bias",
      "blocks.13.attn.q_bias",
      "blocks.13.attn.v_bias",
      "blocks.13.attn.proj.bias",
      "blocks.13.norm2.weight",
      "blocks.13.norm2.bias",
      "blocks.13.mlp.fc1.bias",
      "blocks.13.mlp.fc2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_14_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.13.attn.relative_position_bias_table",
      "blocks.13.attn.qkv.weight",
      "blocks.13.attn.proj.weight",
      "blocks.13.mlp.fc1.weight",
      "blocks.13.mlp.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_15_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.14.gamma_1",
      "blocks.14.gamma_2",
      "blocks.14.norm1.weight",
      "blocks.14.norm1.bias",
      "blocks.14.attn.q_bias",
      "blocks.14.attn.v_bias",
      "blocks.14.attn.proj.bias",
      "blocks.14.norm2.weight",
      "blocks.14.norm2.bias",
      "blocks.14.mlp.fc1.bias",
      "blocks.14.mlp.fc2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_15_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.14.attn.relative_position_bias_table",
      "blocks.14.attn.qkv.weight",
      "blocks.14.attn.proj.weight",
      "blocks.14.mlp.fc1.weight",
      "blocks.14.mlp.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_16_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.15.gamma_1",
      "blocks.15.gamma_2",
      "blocks.15.norm1.weight",
      "blocks.15.norm1.bias",
      "blocks.15.attn.q_bias",
      "blocks.15.attn.v_bias",
      "blocks.15.attn.proj.bias",
      "blocks.15.norm2.weight",
      "blocks.15.norm2.bias",
      "blocks.15.mlp.fc1.bias",
      "blocks.15.mlp.fc2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_16_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.15.attn.relative_position_bias_table",
      "blocks.15.attn.qkv.weight",
      "blocks.15.attn.proj.weight",
      "blocks.15.mlp.fc1.weight",
      "blocks.15.mlp.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.16.gamma_1",
      "blocks.16.gamma_2",
      "blocks.16.norm1.weight",
      "blocks.16.norm1.bias",
      "blocks.16.attn.q_bias",
      "blocks.16.attn.v_bias",
      "blocks.16.attn.proj.bias",
      "blocks.16.norm2.weight",
      "blocks.16.norm2.bias",
      "blocks.16.mlp.fc1.bias",
      "blocks.16.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.16.attn.relative_position_bias_table",
      "blocks.16.attn.qkv.weight",
      "blocks.16.attn.proj.weight",
      "blocks.16.mlp.fc1.weight",
      "blocks.16.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_18_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.17.gamma_1",
      "blocks.17.gamma_2",
      "blocks.17.norm1.weight",
      "blocks.17.norm1.bias",
      "blocks.17.attn.q_bias",
      "blocks.17.attn.v_bias",
      "blocks.17.attn.proj.bias",
      "blocks.17.norm2.weight",
      "blocks.17.norm2.bias",
      "blocks.17.mlp.fc1.bias",
      "blocks.17.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_18_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.17.attn.relative_position_bias_table",
      "blocks.17.attn.qkv.weight",
      "blocks.17.attn.proj.weight",
      "blocks.17.mlp.fc1.weight",
      "blocks.17.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_19_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.18.gamma_1",
      "blocks.18.gamma_2",
      "blocks.18.norm1.weight",
      "blocks.18.norm1.bias",
      "blocks.18.attn.q_bias",
      "blocks.18.attn.v_bias",
      "blocks.18.attn.proj.bias",
      "blocks.18.norm2.weight",
      "blocks.18.norm2.bias",
      "blocks.18.mlp.fc1.bias",
      "blocks.18.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_19_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.18.attn.relative_position_bias_table",
      "blocks.18.attn.qkv.weight",
      "blocks.18.attn.proj.weight",
      "blocks.18.mlp.fc1.weight",
      "blocks.18.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_20_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.19.gamma_1",
      "blocks.19.gamma_2",
      "blocks.19.norm1.weight",
      "blocks.19.norm1.bias",
      "blocks.19.attn.q_bias",
      "blocks.19.attn.v_bias",
      "blocks.19.attn.proj.bias",
      "blocks.19.norm2.weight",
      "blocks.19.norm2.bias",
      "blocks.19.mlp.fc1.bias",
      "blocks.19.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_20_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.19.attn.relative_position_bias_table",
      "blocks.19.attn.qkv.weight",
      "blocks.19.attn.proj.weight",
      "blocks.19.mlp.fc1.weight",
      "blocks.19.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_21_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.20.gamma_1",
      "blocks.20.gamma_2",
      "blocks.20.norm1.weight",
      "blocks.20.norm1.bias",
      "blocks.20.attn.q_bias",
      "blocks.20.attn.v_bias",
      "blocks.20.attn.proj.bias",
      "blocks.20.norm2.weight",
      "blocks.20.norm2.bias",
      "blocks.20.mlp.fc1.bias",
      "blocks.20.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_21_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.20.attn.relative_position_bias_table",
      "blocks.20.attn.qkv.weight",
      "blocks.20.attn.proj.weight",
      "blocks.20.mlp.fc1.weight",
      "blocks.20.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_22_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.21.gamma_1",
      "blocks.21.gamma_2",
      "blocks.21.norm1.weight",
      "blocks.21.norm1.bias",
      "blocks.21.attn.q_bias",
      "blocks.21.attn.v_bias",
      "blocks.21.attn.proj.bias",
      "blocks.21.norm2.weight",
      "blocks.21.norm2.bias",
      "blocks.21.mlp.fc1.bias",
      "blocks.21.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_22_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.21.attn.relative_position_bias_table",
      "blocks.21.attn.qkv.weight",
      "blocks.21.attn.proj.weight",
      "blocks.21.mlp.fc1.weight",
      "blocks.21.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_23_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.22.gamma_1",
      "blocks.22.gamma_2",
      "blocks.22.norm1.weight",
      "blocks.22.norm1.bias",
      "blocks.22.attn.q_bias",
      "blocks.22.attn.v_bias",
      "blocks.22.attn.proj.bias",
      "blocks.22.norm2.weight",
      "blocks.22.norm2.bias",
      "blocks.22.mlp.fc1.bias",
      "blocks.22.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_23_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.22.attn.relative_position_bias_table",
      "blocks.22.attn.qkv.weight",
      "blocks.22.attn.proj.weight",
      "blocks.22.mlp.fc1.weight",
      "blocks.22.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_24_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.23.gamma_1",
      "blocks.23.gamma_2",
      "blocks.23.norm1.weight",
      "blocks.23.norm1.bias",
      "blocks.23.attn.q_bias",
      "blocks.23.attn.v_bias",
      "blocks.23.attn.proj.bias",
      "blocks.23.norm2.weight",
      "blocks.23.norm2.bias",
      "blocks.23.mlp.fc1.bias",
      "blocks.23.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_24_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.23.attn.relative_position_bias_table",
      "blocks.23.attn.qkv.weight",
      "blocks.23.attn.proj.weight",
      "blocks.23.mlp.fc1.weight",
      "blocks.23.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_25_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_25_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
/home/share/FM_Code/PanDerm/classification/furnace/utils.py:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
Use step level LR scheduler!
Set warmup steps = 640
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 50 epochs
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [0]  [ 0/64]  eta: 0:06:17  lr: 0.000000  min_lr: 0.000000  loss: 1.9459 (1.9459)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6760 (0.6760)  time: 5.9060  data: 3.2645  max mem: 37088
Epoch: [0]  [10/64]  eta: 0:01:05  lr: 0.000008  min_lr: 0.000000  loss: 1.9459 (1.9458)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7805 (0.7922)  time: 1.2131  data: 0.2971  max mem: 39406
Epoch: [0]  [20/64]  eta: 0:00:43  lr: 0.000016  min_lr: 0.000000  loss: 1.9457 (1.9456)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7713 (0.7986)  time: 0.7482  data: 0.0003  max mem: 39406
Epoch: [0]  [30/64]  eta: 0:00:31  lr: 0.000023  min_lr: 0.000000  loss: 1.9446 (1.9449)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7959 (0.8444)  time: 0.7568  data: 0.0004  max mem: 39406
Epoch: [0]  [40/64]  eta: 0:00:21  lr: 0.000031  min_lr: 0.000000  loss: 1.9409 (1.9431)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1337 (0.9677)  time: 0.7652  data: 0.0004  max mem: 39406
Epoch: [0]  [50/64]  eta: 0:00:12  lr: 0.000039  min_lr: 0.000000  loss: 1.9347 (1.9395)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4744 (1.0852)  time: 0.7733  data: 0.0004  max mem: 39406
Epoch: [0]  [60/64]  eta: 0:00:03  lr: 0.000047  min_lr: 0.000000  loss: 1.9150 (1.9318)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6831 (1.2156)  time: 0.7810  data: 0.0002  max mem: 39406
Epoch: [0]  [63/64]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000000  loss: 1.9044 (1.9283)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8684 (1.2474)  time: 0.7829  data: 0.0002  max mem: 39406
Epoch: [0] Total time: 0:00:54 (0.8488 s / it)
2025-04-28 16:26:15 Averaged stats: lr: 0.000049  min_lr: 0.000000  loss: 1.9044 (1.9283)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8684 (1.2474)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:11    time: 3.6768  data: 2.8332  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.5297  data: 0.9445  max mem: 39406
Test: Total time: 0:00:04 (1.5795 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.4415 Acc: 0.4226 Recall_macro: 0.4415 Recall_weighted: 0.4226 AUC-ROC: 0.8942 Weighted F1-score: 0.5214
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 0, 'Val Loss': 1.8415054082870483, 'Val BAcc': np.float64(0.4414703425229741), 'Val Acc': 0.4226086956521739, 'Val ROC': np.float64(0.8941823265228361), 'Val W_F1': 0.5213601136543495, 'Val Recall_macro': 0.4414703425229741, 'Val Recall_weighted': 0.4226086956521739}
Max val mean recall: 0.44%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [1]  [ 0/64]  eta: 0:04:50  lr: 0.000050  min_lr: 0.000000  loss: 1.8698 (1.8698)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3780 (1.3780)  time: 4.5447  data: 3.7574  max mem: 39406
Epoch: [1]  [10/64]  eta: 0:01:01  lr: 0.000058  min_lr: 0.000000  loss: 1.8183 (1.8165)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8829 (1.9426)  time: 1.1306  data: 0.3420  max mem: 39406
Epoch: [1]  [20/64]  eta: 0:00:42  lr: 0.000066  min_lr: 0.000000  loss: 1.7988 (1.7954)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8559 (1.8339)  time: 0.7925  data: 0.0005  max mem: 39406
Epoch: [1]  [30/64]  eta: 0:00:31  lr: 0.000074  min_lr: 0.000000  loss: 1.7629 (1.7757)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7768 (1.8571)  time: 0.7988  data: 0.0004  max mem: 39406
Epoch: [1]  [40/64]  eta: 0:00:21  lr: 0.000081  min_lr: 0.000000  loss: 1.7550 (1.7638)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9295 (1.8696)  time: 0.8064  data: 0.0005  max mem: 39406
Epoch: [1]  [50/64]  eta: 0:00:12  lr: 0.000089  min_lr: 0.000000  loss: 1.7148 (1.7404)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8381 (1.8504)  time: 0.8139  data: 0.0004  max mem: 39406
Epoch: [1]  [60/64]  eta: 0:00:03  lr: 0.000097  min_lr: 0.000000  loss: 1.5946 (1.7174)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6957 (1.8387)  time: 0.8180  data: 0.0002  max mem: 39406
Epoch: [1]  [63/64]  eta: 0:00:00  lr: 0.000099  min_lr: 0.000000  loss: 1.5890 (1.7115)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7190 (1.8393)  time: 0.8189  data: 0.0002  max mem: 39406
Epoch: [1] Total time: 0:00:55 (0.8679 s / it)
2025-04-28 16:27:20 Averaged stats: lr: 0.000099  min_lr: 0.000000  loss: 1.5890 (1.7115)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7190 (1.8393)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.2966  data: 2.9055  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3680  data: 0.9686  max mem: 39406
Test: Total time: 0:00:04 (1.4241 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6644 Acc: 0.7722 Recall_macro: 0.6644 Recall_weighted: 0.7722 AUC-ROC: 0.9362 Weighted F1-score: 0.8034
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 1, 'Val Loss': 1.1063532829284668, 'Val BAcc': np.float64(0.6643502696134275), 'Val Acc': 0.7721739130434783, 'Val ROC': np.float64(0.9361559334929618), 'Val W_F1': 0.8034202743729845, 'Val Recall_macro': 0.6643502696134275, 'Val Recall_weighted': 0.7721739130434783}
Max val mean recall: 0.66%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [2]  [ 0/64]  eta: 0:04:08  lr: 0.000100  min_lr: 0.000000  loss: 1.4855 (1.4855)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5782 (1.5782)  time: 3.8858  data: 3.0877  max mem: 39406
Epoch: [2]  [10/64]  eta: 0:00:58  lr: 0.000108  min_lr: 0.000000  loss: 1.4980 (1.5470)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0499 (1.9713)  time: 1.0858  data: 0.2811  max mem: 39406
Epoch: [2]  [20/64]  eta: 0:00:42  lr: 0.000116  min_lr: 0.000000  loss: 1.4980 (1.5134)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0720 (2.1386)  time: 0.8101  data: 0.0005  max mem: 39406
Epoch: [2]  [30/64]  eta: 0:00:31  lr: 0.000124  min_lr: 0.000000  loss: 1.5299 (1.5201)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2189 (2.2356)  time: 0.8170  data: 0.0005  max mem: 39406
Epoch: [2]  [40/64]  eta: 0:00:21  lr: 0.000131  min_lr: 0.000000  loss: 1.5299 (1.5229)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2173 (2.2010)  time: 0.8217  data: 0.0005  max mem: 39406
Epoch: [2]  [50/64]  eta: 0:00:12  lr: 0.000139  min_lr: 0.000000  loss: 1.4931 (1.4992)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2453 (2.2204)  time: 0.8265  data: 0.0004  max mem: 39406
Epoch: [2]  [60/64]  eta: 0:00:03  lr: 0.000147  min_lr: 0.000000  loss: 1.4147 (1.4974)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1488 (2.1926)  time: 0.8310  data: 0.0003  max mem: 39406
Epoch: [2]  [63/64]  eta: 0:00:00  lr: 0.000149  min_lr: 0.000000  loss: 1.4140 (1.4943)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0609 (2.2015)  time: 0.8320  data: 0.0002  max mem: 39406
Epoch: [2] Total time: 0:00:55 (0.8720 s / it)
2025-04-28 16:28:26 Averaged stats: lr: 0.000149  min_lr: 0.000000  loss: 1.4140 (1.4943)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0609 (2.2015)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.2047  data: 2.8074  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3426  data: 0.9360  max mem: 39406
Test: Total time: 0:00:04 (1.3956 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7513 Acc: 0.7635 Recall_macro: 0.7513 Recall_weighted: 0.7635 AUC-ROC: 0.9564 Weighted F1-score: 0.8028
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 2, 'Val Loss': 0.9951338171958923, 'Val BAcc': np.float64(0.7512766765398344), 'Val Acc': 0.7634782608695653, 'Val ROC': np.float64(0.9564286132797655), 'Val W_F1': 0.802836242479418, 'Val Recall_macro': 0.7512766765398344, 'Val Recall_weighted': 0.7634782608695653}
Max val mean recall: 0.75%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [3]  [ 0/64]  eta: 0:04:30  lr: 0.000150  min_lr: 0.000000  loss: 1.4253 (1.4253)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9382 (1.9382)  time: 4.2295  data: 3.4062  max mem: 39406
Epoch: [3]  [10/64]  eta: 0:01:00  lr: 0.000158  min_lr: 0.000000  loss: 1.3311 (1.3307)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5034 (2.5085)  time: 1.1255  data: 0.3102  max mem: 39406
Epoch: [3]  [20/64]  eta: 0:00:43  lr: 0.000166  min_lr: 0.000000  loss: 1.3660 (1.3534)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5241 (2.6030)  time: 0.8184  data: 0.0006  max mem: 39406
Epoch: [3]  [30/64]  eta: 0:00:31  lr: 0.000174  min_lr: 0.000000  loss: 1.3720 (1.3607)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2830 (2.4868)  time: 0.8237  data: 0.0006  max mem: 39406
Epoch: [3]  [40/64]  eta: 0:00:21  lr: 0.000182  min_lr: 0.000000  loss: 1.3720 (1.3742)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3172 (2.5404)  time: 0.8288  data: 0.0006  max mem: 39406
Epoch: [3]  [50/64]  eta: 0:00:12  lr: 0.000189  min_lr: 0.000000  loss: 1.4353 (1.3841)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4910 (2.5195)  time: 0.8330  data: 0.0005  max mem: 39406
Epoch: [3]  [60/64]  eta: 0:00:03  lr: 0.000197  min_lr: 0.000000  loss: 1.3798 (1.3737)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2686 (2.5142)  time: 0.8370  data: 0.0003  max mem: 39406
Epoch: [3]  [63/64]  eta: 0:00:00  lr: 0.000200  min_lr: 0.000000  loss: 1.2753 (1.3617)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2686 (2.5019)  time: 0.8376  data: 0.0002  max mem: 39406
Epoch: [3] Total time: 0:00:56 (0.8849 s / it)
2025-04-28 16:29:34 Averaged stats: lr: 0.000200  min_lr: 0.000000  loss: 1.2753 (1.3617)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2686 (2.5019)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.3545  data: 2.9632  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3955  data: 0.9879  max mem: 39406
Test: Total time: 0:00:04 (1.4496 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7661 Acc: 0.8261 Recall_macro: 0.7661 Recall_weighted: 0.8261 AUC-ROC: 0.9754 Weighted F1-score: 0.8463
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 3, 'Val Loss': 0.6381650567054749, 'Val BAcc': np.float64(0.7660575681628314), 'Val Acc': 0.8260869565217391, 'Val ROC': np.float64(0.9754150818074375), 'Val W_F1': 0.8462757603865146, 'Val Recall_macro': 0.7660575681628314, 'Val Recall_weighted': 0.8260869565217391}
Max val mean recall: 0.77%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [4]  [ 0/64]  eta: 0:03:56  lr: 0.000200  min_lr: 0.000000  loss: 0.9188 (0.9188)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6069 (2.6069)  time: 3.6926  data: 2.8909  max mem: 39406
Epoch: [4]  [10/64]  eta: 0:00:58  lr: 0.000208  min_lr: 0.000000  loss: 1.2183 (1.2517)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5299 (2.5250)  time: 1.0812  data: 0.2634  max mem: 39406
Epoch: [4]  [20/64]  eta: 0:00:42  lr: 0.000216  min_lr: 0.000000  loss: 1.3829 (1.3780)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5299 (2.5649)  time: 0.8234  data: 0.0007  max mem: 39406
Epoch: [4]  [30/64]  eta: 0:00:31  lr: 0.000224  min_lr: 0.000000  loss: 1.5249 (1.3727)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2931 (2.5221)  time: 0.8285  data: 0.0006  max mem: 39406
Epoch: [4]  [40/64]  eta: 0:00:21  lr: 0.000232  min_lr: 0.000000  loss: 1.1860 (1.3277)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4270 (2.5933)  time: 0.8324  data: 0.0007  max mem: 39406
Epoch: [4]  [50/64]  eta: 0:00:12  lr: 0.000239  min_lr: 0.000000  loss: 1.1860 (1.3310)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6265 (2.6102)  time: 0.8366  data: 0.0006  max mem: 39406
Epoch: [4]  [60/64]  eta: 0:00:03  lr: 0.000247  min_lr: 0.000000  loss: 1.4866 (1.3551)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4458 (2.5636)  time: 0.8391  data: 0.0003  max mem: 39406
Epoch: [4]  [63/64]  eta: 0:00:00  lr: 0.000250  min_lr: 0.000000  loss: 1.4866 (1.3545)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2755 (2.5272)  time: 0.8394  data: 0.0002  max mem: 39406
Epoch: [4] Total time: 0:00:56 (0.8798 s / it)
2025-04-28 16:30:41 Averaged stats: lr: 0.000250  min_lr: 0.000000  loss: 1.4866 (1.3545)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2755 (2.5272)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.5224  data: 3.1192  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4521  data: 1.0399  max mem: 39406
Test: Total time: 0:00:04 (1.5136 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8198 Acc: 0.8330 Recall_macro: 0.8198 Recall_weighted: 0.8330 AUC-ROC: 0.9780 Weighted F1-score: 0.8566
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 4, 'Val Loss': 0.7512079477310181, 'Val BAcc': np.float64(0.8197630439735702), 'Val Acc': 0.8330434782608696, 'Val ROC': np.float64(0.9780435557347734), 'Val W_F1': 0.856569617958779, 'Val Recall_macro': 0.8197630439735702, 'Val Recall_weighted': 0.8330434782608696}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [5]  [ 0/64]  eta: 0:04:00  lr: 0.000250  min_lr: 0.000000  loss: 1.6791 (1.6791)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5784 (2.5784)  time: 3.7503  data: 2.9364  max mem: 39406
Epoch: [5]  [10/64]  eta: 0:00:58  lr: 0.000258  min_lr: 0.000000  loss: 1.4919 (1.3712)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4024 (2.3946)  time: 1.0853  data: 0.2674  max mem: 39406
Epoch: [5]  [20/64]  eta: 0:00:42  lr: 0.000266  min_lr: 0.000000  loss: 1.4144 (1.3936)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4822 (2.6360)  time: 0.8218  data: 0.0005  max mem: 39406
Epoch: [5]  [30/64]  eta: 0:00:31  lr: 0.000274  min_lr: 0.000000  loss: 1.4751 (1.3873)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4822 (2.5189)  time: 0.8275  data: 0.0005  max mem: 39406
Epoch: [5]  [40/64]  eta: 0:00:21  lr: 0.000282  min_lr: 0.000000  loss: 1.3708 (1.3715)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2462 (2.4957)  time: 0.8501  data: 0.0005  max mem: 39406
Epoch: [5]  [50/64]  eta: 0:00:12  lr: 0.000290  min_lr: 0.000000  loss: 1.3688 (1.3527)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1806 (2.4620)  time: 0.8542  data: 0.0004  max mem: 39406
Epoch: [5]  [60/64]  eta: 0:00:03  lr: 0.000297  min_lr: 0.000000  loss: 1.3024 (1.3426)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3923 (2.4967)  time: 0.8400  data: 0.0002  max mem: 39406
Epoch: [5]  [63/64]  eta: 0:00:00  lr: 0.000300  min_lr: 0.000000  loss: 1.2830 (1.3345)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4701 (2.5037)  time: 0.8408  data: 0.0001  max mem: 39406
Epoch: [5] Total time: 0:00:56 (0.8860 s / it)
2025-04-28 16:31:49 Averaged stats: lr: 0.000300  min_lr: 0.000000  loss: 1.2830 (1.3345)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4701 (2.5037)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.3208  data: 2.9164  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3848  data: 0.9723  max mem: 39406
Test: Total time: 0:00:04 (1.4434 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8039 Acc: 0.7913 Recall_macro: 0.8039 Recall_weighted: 0.7913 AUC-ROC: 0.9749 Weighted F1-score: 0.8307
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 5, 'Val Loss': 0.7397013306617737, 'Val BAcc': np.float64(0.803916284217036), 'Val Acc': 0.7913043478260869, 'Val ROC': np.float64(0.9748739308432753), 'Val W_F1': 0.830680019686223, 'Val Recall_macro': 0.803916284217036, 'Val Recall_weighted': 0.7913043478260869}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [6]  [ 0/64]  eta: 0:04:07  lr: 0.000300  min_lr: 0.000000  loss: 0.9464 (0.9464)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5696 (2.5696)  time: 3.8635  data: 3.0404  max mem: 39406
Epoch: [6]  [10/64]  eta: 0:00:59  lr: 0.000308  min_lr: 0.000000  loss: 1.3741 (1.3258)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5743 (2.6933)  time: 1.1069  data: 0.2768  max mem: 39406
Epoch: [6]  [20/64]  eta: 0:00:42  lr: 0.000316  min_lr: 0.000000  loss: 1.3862 (1.3676)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5617 (2.6179)  time: 0.8329  data: 0.0005  max mem: 39406
Epoch: [6]  [30/64]  eta: 0:00:31  lr: 0.000324  min_lr: 0.000000  loss: 1.4264 (1.3774)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5361 (2.6263)  time: 0.8361  data: 0.0005  max mem: 39406
Epoch: [6]  [40/64]  eta: 0:00:21  lr: 0.000332  min_lr: 0.000000  loss: 1.3909 (1.3520)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5044 (2.6117)  time: 0.8390  data: 0.0005  max mem: 39406
Epoch: [6]  [50/64]  eta: 0:00:12  lr: 0.000340  min_lr: 0.000000  loss: 1.4309 (1.3715)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3122 (2.5415)  time: 0.8412  data: 0.0004  max mem: 39406
Epoch: [6]  [60/64]  eta: 0:00:03  lr: 0.000347  min_lr: 0.000000  loss: 1.4306 (1.3669)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2667 (2.5222)  time: 0.8428  data: 0.0002  max mem: 39406
Epoch: [6]  [63/64]  eta: 0:00:00  lr: 0.000350  min_lr: 0.000000  loss: 1.3654 (1.3587)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2433 (2.5299)  time: 0.8434  data: 0.0001  max mem: 39406
Epoch: [6] Total time: 0:00:56 (0.8889 s / it)
2025-04-28 16:32:50 Averaged stats: lr: 0.000350  min_lr: 0.000000  loss: 1.3654 (1.3587)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2433 (2.5299)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4198  data: 3.0231  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4195  data: 1.0078  max mem: 39406
Test: Total time: 0:00:04 (1.4799 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7911 Acc: 0.8383 Recall_macro: 0.7911 Recall_weighted: 0.8383 AUC-ROC: 0.9750 Weighted F1-score: 0.8684
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 6, 'Val Loss': 0.5861218571662903, 'Val BAcc': np.float64(0.7910887608631969), 'Val Acc': 0.8382608695652174, 'Val ROC': np.float64(0.9749858396244713), 'Val W_F1': 0.868410680226556, 'Val Recall_macro': 0.7910887608631969, 'Val Recall_weighted': 0.8382608695652174}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [7]  [ 0/64]  eta: 0:04:14  lr: 0.000351  min_lr: 0.000000  loss: 1.3419 (1.3419)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8893 (1.8893)  time: 3.9829  data: 3.1603  max mem: 39406
Epoch: [7]  [10/64]  eta: 0:01:00  lr: 0.000358  min_lr: 0.000000  loss: 1.1613 (1.1813)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6208 (2.5275)  time: 1.1182  data: 0.2877  max mem: 39406
Epoch: [7]  [20/64]  eta: 0:00:43  lr: 0.000366  min_lr: 0.000000  loss: 1.1059 (1.1557)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4020 (2.4779)  time: 0.8336  data: 0.0005  max mem: 39406
Epoch: [7]  [30/64]  eta: 0:00:31  lr: 0.000374  min_lr: 0.000000  loss: 1.2187 (1.1849)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5624 (2.6039)  time: 0.8372  data: 0.0005  max mem: 39406
Epoch: [7]  [40/64]  eta: 0:00:21  lr: 0.000382  min_lr: 0.000000  loss: 1.2570 (1.2350)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7247 (2.6268)  time: 0.8391  data: 0.0005  max mem: 39406
Epoch: [7]  [50/64]  eta: 0:00:12  lr: 0.000390  min_lr: 0.000000  loss: 1.2427 (1.2400)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3089 (2.5584)  time: 0.8411  data: 0.0004  max mem: 39406
Epoch: [7]  [60/64]  eta: 0:00:03  lr: 0.000397  min_lr: 0.000000  loss: 1.3517 (1.2627)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3546 (2.5625)  time: 0.8431  data: 0.0002  max mem: 39406
Epoch: [7]  [63/64]  eta: 0:00:00  lr: 0.000400  min_lr: 0.000000  loss: 1.3538 (1.2718)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3384 (2.5460)  time: 0.8435  data: 0.0001  max mem: 39406
Epoch: [7] Total time: 0:00:57 (0.8910 s / it)
2025-04-28 16:33:52 Averaged stats: lr: 0.000400  min_lr: 0.000000  loss: 1.3538 (1.2718)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3384 (2.5460)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.3837  data: 2.9816  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4075  data: 0.9940  max mem: 39406
Test: Total time: 0:00:04 (1.4644 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7500 Acc: 0.8435 Recall_macro: 0.7500 Recall_weighted: 0.8435 AUC-ROC: 0.9800 Weighted F1-score: 0.8678
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 7, 'Val Loss': 0.6460740566253662, 'Val BAcc': np.float64(0.7499768902776421), 'Val Acc': 0.8434782608695652, 'Val ROC': np.float64(0.9800380635844107), 'Val W_F1': 0.8677917351243931, 'Val Recall_macro': 0.7499768902776421, 'Val Recall_weighted': 0.8434782608695652}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [8]  [ 0/64]  eta: 0:04:31  lr: 0.000401  min_lr: 0.000000  loss: 1.5029 (1.5029)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3960 (2.3960)  time: 4.2349  data: 3.4114  max mem: 39406
Epoch: [8]  [10/64]  eta: 0:01:01  lr: 0.000408  min_lr: 0.000000  loss: 1.3554 (1.2457)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4333 (2.3762)  time: 1.1435  data: 0.3108  max mem: 39406
Epoch: [8]  [20/64]  eta: 0:00:43  lr: 0.000416  min_lr: 0.000000  loss: 1.3620 (1.2638)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6698 (2.6757)  time: 0.8351  data: 0.0006  max mem: 39406
Epoch: [8]  [30/64]  eta: 0:00:32  lr: 0.000424  min_lr: 0.000000  loss: 1.3477 (1.2543)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7277 (2.6225)  time: 0.8363  data: 0.0005  max mem: 39406
Epoch: [8]  [40/64]  eta: 0:00:22  lr: 0.000432  min_lr: 0.000000  loss: 1.3317 (1.2647)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4283 (2.5504)  time: 0.8382  data: 0.0005  max mem: 39406
Epoch: [8]  [50/64]  eta: 0:00:12  lr: 0.000440  min_lr: 0.000000  loss: 1.4237 (1.2854)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4353 (2.5993)  time: 0.8408  data: 0.0004  max mem: 39406
Epoch: [8]  [60/64]  eta: 0:00:03  lr: 0.000448  min_lr: 0.000000  loss: 1.4237 (1.2916)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4886 (2.5908)  time: 0.8426  data: 0.0002  max mem: 39406
Epoch: [8]  [63/64]  eta: 0:00:00  lr: 0.000450  min_lr: 0.000000  loss: 1.3804 (1.2872)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5601 (2.6015)  time: 0.8433  data: 0.0002  max mem: 39406
Epoch: [8] Total time: 0:00:57 (0.8951 s / it)
2025-04-28 16:34:54 Averaged stats: lr: 0.000450  min_lr: 0.000000  loss: 1.3804 (1.2872)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5601 (2.6015)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.6254  data: 3.2233  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4866  data: 1.0746  max mem: 39406
Test: Total time: 0:00:04 (1.5486 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8044 Acc: 0.8330 Recall_macro: 0.8044 Recall_weighted: 0.8330 AUC-ROC: 0.9859 Weighted F1-score: 0.8634
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 8, 'Val Loss': 0.5747830867767334, 'Val BAcc': np.float64(0.8044205752476429), 'Val Acc': 0.8330434782608696, 'Val ROC': np.float64(0.9859167990623041), 'Val W_F1': 0.8633880682199089, 'Val Recall_macro': 0.8044205752476429, 'Val Recall_weighted': 0.8330434782608696}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [9]  [ 0/64]  eta: 0:04:14  lr: 0.000451  min_lr: 0.000000  loss: 1.6193 (1.6193)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4856 (2.4856)  time: 3.9832  data: 3.1610  max mem: 39406
Epoch: [9]  [10/64]  eta: 0:01:00  lr: 0.000459  min_lr: 0.000000  loss: 1.4106 (1.3521)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4856 (2.4807)  time: 1.1182  data: 0.2880  max mem: 39406
Epoch: [9]  [20/64]  eta: 0:00:43  lr: 0.000466  min_lr: 0.000000  loss: 1.4106 (1.3407)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5414 (2.5525)  time: 0.8339  data: 0.0007  max mem: 39406
Epoch: [9]  [30/64]  eta: 0:00:31  lr: 0.000474  min_lr: 0.000000  loss: 1.2731 (1.2954)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6750 (2.5261)  time: 0.8373  data: 0.0006  max mem: 39406
Epoch: [9]  [40/64]  eta: 0:00:21  lr: 0.000482  min_lr: 0.000000  loss: 1.2207 (1.2888)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6993 (2.5917)  time: 0.8390  data: 0.0006  max mem: 39406
Epoch: [9]  [50/64]  eta: 0:00:12  lr: 0.000490  min_lr: 0.000000  loss: 1.2773 (1.2948)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3864 (2.5393)  time: 0.8402  data: 0.0004  max mem: 39406
Epoch: [9]  [60/64]  eta: 0:00:03  lr: 0.000498  min_lr: 0.000000  loss: 1.2650 (1.2843)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0972 (2.4634)  time: 0.8423  data: 0.0002  max mem: 39406
Epoch: [9]  [63/64]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000000  loss: 1.2650 (1.2789)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0599 (2.4615)  time: 0.8428  data: 0.0002  max mem: 39406
Epoch: [9] Total time: 0:00:57 (0.8909 s / it)
2025-04-28 16:35:55 Averaged stats: lr: 0.000500  min_lr: 0.000000  loss: 1.2650 (1.2789)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0599 (2.4615)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4635  data: 3.0620  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4333  data: 1.0208  max mem: 39406
Test: Total time: 0:00:04 (1.4895 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7850 Acc: 0.8765 Recall_macro: 0.7850 Recall_weighted: 0.8765 AUC-ROC: 0.9886 Weighted F1-score: 0.8946
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 9, 'Val Loss': 0.4595097303390503, 'Val BAcc': np.float64(0.7850418253425772), 'Val Acc': 0.8765217391304347, 'Val ROC': np.float64(0.9885766131119983), 'Val W_F1': 0.8945992997027777, 'Val Recall_macro': 0.7850418253425772, 'Val Recall_weighted': 0.8765217391304347}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [10]  [ 0/64]  eta: 0:04:41  lr: 0.000500  min_lr: 0.000000  loss: 0.7583 (0.7583)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5446 (2.5446)  time: 4.4055  data: 3.5821  max mem: 39406
Epoch: [10]  [10/64]  eta: 0:01:02  lr: 0.000500  min_lr: 0.000000  loss: 1.1262 (1.1607)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5446 (2.4318)  time: 1.1563  data: 0.3262  max mem: 39406
Epoch: [10]  [20/64]  eta: 0:00:44  lr: 0.000500  min_lr: 0.000000  loss: 1.0750 (1.1277)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3502 (2.3672)  time: 0.8332  data: 0.0006  max mem: 39406
Epoch: [10]  [30/64]  eta: 0:00:32  lr: 0.000500  min_lr: 0.000000  loss: 1.3244 (1.2058)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2670 (2.4007)  time: 0.8365  data: 0.0006  max mem: 39406
Epoch: [10]  [40/64]  eta: 0:00:22  lr: 0.000500  min_lr: 0.000000  loss: 1.3332 (1.2134)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0664 (2.2981)  time: 0.8390  data: 0.0006  max mem: 39406
Epoch: [10]  [50/64]  eta: 0:00:12  lr: 0.000500  min_lr: 0.000000  loss: 1.1240 (1.1946)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0729 (2.3444)  time: 0.8406  data: 0.0004  max mem: 39406
Epoch: [10]  [60/64]  eta: 0:00:03  lr: 0.000499  min_lr: 0.000000  loss: 1.1495 (1.1985)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4697 (2.4046)  time: 0.8420  data: 0.0002  max mem: 39406
Epoch: [10]  [63/64]  eta: 0:00:00  lr: 0.000499  min_lr: 0.000000  loss: 1.0718 (1.2025)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4547 (2.3969)  time: 0.8421  data: 0.0002  max mem: 39406
Epoch: [10] Total time: 0:00:57 (0.8969 s / it)
2025-04-28 16:36:57 Averaged stats: lr: 0.000499  min_lr: 0.000000  loss: 1.0718 (1.2025)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4547 (2.3969)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4750  data: 3.0708  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4370  data: 1.0237  max mem: 39406
Test: Total time: 0:00:04 (1.4966 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8257 Acc: 0.8974 Recall_macro: 0.8257 Recall_weighted: 0.8974 AUC-ROC: 0.9818 Weighted F1-score: 0.9049
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 10, 'Val Loss': 0.4375028908252716, 'Val BAcc': np.float64(0.8256782649263853), 'Val Acc': 0.8973913043478261, 'Val ROC': np.float64(0.9818476417408669), 'Val W_F1': 0.904879644722119, 'Val Recall_macro': 0.8256782649263853, 'Val Recall_weighted': 0.8973913043478261}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [11]  [ 0/64]  eta: 0:04:13  lr: 0.000499  min_lr: 0.000000  loss: 1.4963 (1.4963)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7218 (2.7218)  time: 3.9595  data: 3.1462  max mem: 39406
Epoch: [11]  [10/64]  eta: 0:00:59  lr: 0.000499  min_lr: 0.000000  loss: 1.1134 (1.1599)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5352 (2.5252)  time: 1.1063  data: 0.2865  max mem: 39406
Epoch: [11]  [20/64]  eta: 0:00:42  lr: 0.000499  min_lr: 0.000000  loss: 1.1134 (1.1835)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4872 (2.5472)  time: 0.8241  data: 0.0005  max mem: 39406
Epoch: [11]  [30/64]  eta: 0:00:31  lr: 0.000498  min_lr: 0.000000  loss: 1.2389 (1.2239)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3837 (2.4173)  time: 0.8291  data: 0.0005  max mem: 39406
Epoch: [11]  [40/64]  eta: 0:00:21  lr: 0.000498  min_lr: 0.000000  loss: 1.4299 (1.2560)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0973 (2.4045)  time: 0.8333  data: 0.0004  max mem: 39406
Epoch: [11]  [50/64]  eta: 0:00:12  lr: 0.000498  min_lr: 0.000000  loss: 1.3828 (1.2282)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2639 (2.4214)  time: 0.8375  data: 0.0003  max mem: 39406
Epoch: [11]  [60/64]  eta: 0:00:03  lr: 0.000497  min_lr: 0.000000  loss: 1.2647 (1.2478)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4115 (2.4523)  time: 0.8402  data: 0.0002  max mem: 39406
Epoch: [11]  [63/64]  eta: 0:00:00  lr: 0.000497  min_lr: 0.000000  loss: 1.2647 (1.2467)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5354 (2.4504)  time: 0.8406  data: 0.0001  max mem: 39406
Epoch: [11] Total time: 0:00:56 (0.8849 s / it)
2025-04-28 16:38:05 Averaged stats: lr: 0.000497  min_lr: 0.000000  loss: 1.2647 (1.2467)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5354 (2.4504)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4732  data: 3.0699  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4356  data: 1.0234  max mem: 39406
Test: Total time: 0:00:04 (1.4950 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7993 Acc: 0.8661 Recall_macro: 0.7993 Recall_weighted: 0.8661 AUC-ROC: 0.9842 Weighted F1-score: 0.8876
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 11, 'Val Loss': 0.47983667254447937, 'Val BAcc': np.float64(0.7993153880371926), 'Val Acc': 0.8660869565217392, 'Val ROC': np.float64(0.9841995401142253), 'Val W_F1': 0.8876018839523038, 'Val Recall_macro': 0.7993153880371926, 'Val Recall_weighted': 0.8660869565217392}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [12]  [ 0/64]  eta: 0:04:11  lr: 0.000497  min_lr: 0.000000  loss: 1.0949 (1.0949)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9419 (1.9419)  time: 3.9321  data: 3.1080  max mem: 39406
Epoch: [12]  [10/64]  eta: 0:01:00  lr: 0.000496  min_lr: 0.000000  loss: 1.3143 (1.2306)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3774 (2.4090)  time: 1.1125  data: 0.2830  max mem: 39406
Epoch: [12]  [20/64]  eta: 0:00:43  lr: 0.000496  min_lr: 0.000000  loss: 1.2201 (1.2224)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4859 (2.4733)  time: 0.8324  data: 0.0005  max mem: 39406
Epoch: [12]  [30/64]  eta: 0:00:31  lr: 0.000495  min_lr: 0.000000  loss: 1.2325 (1.2197)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2648 (2.4185)  time: 0.8363  data: 0.0005  max mem: 39406
Epoch: [12]  [40/64]  eta: 0:00:21  lr: 0.000495  min_lr: 0.000000  loss: 1.1091 (1.1780)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2648 (2.4615)  time: 0.8393  data: 0.0005  max mem: 39406
Epoch: [12]  [50/64]  eta: 0:00:12  lr: 0.000494  min_lr: 0.000000  loss: 1.1091 (1.1975)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4906 (2.5008)  time: 0.8411  data: 0.0004  max mem: 39406
Epoch: [12]  [60/64]  eta: 0:00:03  lr: 0.000493  min_lr: 0.000000  loss: 1.1926 (1.1729)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2485 (2.4425)  time: 0.8432  data: 0.0002  max mem: 39406
Epoch: [12]  [63/64]  eta: 0:00:00  lr: 0.000493  min_lr: 0.000000  loss: 1.1926 (1.1709)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1970 (2.4428)  time: 0.8439  data: 0.0002  max mem: 39406
Epoch: [12] Total time: 0:00:56 (0.8895 s / it)
2025-04-28 16:39:06 Averaged stats: lr: 0.000493  min_lr: 0.000000  loss: 1.1926 (1.1709)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1970 (2.4428)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.2613  data: 2.8583  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3648  data: 0.9529  max mem: 39406
Test: Total time: 0:00:04 (1.4245 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7888 Acc: 0.8730 Recall_macro: 0.7888 Recall_weighted: 0.8730 AUC-ROC: 0.9859 Weighted F1-score: 0.8886
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 12, 'Val Loss': 0.3807775676250458, 'Val BAcc': np.float64(0.7888072996343672), 'Val Acc': 0.8730434782608696, 'Val ROC': np.float64(0.9859150763868326), 'Val W_F1': 0.8886345239279674, 'Val Recall_macro': 0.7888072996343672, 'Val Recall_weighted': 0.8730434782608696}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [13]  [ 0/64]  eta: 0:03:58  lr: 0.000493  min_lr: 0.000000  loss: 1.2080 (1.2080)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1286 (2.1286)  time: 3.7245  data: 2.8987  max mem: 39406
Epoch: [13]  [10/64]  eta: 0:00:59  lr: 0.000492  min_lr: 0.000000  loss: 1.2080 (1.1357)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1579 (2.4178)  time: 1.0948  data: 0.2640  max mem: 39406
Epoch: [13]  [20/64]  eta: 0:00:42  lr: 0.000492  min_lr: 0.000000  loss: 1.0594 (1.1341)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3132 (2.4218)  time: 0.8337  data: 0.0005  max mem: 39406
Epoch: [13]  [30/64]  eta: 0:00:31  lr: 0.000491  min_lr: 0.000000  loss: 1.1295 (1.1409)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1721 (2.3208)  time: 0.8367  data: 0.0005  max mem: 39406
Epoch: [13]  [40/64]  eta: 0:00:21  lr: 0.000490  min_lr: 0.000000  loss: 1.1714 (1.1599)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1533 (2.3381)  time: 0.8396  data: 0.0005  max mem: 39406
Epoch: [13]  [50/64]  eta: 0:00:12  lr: 0.000489  min_lr: 0.000000  loss: 1.1714 (1.1532)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3434 (2.3220)  time: 0.8423  data: 0.0004  max mem: 39406
Epoch: [13]  [60/64]  eta: 0:00:03  lr: 0.000488  min_lr: 0.000000  loss: 1.1439 (1.1532)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2686 (2.3125)  time: 0.8431  data: 0.0002  max mem: 39406
Epoch: [13]  [63/64]  eta: 0:00:00  lr: 0.000488  min_lr: 0.000000  loss: 1.2053 (1.1574)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3386 (2.3132)  time: 0.8433  data: 0.0001  max mem: 39406
Epoch: [13] Total time: 0:00:56 (0.8871 s / it)
2025-04-28 16:40:08 Averaged stats: lr: 0.000488  min_lr: 0.000000  loss: 1.2053 (1.1574)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3386 (2.3132)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.3490  data: 2.9444  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3947  data: 0.9816  max mem: 39406
Test: Total time: 0:00:04 (1.4566 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7888 Acc: 0.8052 Recall_macro: 0.7888 Recall_weighted: 0.8052 AUC-ROC: 0.9799 Weighted F1-score: 0.8430
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 13, 'Val Loss': 0.5567061305046082, 'Val BAcc': np.float64(0.7887784396055073), 'Val Acc': 0.8052173913043478, 'Val ROC': np.float64(0.9799314971306402), 'Val W_F1': 0.8430013788410761, 'Val Recall_macro': 0.7887784396055073, 'Val Recall_weighted': 0.8052173913043478}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [14]  [ 0/64]  eta: 0:04:26  lr: 0.000488  min_lr: 0.000000  loss: 1.0106 (1.0106)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1272 (2.1272)  time: 4.1586  data: 3.3361  max mem: 39406
Epoch: [14]  [10/64]  eta: 0:01:01  lr: 0.000487  min_lr: 0.000000  loss: 1.2862 (1.1453)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1272 (2.3062)  time: 1.1341  data: 0.3036  max mem: 39406
Epoch: [14]  [20/64]  eta: 0:00:43  lr: 0.000486  min_lr: 0.000000  loss: 1.2862 (1.1662)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2218 (2.3134)  time: 0.8337  data: 0.0004  max mem: 39406
Epoch: [14]  [30/64]  eta: 0:00:32  lr: 0.000485  min_lr: 0.000000  loss: 1.3106 (1.2182)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2943 (2.3513)  time: 0.8371  data: 0.0005  max mem: 39406
Epoch: [14]  [40/64]  eta: 0:00:22  lr: 0.000484  min_lr: 0.000000  loss: 1.2979 (1.1979)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2663 (2.3221)  time: 0.8390  data: 0.0005  max mem: 39406
Epoch: [14]  [50/64]  eta: 0:00:12  lr: 0.000483  min_lr: 0.000000  loss: 1.0958 (1.1753)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4202 (2.4360)  time: 0.8411  data: 0.0003  max mem: 39406
Epoch: [14]  [60/64]  eta: 0:00:03  lr: 0.000481  min_lr: 0.000000  loss: 1.0932 (1.1573)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4696 (2.4430)  time: 0.8434  data: 0.0002  max mem: 39406
Epoch: [14]  [63/64]  eta: 0:00:00  lr: 0.000481  min_lr: 0.000000  loss: 1.0932 (1.1555)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4606 (2.4298)  time: 0.8438  data: 0.0002  max mem: 39406
Epoch: [14] Total time: 0:00:57 (0.8939 s / it)
2025-04-28 16:41:09 Averaged stats: lr: 0.000481  min_lr: 0.000000  loss: 1.0932 (1.1555)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4606 (2.4298)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.2332  data: 2.8301  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3585  data: 0.9435  max mem: 39406
Test: Total time: 0:00:04 (1.4178 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7728 Acc: 0.9061 Recall_macro: 0.7728 Recall_weighted: 0.9061 AUC-ROC: 0.9818 Weighted F1-score: 0.9134
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 14, 'Val Loss': 0.3372710049152374, 'Val BAcc': np.float64(0.7728173246970238), 'Val Acc': 0.9060869565217391, 'Val ROC': np.float64(0.9818054539382083), 'Val W_F1': 0.9134260090919485, 'Val Recall_macro': 0.7728173246970238, 'Val Recall_weighted': 0.9060869565217391}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [15]  [ 0/64]  eta: 0:04:05  lr: 0.000481  min_lr: 0.000000  loss: 1.3437 (1.3437)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1095 (2.1095)  time: 3.8382  data: 3.0173  max mem: 39406
Epoch: [15]  [10/64]  eta: 0:00:59  lr: 0.000480  min_lr: 0.000000  loss: 1.3065 (1.2862)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3020 (2.3594)  time: 1.1049  data: 0.2748  max mem: 39406
Epoch: [15]  [20/64]  eta: 0:00:42  lr: 0.000479  min_lr: 0.000000  loss: 1.2940 (1.2562)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3585 (2.4026)  time: 0.8336  data: 0.0005  max mem: 39406
Epoch: [15]  [30/64]  eta: 0:00:31  lr: 0.000477  min_lr: 0.000000  loss: 1.3023 (1.2393)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4847 (2.4619)  time: 0.8364  data: 0.0005  max mem: 39406
Epoch: [15]  [40/64]  eta: 0:00:21  lr: 0.000476  min_lr: 0.000000  loss: 1.2680 (1.2192)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3319 (2.4005)  time: 0.8386  data: 0.0005  max mem: 39406
Epoch: [15]  [50/64]  eta: 0:00:12  lr: 0.000475  min_lr: 0.000000  loss: 1.2480 (1.2223)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2579 (2.3962)  time: 0.8411  data: 0.0004  max mem: 39406
Epoch: [15]  [60/64]  eta: 0:00:03  lr: 0.000473  min_lr: 0.000000  loss: 1.2949 (1.2114)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2483 (2.3438)  time: 0.8429  data: 0.0002  max mem: 39406
Epoch: [15]  [63/64]  eta: 0:00:00  lr: 0.000473  min_lr: 0.000000  loss: 1.2949 (1.1974)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1811 (2.3416)  time: 0.8433  data: 0.0001  max mem: 39406
Epoch: [15] Total time: 0:00:56 (0.8886 s / it)
2025-04-28 16:42:10 Averaged stats: lr: 0.000473  min_lr: 0.000000  loss: 1.2949 (1.1974)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1811 (2.3416)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4250  data: 3.0216  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4202  data: 1.0073  max mem: 39406
Test: Total time: 0:00:04 (1.4818 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7885 Acc: 0.8765 Recall_macro: 0.7885 Recall_weighted: 0.8765 AUC-ROC: 0.9871 Weighted F1-score: 0.8932
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 15, 'Val Loss': 0.3697458505630493, 'Val BAcc': np.float64(0.7884883203680195), 'Val Acc': 0.8765217391304347, 'Val ROC': np.float64(0.9871128670852265), 'Val W_F1': 0.8932456665244389, 'Val Recall_macro': 0.7884883203680195, 'Val Recall_weighted': 0.8765217391304347}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [16]  [ 0/64]  eta: 0:04:23  lr: 0.000473  min_lr: 0.000000  loss: 0.6922 (0.6922)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7780 (2.7780)  time: 4.1126  data: 3.2896  max mem: 39406
Epoch: [16]  [10/64]  eta: 0:01:00  lr: 0.000471  min_lr: 0.000000  loss: 1.1565 (1.0754)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2319 (2.4447)  time: 1.1292  data: 0.2995  max mem: 39406
Epoch: [16]  [20/64]  eta: 0:00:43  lr: 0.000470  min_lr: 0.000000  loss: 1.2158 (1.1577)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1568 (2.2923)  time: 0.8331  data: 0.0004  max mem: 39406
Epoch: [16]  [30/64]  eta: 0:00:31  lr: 0.000468  min_lr: 0.000000  loss: 1.3396 (1.1843)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2375 (2.3535)  time: 0.8368  data: 0.0005  max mem: 39406
Epoch: [16]  [40/64]  eta: 0:00:21  lr: 0.000467  min_lr: 0.000000  loss: 1.2627 (1.1946)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4758 (2.3462)  time: 0.8385  data: 0.0005  max mem: 39406
Epoch: [16]  [50/64]  eta: 0:00:12  lr: 0.000465  min_lr: 0.000000  loss: 1.1751 (1.1594)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1414 (2.3268)  time: 0.8400  data: 0.0004  max mem: 39406
Epoch: [16]  [60/64]  eta: 0:00:03  lr: 0.000464  min_lr: 0.000000  loss: 1.0311 (1.1621)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3164 (2.3869)  time: 0.8419  data: 0.0002  max mem: 39406
Epoch: [16]  [63/64]  eta: 0:00:00  lr: 0.000463  min_lr: 0.000000  loss: 1.0572 (1.1521)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3710 (2.3807)  time: 0.8425  data: 0.0001  max mem: 39406
Epoch: [16] Total time: 0:00:57 (0.8921 s / it)
2025-04-28 16:43:12 Averaged stats: lr: 0.000463  min_lr: 0.000000  loss: 1.0572 (1.1521)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3710 (2.3807)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.3017  data: 2.8954  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3783  data: 0.9653  max mem: 39406
Test: Total time: 0:00:04 (1.4394 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7716 Acc: 0.8591 Recall_macro: 0.7716 Recall_weighted: 0.8591 AUC-ROC: 0.9820 Weighted F1-score: 0.8800
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 16, 'Val Loss': 0.41934412717819214, 'Val BAcc': np.float64(0.771565710813831), 'Val Acc': 0.8591304347826086, 'Val ROC': np.float64(0.9819527644598564), 'Val W_F1': 0.8800398692761195, 'Val Recall_macro': 0.771565710813831, 'Val Recall_weighted': 0.8591304347826086}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [17]  [ 0/64]  eta: 0:04:10  lr: 0.000463  min_lr: 0.000000  loss: 1.2574 (1.2574)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9176 (1.9176)  time: 3.9183  data: 3.0960  max mem: 39406
Epoch: [17]  [10/64]  eta: 0:01:00  lr: 0.000462  min_lr: 0.000000  loss: 1.1350 (1.1205)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0670 (2.5785)  time: 1.1129  data: 0.2819  max mem: 39406
Epoch: [17]  [20/64]  eta: 0:00:43  lr: 0.000460  min_lr: 0.000000  loss: 1.1350 (1.1624)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2842 (2.5424)  time: 0.8336  data: 0.0005  max mem: 39406
Epoch: [17]  [30/64]  eta: 0:00:31  lr: 0.000458  min_lr: 0.000000  loss: 1.3462 (1.2227)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5169 (2.4987)  time: 0.8364  data: 0.0005  max mem: 39406
Epoch: [17]  [40/64]  eta: 0:00:21  lr: 0.000457  min_lr: 0.000000  loss: 1.2916 (1.2073)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2256 (2.4418)  time: 0.8393  data: 0.0005  max mem: 39406
Epoch: [17]  [50/64]  eta: 0:00:12  lr: 0.000455  min_lr: 0.000000  loss: 1.1845 (1.1932)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2256 (2.4410)  time: 0.8410  data: 0.0003  max mem: 39406
Epoch: [17]  [60/64]  eta: 0:00:03  lr: 0.000453  min_lr: 0.000000  loss: 1.1884 (1.1857)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2548 (2.4126)  time: 0.8425  data: 0.0002  max mem: 39406
Epoch: [17]  [63/64]  eta: 0:00:00  lr: 0.000453  min_lr: 0.000000  loss: 1.1884 (1.1879)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3364 (2.4218)  time: 0.8430  data: 0.0001  max mem: 39406
Epoch: [17] Total time: 0:00:56 (0.8898 s / it)
2025-04-28 16:44:13 Averaged stats: lr: 0.000453  min_lr: 0.000000  loss: 1.1884 (1.1879)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3364 (2.4218)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4227  data: 3.0161  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4192  data: 1.0055  max mem: 39406
Test: Total time: 0:00:04 (1.4805 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8045 Acc: 0.8678 Recall_macro: 0.8045 Recall_weighted: 0.8678 AUC-ROC: 0.9833 Weighted F1-score: 0.8914
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 17, 'Val Loss': 0.4227636754512787, 'Val BAcc': np.float64(0.8044509542253904), 'Val Acc': 0.8678260869565217, 'Val ROC': np.float64(0.9833323041962653), 'Val W_F1': 0.8913549166669339, 'Val Recall_macro': 0.8044509542253904, 'Val Recall_weighted': 0.8678260869565217}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [18]  [ 0/64]  eta: 0:04:03  lr: 0.000452  min_lr: 0.000000  loss: 1.3474 (1.3474)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8639 (1.8639)  time: 3.8091  data: 2.9873  max mem: 39406
Epoch: [18]  [10/64]  eta: 0:00:59  lr: 0.000451  min_lr: 0.000000  loss: 1.1040 (1.0726)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0994 (2.2249)  time: 1.1021  data: 0.2720  max mem: 39406
Epoch: [18]  [20/64]  eta: 0:00:42  lr: 0.000449  min_lr: 0.000000  loss: 1.1522 (1.1308)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4400 (2.4347)  time: 0.8343  data: 0.0005  max mem: 39406
Epoch: [18]  [30/64]  eta: 0:00:31  lr: 0.000447  min_lr: 0.000000  loss: 1.3225 (1.2002)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3785 (2.3455)  time: 0.8382  data: 0.0005  max mem: 39406
Epoch: [18]  [40/64]  eta: 0:00:21  lr: 0.000445  min_lr: 0.000000  loss: 1.2901 (1.1914)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0987 (2.2918)  time: 0.8405  data: 0.0005  max mem: 39406
Epoch: [18]  [50/64]  eta: 0:00:12  lr: 0.000443  min_lr: 0.000000  loss: 1.2225 (1.1896)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2256 (2.3009)  time: 0.8429  data: 0.0004  max mem: 39406
Epoch: [18]  [60/64]  eta: 0:00:03  lr: 0.000441  min_lr: 0.000000  loss: 1.1784 (1.1828)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2570 (2.3136)  time: 0.8439  data: 0.0002  max mem: 39406
Epoch: [18]  [63/64]  eta: 0:00:00  lr: 0.000440  min_lr: 0.000000  loss: 1.1784 (1.1810)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3804 (2.3254)  time: 0.8441  data: 0.0001  max mem: 39406
Epoch: [18] Total time: 0:00:56 (0.8891 s / it)
2025-04-28 16:45:15 Averaged stats: lr: 0.000440  min_lr: 0.000000  loss: 1.1784 (1.1810)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3804 (2.3254)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.3729  data: 2.9745  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4042  data: 0.9916  max mem: 39406
Test: Total time: 0:00:04 (1.4633 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7857 Acc: 0.8887 Recall_macro: 0.7857 Recall_weighted: 0.8887 AUC-ROC: 0.9737 Weighted F1-score: 0.9004
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 18, 'Val Loss': 0.39891427755355835, 'Val BAcc': np.float64(0.7857496555240916), 'Val Acc': 0.888695652173913, 'Val ROC': np.float64(0.9736827986353358), 'Val W_F1': 0.9004265603560188, 'Val Recall_macro': 0.7857496555240916, 'Val Recall_weighted': 0.888695652173913}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [19]  [ 0/64]  eta: 0:04:16  lr: 0.000440  min_lr: 0.000000  loss: 1.4989 (1.4989)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8635 (2.8635)  time: 4.0089  data: 3.1854  max mem: 39406
Epoch: [19]  [10/64]  eta: 0:01:00  lr: 0.000438  min_lr: 0.000000  loss: 1.1017 (1.1009)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0877 (2.1338)  time: 1.1214  data: 0.2900  max mem: 39406
Epoch: [19]  [20/64]  eta: 0:00:43  lr: 0.000436  min_lr: 0.000000  loss: 1.1017 (1.0982)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0877 (2.2413)  time: 0.8336  data: 0.0004  max mem: 39406
Epoch: [19]  [30/64]  eta: 0:00:31  lr: 0.000434  min_lr: 0.000000  loss: 1.1445 (1.1174)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2464 (2.2115)  time: 0.8371  data: 0.0004  max mem: 39406
Epoch: [19]  [40/64]  eta: 0:00:21  lr: 0.000432  min_lr: 0.000000  loss: 1.1421 (1.0908)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2464 (2.2736)  time: 0.8398  data: 0.0005  max mem: 39406
Epoch: [19]  [50/64]  eta: 0:00:12  lr: 0.000430  min_lr: 0.000000  loss: 1.1670 (1.1295)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4474 (2.2919)  time: 0.8409  data: 0.0003  max mem: 39406
Epoch: [19]  [60/64]  eta: 0:00:03  lr: 0.000428  min_lr: 0.000000  loss: 1.2670 (1.1351)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1249 (2.2534)  time: 0.8435  data: 0.0002  max mem: 39406
Epoch: [19]  [63/64]  eta: 0:00:00  lr: 0.000427  min_lr: 0.000000  loss: 1.2358 (1.1394)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0976 (2.2523)  time: 0.8436  data: 0.0001  max mem: 39406
Epoch: [19] Total time: 0:00:57 (0.8917 s / it)
2025-04-28 16:46:16 Averaged stats: lr: 0.000427  min_lr: 0.000000  loss: 1.2358 (1.1394)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0976 (2.2523)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.2238  data: 2.8247  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3544  data: 0.9417  max mem: 39406
Test: Total time: 0:00:04 (1.4116 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7956 Acc: 0.9026 Recall_macro: 0.7956 Recall_weighted: 0.9026 AUC-ROC: 0.9839 Weighted F1-score: 0.9129
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 19, 'Val Loss': 0.34185025095939636, 'Val BAcc': np.float64(0.7955620653365015), 'Val Acc': 0.9026086956521739, 'Val ROC': np.float64(0.9838852067014751), 'Val W_F1': 0.9129148223666181, 'Val Recall_macro': 0.7955620653365015, 'Val Recall_weighted': 0.9026086956521739}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [20]  [ 0/64]  eta: 0:03:59  lr: 0.000427  min_lr: 0.000000  loss: 1.3582 (1.3582)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4473 (2.4473)  time: 3.7426  data: 2.9172  max mem: 39406
Epoch: [20]  [10/64]  eta: 0:00:59  lr: 0.000425  min_lr: 0.000000  loss: 1.3365 (1.2419)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3178 (2.3133)  time: 1.0969  data: 0.2656  max mem: 39406
Epoch: [20]  [20/64]  eta: 0:00:42  lr: 0.000423  min_lr: 0.000000  loss: 1.1928 (1.2065)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1592 (2.3100)  time: 0.8334  data: 0.0005  max mem: 39406
Epoch: [20]  [30/64]  eta: 0:00:31  lr: 0.000420  min_lr: 0.000000  loss: 1.1213 (1.1484)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1592 (2.3206)  time: 0.8355  data: 0.0005  max mem: 39406
Epoch: [20]  [40/64]  eta: 0:00:21  lr: 0.000418  min_lr: 0.000000  loss: 1.0038 (1.1083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2428 (2.3600)  time: 0.8377  data: 0.0005  max mem: 39406
Epoch: [20]  [50/64]  eta: 0:00:12  lr: 0.000416  min_lr: 0.000000  loss: 1.0852 (1.1179)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2428 (2.3487)  time: 0.8400  data: 0.0003  max mem: 39406
Epoch: [20]  [60/64]  eta: 0:00:03  lr: 0.000413  min_lr: 0.000000  loss: 1.2621 (1.1248)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3743 (2.3382)  time: 0.8417  data: 0.0002  max mem: 39406
Epoch: [20]  [63/64]  eta: 0:00:00  lr: 0.000413  min_lr: 0.000000  loss: 1.2297 (1.1145)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3558 (2.3326)  time: 0.8420  data: 0.0002  max mem: 39406
Epoch: [20] Total time: 0:00:56 (0.8858 s / it)
2025-04-28 16:47:17 Averaged stats: lr: 0.000413  min_lr: 0.000000  loss: 1.2297 (1.1145)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3558 (2.3326)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1259  data: 2.7285  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3212  data: 0.9096  max mem: 39406
Test: Total time: 0:00:04 (1.3730 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8027 Acc: 0.8643 Recall_macro: 0.8027 Recall_weighted: 0.8643 AUC-ROC: 0.9781 Weighted F1-score: 0.8878
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 20, 'Val Loss': 0.41629651188850403, 'Val BAcc': np.float64(0.8027193524937885), 'Val Acc': 0.8643478260869565, 'Val ROC': np.float64(0.9780966531394182), 'Val W_F1': 0.887835792864505, 'Val Recall_macro': 0.8027193524937885, 'Val Recall_weighted': 0.8643478260869565}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [21]  [ 0/64]  eta: 0:03:53  lr: 0.000413  min_lr: 0.000000  loss: 1.0152 (1.0152)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2050 (2.2050)  time: 3.6561  data: 2.8352  max mem: 39406
Epoch: [21]  [10/64]  eta: 0:00:58  lr: 0.000410  min_lr: 0.000000  loss: 1.1773 (1.1849)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3253 (2.3834)  time: 1.0879  data: 0.2582  max mem: 39406
Epoch: [21]  [20/64]  eta: 0:00:42  lr: 0.000408  min_lr: 0.000000  loss: 1.2575 (1.2197)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2758 (2.3315)  time: 0.8327  data: 0.0005  max mem: 39406
Epoch: [21]  [30/64]  eta: 0:00:31  lr: 0.000405  min_lr: 0.000000  loss: 1.2580 (1.2330)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2735 (2.3243)  time: 0.8365  data: 0.0005  max mem: 39406
Epoch: [21]  [40/64]  eta: 0:00:21  lr: 0.000403  min_lr: 0.000000  loss: 1.2329 (1.2102)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2592 (2.3122)  time: 0.8398  data: 0.0005  max mem: 39406
Epoch: [21]  [50/64]  eta: 0:00:12  lr: 0.000401  min_lr: 0.000000  loss: 1.2031 (1.1966)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1626 (2.3196)  time: 0.8409  data: 0.0003  max mem: 39406
Epoch: [21]  [60/64]  eta: 0:00:03  lr: 0.000398  min_lr: 0.000000  loss: 1.2054 (1.2001)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2571 (2.3332)  time: 0.8421  data: 0.0002  max mem: 39406
Epoch: [21]  [63/64]  eta: 0:00:00  lr: 0.000397  min_lr: 0.000000  loss: 1.2158 (1.1966)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2571 (2.3190)  time: 0.8428  data: 0.0001  max mem: 39406
Epoch: [21] Total time: 0:00:56 (0.8852 s / it)
2025-04-28 16:48:18 Averaged stats: lr: 0.000397  min_lr: 0.000000  loss: 1.2158 (1.1966)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2571 (2.3190)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1117  data: 2.7080  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3167  data: 0.9028  max mem: 39406
Test: Total time: 0:00:04 (1.3610 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8100 Acc: 0.9043 Recall_macro: 0.8100 Recall_weighted: 0.9043 AUC-ROC: 0.9813 Weighted F1-score: 0.9145
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 21, 'Val Loss': 0.36984941363334656, 'Val BAcc': np.float64(0.8100209397953758), 'Val Acc': 0.9043478260869565, 'Val ROC': np.float64(0.9812832477441249), 'Val W_F1': 0.9145132091681752, 'Val Recall_macro': 0.8100209397953758, 'Val Recall_weighted': 0.9043478260869565}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [22]  [ 0/64]  eta: 0:03:59  lr: 0.000397  min_lr: 0.000000  loss: 1.0587 (1.0587)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4753 (1.4753)  time: 3.7459  data: 2.9244  max mem: 39406
Epoch: [22]  [10/64]  eta: 0:00:59  lr: 0.000395  min_lr: 0.000000  loss: 1.2052 (1.0880)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1293 (2.1779)  time: 1.0967  data: 0.2663  max mem: 39406
Epoch: [22]  [20/64]  eta: 0:00:43  lr: 0.000392  min_lr: 0.000000  loss: 1.2050 (1.0861)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1293 (2.2199)  time: 0.8392  data: 0.0005  max mem: 39406
Epoch: [22]  [30/64]  eta: 0:00:31  lr: 0.000390  min_lr: 0.000000  loss: 1.1898 (1.0956)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3773 (2.2664)  time: 0.8417  data: 0.0005  max mem: 39406
Epoch: [22]  [40/64]  eta: 0:00:21  lr: 0.000387  min_lr: 0.000000  loss: 1.1967 (1.1071)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3876 (2.2461)  time: 0.8378  data: 0.0004  max mem: 39406
Epoch: [22]  [50/64]  eta: 0:00:12  lr: 0.000384  min_lr: 0.000000  loss: 1.2361 (1.1146)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2222 (2.2352)  time: 0.8397  data: 0.0003  max mem: 39406
Epoch: [22]  [60/64]  eta: 0:00:03  lr: 0.000382  min_lr: 0.000000  loss: 1.2462 (1.1217)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1934 (2.2390)  time: 0.8411  data: 0.0002  max mem: 39406
Epoch: [22]  [63/64]  eta: 0:00:00  lr: 0.000381  min_lr: 0.000000  loss: 1.2462 (1.1213)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1934 (2.2535)  time: 0.8413  data: 0.0001  max mem: 39406
Epoch: [22] Total time: 0:00:56 (0.8875 s / it)
2025-04-28 16:49:19 Averaged stats: lr: 0.000381  min_lr: 0.000000  loss: 1.2462 (1.1213)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1934 (2.2535)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1810  data: 2.7757  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3389  data: 0.9254  max mem: 39406
Test: Total time: 0:00:04 (1.3910 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8072 Acc: 0.9200 Recall_macro: 0.8072 Recall_weighted: 0.9200 AUC-ROC: 0.9724 Weighted F1-score: 0.9264
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 22, 'Val Loss': 0.3142351806163788, 'Val BAcc': np.float64(0.8071911380182056), 'Val Acc': 0.92, 'Val ROC': np.float64(0.9723614370725328), 'Val W_F1': 0.9263824324031352, 'Val Recall_macro': 0.8071911380182056, 'Val Recall_weighted': 0.92}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [23]  [ 0/64]  eta: 0:04:09  lr: 0.000381  min_lr: 0.000000  loss: 1.0139 (1.0139)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4629 (1.4629)  time: 3.9048  data: 3.0869  max mem: 39406
Epoch: [23]  [10/64]  eta: 0:00:59  lr: 0.000378  min_lr: 0.000000  loss: 0.8324 (0.8897)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1095 (2.1240)  time: 1.1092  data: 0.2811  max mem: 39406
Epoch: [23]  [20/64]  eta: 0:00:43  lr: 0.000376  min_lr: 0.000000  loss: 0.8324 (0.9350)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2917 (2.3988)  time: 0.8317  data: 0.0005  max mem: 39406
Epoch: [23]  [30/64]  eta: 0:00:31  lr: 0.000373  min_lr: 0.000000  loss: 1.0803 (0.9815)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2500 (2.3204)  time: 0.8361  data: 0.0006  max mem: 39406
Epoch: [23]  [40/64]  eta: 0:00:21  lr: 0.000370  min_lr: 0.000000  loss: 1.1227 (1.0430)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1804 (2.3719)  time: 0.8399  data: 0.0006  max mem: 39406
Epoch: [23]  [50/64]  eta: 0:00:12  lr: 0.000368  min_lr: 0.000000  loss: 1.2330 (1.0667)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1710 (2.3100)  time: 0.8413  data: 0.0004  max mem: 39406
Epoch: [23]  [60/64]  eta: 0:00:03  lr: 0.000365  min_lr: 0.000000  loss: 1.1125 (1.0650)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1416 (2.2930)  time: 0.8416  data: 0.0002  max mem: 39406
Epoch: [23]  [63/64]  eta: 0:00:00  lr: 0.000364  min_lr: 0.000000  loss: 1.1125 (1.0741)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1358 (2.3026)  time: 0.8417  data: 0.0002  max mem: 39406
Epoch: [23] Total time: 0:00:56 (0.8884 s / it)
2025-04-28 16:50:20 Averaged stats: lr: 0.000364  min_lr: 0.000000  loss: 1.1125 (1.0741)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1358 (2.3026)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1678  data: 2.7653  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3358  data: 0.9219  max mem: 39406
Test: Total time: 0:00:04 (1.3941 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8231 Acc: 0.9148 Recall_macro: 0.8231 Recall_weighted: 0.9148 AUC-ROC: 0.9794 Weighted F1-score: 0.9217
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 23, 'Val Loss': 0.29286906123161316, 'Val BAcc': np.float64(0.8231097223578426), 'Val Acc': 0.9147826086956522, 'Val ROC': np.float64(0.9793596743772086), 'Val W_F1': 0.9217467445328765, 'Val Recall_macro': 0.8231097223578426, 'Val Recall_weighted': 0.9147826086956522}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [24]  [ 0/64]  eta: 0:04:18  lr: 0.000364  min_lr: 0.000000  loss: 1.2013 (1.2013)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5957 (2.5957)  time: 4.0353  data: 3.2109  max mem: 39406
Epoch: [24]  [10/64]  eta: 0:01:00  lr: 0.000361  min_lr: 0.000000  loss: 1.0754 (1.0423)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0678 (2.1885)  time: 1.1209  data: 0.2924  max mem: 39406
Epoch: [24]  [20/64]  eta: 0:00:43  lr: 0.000358  min_lr: 0.000000  loss: 1.0754 (1.0731)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0688 (2.2353)  time: 0.8315  data: 0.0006  max mem: 39406
Epoch: [24]  [30/64]  eta: 0:00:31  lr: 0.000356  min_lr: 0.000000  loss: 1.0239 (1.0215)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3751 (2.2678)  time: 0.8353  data: 0.0006  max mem: 39406
Epoch: [24]  [40/64]  eta: 0:00:21  lr: 0.000353  min_lr: 0.000000  loss: 0.9431 (1.0229)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2867 (2.2376)  time: 0.8376  data: 0.0006  max mem: 39406
Epoch: [24]  [50/64]  eta: 0:00:12  lr: 0.000350  min_lr: 0.000000  loss: 1.1067 (1.0360)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1168 (2.2329)  time: 0.8392  data: 0.0004  max mem: 39406
Epoch: [24]  [60/64]  eta: 0:00:03  lr: 0.000347  min_lr: 0.000000  loss: 1.1507 (1.0596)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1381 (2.2220)  time: 0.8408  data: 0.0002  max mem: 39406
Epoch: [24]  [63/64]  eta: 0:00:00  lr: 0.000346  min_lr: 0.000000  loss: 1.1507 (1.0691)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1729 (2.2372)  time: 0.8412  data: 0.0001  max mem: 39406
Epoch: [24] Total time: 0:00:56 (0.8897 s / it)
2025-04-28 16:51:21 Averaged stats: lr: 0.000346  min_lr: 0.000000  loss: 1.1507 (1.0691)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1729 (2.2372)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.2239  data: 2.8288  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3542  data: 0.9430  max mem: 39406
Test: Total time: 0:00:04 (1.4105 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7987 Acc: 0.8887 Recall_macro: 0.7987 Recall_weighted: 0.8887 AUC-ROC: 0.9700 Weighted F1-score: 0.9014
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 24, 'Val Loss': 0.4081983268260956, 'Val BAcc': np.float64(0.7987093274311319), 'Val Acc': 0.888695652173913, 'Val ROC': np.float64(0.9700107330501977), 'Val W_F1': 0.9014487682625256, 'Val Recall_macro': 0.7987093274311319, 'Val Recall_weighted': 0.888695652173913}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [25]  [ 0/64]  eta: 0:04:10  lr: 0.000346  min_lr: 0.000000  loss: 1.2733 (1.2733)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2160 (2.2160)  time: 3.9066  data: 3.0866  max mem: 39406
Epoch: [25]  [10/64]  eta: 0:00:59  lr: 0.000343  min_lr: 0.000000  loss: 1.1751 (1.1529)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2160 (2.2332)  time: 1.1092  data: 0.2810  max mem: 39406
Epoch: [25]  [20/64]  eta: 0:00:43  lr: 0.000340  min_lr: 0.000000  loss: 1.0986 (1.1107)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1997 (2.2908)  time: 0.8317  data: 0.0005  max mem: 39406
Epoch: [25]  [30/64]  eta: 0:00:31  lr: 0.000337  min_lr: 0.000000  loss: 1.0986 (1.1212)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1987 (2.2507)  time: 0.8347  data: 0.0004  max mem: 39406
Epoch: [25]  [40/64]  eta: 0:00:21  lr: 0.000335  min_lr: 0.000000  loss: 1.1245 (1.1217)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0162 (2.2851)  time: 0.8369  data: 0.0005  max mem: 39406
Epoch: [25]  [50/64]  eta: 0:00:12  lr: 0.000332  min_lr: 0.000000  loss: 1.0057 (1.0751)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0907 (2.3030)  time: 0.8393  data: 0.0004  max mem: 39406
Epoch: [25]  [60/64]  eta: 0:00:03  lr: 0.000329  min_lr: 0.000000  loss: 0.8212 (1.0512)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2001 (2.3036)  time: 0.8408  data: 0.0002  max mem: 39406
Epoch: [25]  [63/64]  eta: 0:00:00  lr: 0.000328  min_lr: 0.000000  loss: 0.8763 (1.0459)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1388 (2.2824)  time: 0.8415  data: 0.0002  max mem: 39406
Epoch: [25] Total time: 0:00:56 (0.8873 s / it)
2025-04-28 16:52:22 Averaged stats: lr: 0.000328  min_lr: 0.000000  loss: 0.8763 (1.0459)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1388 (2.2824)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1344  data: 2.7344  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3240  data: 0.9116  max mem: 39406
Test: Total time: 0:00:04 (1.3744 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7974 Acc: 0.8591 Recall_macro: 0.7974 Recall_weighted: 0.8591 AUC-ROC: 0.9640 Weighted F1-score: 0.8831
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 25, 'Val Loss': 0.4984913766384125, 'Val BAcc': np.float64(0.7973665766146969), 'Val Acc': 0.8591304347826086, 'Val ROC': np.float64(0.9640147042349431), 'Val W_F1': 0.8831457236269985, 'Val Recall_macro': 0.7973665766146969, 'Val Recall_weighted': 0.8591304347826086}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [26]  [ 0/64]  eta: 0:04:11  lr: 0.000328  min_lr: 0.000000  loss: 1.0926 (1.0926)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8121 (1.8121)  time: 3.9360  data: 3.1161  max mem: 39406
Epoch: [26]  [10/64]  eta: 0:01:00  lr: 0.000325  min_lr: 0.000000  loss: 1.2694 (1.1050)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1636 (2.3363)  time: 1.1130  data: 0.2838  max mem: 39406
Epoch: [26]  [20/64]  eta: 0:00:43  lr: 0.000322  min_lr: 0.000000  loss: 1.1448 (1.0793)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2503 (2.3034)  time: 0.8325  data: 0.0005  max mem: 39406
Epoch: [26]  [30/64]  eta: 0:00:31  lr: 0.000319  min_lr: 0.000000  loss: 1.1448 (1.1049)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2323 (2.2570)  time: 0.8360  data: 0.0005  max mem: 39406
Epoch: [26]  [40/64]  eta: 0:00:21  lr: 0.000316  min_lr: 0.000000  loss: 1.2588 (1.1386)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1724 (2.2435)  time: 0.8389  data: 0.0005  max mem: 39406
Epoch: [26]  [50/64]  eta: 0:00:12  lr: 0.000313  min_lr: 0.000000  loss: 1.1175 (1.1080)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9448 (2.1575)  time: 0.8404  data: 0.0004  max mem: 39406
Epoch: [26]  [60/64]  eta: 0:00:03  lr: 0.000310  min_lr: 0.000000  loss: 0.9933 (1.1034)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1438 (2.2004)  time: 0.8413  data: 0.0002  max mem: 39406
Epoch: [26]  [63/64]  eta: 0:00:00  lr: 0.000309  min_lr: 0.000000  loss: 0.9933 (1.1026)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1592 (2.1947)  time: 0.8416  data: 0.0001  max mem: 39406
Epoch: [26] Total time: 0:00:56 (0.8887 s / it)
2025-04-28 16:53:23 Averaged stats: lr: 0.000309  min_lr: 0.000000  loss: 0.9933 (1.1026)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1592 (2.1947)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0990  data: 2.6941  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3113  data: 0.8981  max mem: 39406
Test: Total time: 0:00:04 (1.3711 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7945 Acc: 0.9148 Recall_macro: 0.7945 Recall_weighted: 0.9148 AUC-ROC: 0.9688 Weighted F1-score: 0.9203
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 26, 'Val Loss': 0.3324942886829376, 'Val BAcc': np.float64(0.7944942442686803), 'Val Acc': 0.9147826086956522, 'Val ROC': np.float64(0.9687658795205566), 'Val W_F1': 0.9203333679483597, 'Val Recall_macro': 0.7944942442686803, 'Val Recall_weighted': 0.9147826086956522}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [27]  [ 0/64]  eta: 0:03:54  lr: 0.000309  min_lr: 0.000000  loss: 1.2798 (1.2798)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7469 (2.7469)  time: 3.6643  data: 2.8485  max mem: 39406
Epoch: [27]  [10/64]  eta: 0:00:58  lr: 0.000306  min_lr: 0.000000  loss: 1.2249 (1.0954)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1841 (2.1555)  time: 1.0880  data: 0.2594  max mem: 39406
Epoch: [27]  [20/64]  eta: 0:00:42  lr: 0.000303  min_lr: 0.000000  loss: 1.1049 (1.0691)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1841 (2.2735)  time: 0.8319  data: 0.0005  max mem: 39406
Epoch: [27]  [30/64]  eta: 0:00:31  lr: 0.000300  min_lr: 0.000000  loss: 0.9286 (1.0240)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2164 (2.2129)  time: 0.8347  data: 0.0004  max mem: 39406
Epoch: [27]  [40/64]  eta: 0:00:21  lr: 0.000297  min_lr: 0.000000  loss: 1.0944 (1.0626)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1071 (2.2373)  time: 0.8375  data: 0.0004  max mem: 39406
Epoch: [27]  [50/64]  eta: 0:00:12  lr: 0.000294  min_lr: 0.000000  loss: 1.1818 (1.0560)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1071 (2.2064)  time: 0.8393  data: 0.0003  max mem: 39406
Epoch: [27]  [60/64]  eta: 0:00:03  lr: 0.000291  min_lr: 0.000000  loss: 1.0538 (1.0521)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2514 (2.2433)  time: 0.8402  data: 0.0002  max mem: 39406
Epoch: [27]  [63/64]  eta: 0:00:00  lr: 0.000290  min_lr: 0.000000  loss: 1.1224 (1.0594)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3383 (2.2486)  time: 0.8408  data: 0.0001  max mem: 39406
Epoch: [27] Total time: 0:00:56 (0.8839 s / it)
2025-04-28 16:54:24 Averaged stats: lr: 0.000290  min_lr: 0.000000  loss: 1.1224 (1.0594)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3383 (2.2486)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1654  data: 2.7622  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3319  data: 0.9209  max mem: 39406
Test: Total time: 0:00:04 (1.3894 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7657 Acc: 0.8696 Recall_macro: 0.7657 Recall_weighted: 0.8696 AUC-ROC: 0.9675 Weighted F1-score: 0.8914
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 27, 'Val Loss': 0.38300326466560364, 'Val BAcc': np.float64(0.7656767459774978), 'Val Acc': 0.8695652173913043, 'Val ROC': np.float64(0.967517597291623), 'Val W_F1': 0.8914428136186426, 'Val Recall_macro': 0.7656767459774978, 'Val Recall_weighted': 0.8695652173913043}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [28]  [ 0/64]  eta: 0:04:11  lr: 0.000290  min_lr: 0.000000  loss: 1.3074 (1.3074)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3823 (2.3823)  time: 3.9287  data: 3.1075  max mem: 39406
Epoch: [28]  [10/64]  eta: 0:01:00  lr: 0.000287  min_lr: 0.000000  loss: 1.2058 (1.1430)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1822 (2.3143)  time: 1.1120  data: 0.2830  max mem: 39406
Epoch: [28]  [20/64]  eta: 0:00:43  lr: 0.000283  min_lr: 0.000000  loss: 1.1143 (1.0975)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1649 (2.2919)  time: 0.8319  data: 0.0005  max mem: 39406
Epoch: [28]  [30/64]  eta: 0:00:31  lr: 0.000280  min_lr: 0.000000  loss: 1.0834 (1.0915)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3472 (2.3542)  time: 0.8347  data: 0.0004  max mem: 39406
Epoch: [28]  [40/64]  eta: 0:00:21  lr: 0.000277  min_lr: 0.000000  loss: 1.1622 (1.1189)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3504 (2.3694)  time: 0.8382  data: 0.0004  max mem: 39406
Epoch: [28]  [50/64]  eta: 0:00:12  lr: 0.000274  min_lr: 0.000000  loss: 1.1622 (1.0960)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1526 (2.3287)  time: 0.8398  data: 0.0004  max mem: 39406
Epoch: [28]  [60/64]  eta: 0:00:03  lr: 0.000271  min_lr: 0.000000  loss: 1.0162 (1.0779)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0043 (2.2672)  time: 0.8392  data: 0.0002  max mem: 39406
Epoch: [28]  [63/64]  eta: 0:00:00  lr: 0.000270  min_lr: 0.000000  loss: 1.0019 (1.0683)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8548 (2.2545)  time: 0.8393  data: 0.0002  max mem: 39406
Epoch: [28] Total time: 0:00:56 (0.8878 s / it)
2025-04-28 16:55:25 Averaged stats: lr: 0.000270  min_lr: 0.000000  loss: 1.0019 (1.0683)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8548 (2.2545)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.2214  data: 2.8251  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3496  data: 0.9418  max mem: 39406
Test: Total time: 0:00:04 (1.4011 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7981 Acc: 0.8957 Recall_macro: 0.7981 Recall_weighted: 0.8957 AUC-ROC: 0.9761 Weighted F1-score: 0.9079
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 28, 'Val Loss': 0.30472519993782043, 'Val BAcc': np.float64(0.7980850394384228), 'Val Acc': 0.8956521739130435, 'Val ROC': np.float64(0.9760939845040302), 'Val W_F1': 0.9078660405622233, 'Val Recall_macro': 0.7980850394384228, 'Val Recall_weighted': 0.8956521739130435}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [29]  [ 0/64]  eta: 0:03:56  lr: 0.000270  min_lr: 0.000000  loss: 0.9797 (0.9797)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3970 (2.3970)  time: 3.6946  data: 2.8663  max mem: 39406
Epoch: [29]  [10/64]  eta: 0:00:58  lr: 0.000267  min_lr: 0.000000  loss: 1.0720 (1.0052)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2379 (2.2002)  time: 1.0894  data: 0.2611  max mem: 39406
Epoch: [29]  [20/64]  eta: 0:00:42  lr: 0.000264  min_lr: 0.000000  loss: 1.1410 (1.0787)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2963 (2.3315)  time: 0.8313  data: 0.0005  max mem: 39406
Epoch: [29]  [30/64]  eta: 0:00:31  lr: 0.000261  min_lr: 0.000000  loss: 1.2244 (1.0879)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1372 (2.2282)  time: 0.8355  data: 0.0005  max mem: 39406
Epoch: [29]  [40/64]  eta: 0:00:21  lr: 0.000258  min_lr: 0.000000  loss: 1.0875 (1.0521)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9625 (2.2073)  time: 0.8381  data: 0.0005  max mem: 39406
Epoch: [29]  [50/64]  eta: 0:00:12  lr: 0.000255  min_lr: 0.000000  loss: 1.0412 (1.0611)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0487 (2.1797)  time: 0.8390  data: 0.0004  max mem: 39406
Epoch: [29]  [60/64]  eta: 0:00:03  lr: 0.000252  min_lr: 0.000000  loss: 0.9987 (1.0345)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7896 (2.0690)  time: 0.8394  data: 0.0002  max mem: 39406
Epoch: [29]  [63/64]  eta: 0:00:00  lr: 0.000251  min_lr: 0.000000  loss: 0.9934 (1.0293)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7896 (2.0737)  time: 0.8397  data: 0.0001  max mem: 39406
Epoch: [29] Total time: 0:00:56 (0.8838 s / it)
2025-04-28 16:56:26 Averaged stats: lr: 0.000251  min_lr: 0.000000  loss: 0.9934 (1.0293)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7896 (2.0737)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0952  data: 2.6890  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3092  data: 0.8965  max mem: 39406
Test: Total time: 0:00:04 (1.3576 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7942 Acc: 0.9165 Recall_macro: 0.7942 Recall_weighted: 0.9165 AUC-ROC: 0.9717 Weighted F1-score: 0.9216
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 29, 'Val Loss': 0.30992597341537476, 'Val BAcc': np.float64(0.7942041250311925), 'Val Acc': 0.9165217391304348, 'Val ROC': np.float64(0.971659521722528), 'Val W_F1': 0.92164255582514, 'Val Recall_macro': 0.7942041250311925, 'Val Recall_weighted': 0.9165217391304348}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [30]  [ 0/64]  eta: 0:04:04  lr: 0.000251  min_lr: 0.000000  loss: 0.9326 (0.9326)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9795 (1.9795)  time: 3.8164  data: 2.9960  max mem: 39406
Epoch: [30]  [10/64]  eta: 0:00:59  lr: 0.000247  min_lr: 0.000000  loss: 0.9326 (0.9983)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2681 (2.3111)  time: 1.1010  data: 0.2728  max mem: 39406
Epoch: [30]  [20/64]  eta: 0:00:42  lr: 0.000244  min_lr: 0.000000  loss: 0.9817 (0.9888)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9633 (2.1254)  time: 0.8317  data: 0.0005  max mem: 39406
Epoch: [30]  [30/64]  eta: 0:00:31  lr: 0.000241  min_lr: 0.000000  loss: 1.0184 (0.9815)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9227 (2.1246)  time: 0.8354  data: 0.0005  max mem: 39406
Epoch: [30]  [40/64]  eta: 0:00:21  lr: 0.000238  min_lr: 0.000000  loss: 1.0384 (0.9867)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0133 (2.1176)  time: 0.8384  data: 0.0005  max mem: 39406
Epoch: [30]  [50/64]  eta: 0:00:12  lr: 0.000235  min_lr: 0.000000  loss: 1.0191 (0.9714)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0504 (2.1076)  time: 0.8397  data: 0.0003  max mem: 39406
Epoch: [30]  [60/64]  eta: 0:00:03  lr: 0.000232  min_lr: 0.000000  loss: 0.9512 (0.9821)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1477 (2.1497)  time: 0.8406  data: 0.0002  max mem: 39406
Epoch: [30]  [63/64]  eta: 0:00:00  lr: 0.000231  min_lr: 0.000000  loss: 0.9512 (0.9735)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1477 (2.1515)  time: 0.8413  data: 0.0001  max mem: 39406
Epoch: [30] Total time: 0:00:56 (0.8864 s / it)
2025-04-28 16:57:27 Averaged stats: lr: 0.000231  min_lr: 0.000000  loss: 0.9512 (0.9735)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1477 (2.1515)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.2026  data: 2.7979  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3455  data: 0.9328  max mem: 39406
Test: Total time: 0:00:04 (1.3934 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8083 Acc: 0.9026 Recall_macro: 0.8083 Recall_weighted: 0.9026 AUC-ROC: 0.9658 Weighted F1-score: 0.9139
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 30, 'Val Loss': 0.381473571062088, 'Val BAcc': np.float64(0.8082604780349142), 'Val Acc': 0.9026086956521739, 'Val ROC': np.float64(0.9658208077142371), 'Val W_F1': 0.913882571941868, 'Val Recall_macro': 0.8082604780349142, 'Val Recall_weighted': 0.9026086956521739}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [31]  [ 0/64]  eta: 0:04:08  lr: 0.000231  min_lr: 0.000000  loss: 1.4183 (1.4183)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7510 (2.7510)  time: 3.8889  data: 3.0685  max mem: 39406
Epoch: [31]  [10/64]  eta: 0:00:59  lr: 0.000228  min_lr: 0.000000  loss: 1.0317 (1.0114)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2827 (2.2963)  time: 1.1078  data: 0.2794  max mem: 39406
Epoch: [31]  [20/64]  eta: 0:00:42  lr: 0.000225  min_lr: 0.000000  loss: 1.1257 (1.0637)  loss_scale: 65536.0000 (84260.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2351 (2.1939)  time: 0.8316  data: 0.0005  max mem: 39406
Epoch: [31]  [30/64]  eta: 0:00:31  lr: 0.000222  min_lr: 0.000000  loss: 1.1512 (1.0806)  loss_scale: 131072.0000 (99361.0323)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0053 (2.1919)  time: 0.8351  data: 0.0005  max mem: 39406
Epoch: [31]  [40/64]  eta: 0:00:21  lr: 0.000219  min_lr: 0.000000  loss: 1.1626 (1.0826)  loss_scale: 131072.0000 (107095.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1898 (2.2031)  time: 0.8377  data: 0.0005  max mem: 39406
Epoch: [31]  [50/64]  eta: 0:00:12  lr: 0.000216  min_lr: 0.000000  loss: 1.1626 (1.0823)  loss_scale: 131072.0000 (111796.7059)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2057 (2.1974)  time: 0.8395  data: 0.0004  max mem: 39406
Epoch: [31]  [60/64]  eta: 0:00:03  lr: 0.000213  min_lr: 0.000000  loss: 1.1491 (1.0880)  loss_scale: 131072.0000 (114956.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1543 (2.2047)  time: 0.8412  data: 0.0002  max mem: 39406
Epoch: [31]  [63/64]  eta: 0:00:00  lr: 0.000212  min_lr: 0.000000  loss: 1.1372 (1.0850)  loss_scale: 131072.0000 (115712.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1849 (2.2075)  time: 0.8409  data: 0.0002  max mem: 39406
Epoch: [31] Total time: 0:00:56 (0.8872 s / it)
2025-04-28 16:58:28 Averaged stats: lr: 0.000212  min_lr: 0.000000  loss: 1.1372 (1.0850)  loss_scale: 131072.0000 (115712.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1849 (2.2075)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.3019  data: 2.9017  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3826  data: 0.9674  max mem: 39406
Test: Total time: 0:00:04 (1.4402 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7955 Acc: 0.9235 Recall_macro: 0.7955 Recall_weighted: 0.9235 AUC-ROC: 0.9779 Weighted F1-score: 0.9282
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 31, 'Val Loss': 0.31347784399986267, 'Val BAcc': np.float64(0.7954739663010338), 'Val Acc': 0.9234782608695652, 'Val ROC': np.float64(0.9778779467022317), 'Val W_F1': 0.928192149361346, 'Val Recall_macro': 0.7954739663010338, 'Val Recall_weighted': 0.9234782608695652}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [32]  [ 0/64]  eta: 0:04:21  lr: 0.000211  min_lr: 0.000000  loss: 0.9556 (0.9556)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0955 (2.0955)  time: 4.0828  data: 3.2657  max mem: 39406
Epoch: [32]  [10/64]  eta: 0:01:00  lr: 0.000208  min_lr: 0.000000  loss: 0.9855 (1.0221)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8002 (1.8922)  time: 1.1261  data: 0.2974  max mem: 39406
Epoch: [32]  [20/64]  eta: 0:00:43  lr: 0.000205  min_lr: 0.000000  loss: 0.9930 (1.0205)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8002 (1.9887)  time: 0.8323  data: 0.0005  max mem: 39406
Epoch: [32]  [30/64]  eta: 0:00:31  lr: 0.000202  min_lr: 0.000000  loss: 1.0723 (1.0529)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9297 (2.0164)  time: 0.8356  data: 0.0005  max mem: 39406
Epoch: [32]  [40/64]  eta: 0:00:21  lr: 0.000199  min_lr: 0.000000  loss: 1.1646 (1.0558)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0001 (2.0399)  time: 0.8381  data: 0.0005  max mem: 39406
Epoch: [32]  [50/64]  eta: 0:00:12  lr: 0.000196  min_lr: 0.000000  loss: 1.1137 (1.0484)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2869 (2.1402)  time: 0.8393  data: 0.0003  max mem: 39406
Epoch: [32]  [60/64]  eta: 0:00:03  lr: 0.000193  min_lr: 0.000000  loss: 1.0891 (1.0478)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4118 (2.1426)  time: 0.8405  data: 0.0002  max mem: 39406
Epoch: [32]  [63/64]  eta: 0:00:00  lr: 0.000193  min_lr: 0.000000  loss: 1.0891 (1.0456)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3209 (2.1398)  time: 0.8408  data: 0.0001  max mem: 39406
Epoch: [32] Total time: 0:00:57 (0.8909 s / it)
2025-04-28 16:59:29 Averaged stats: lr: 0.000193  min_lr: 0.000000  loss: 1.0891 (1.0456)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3209 (2.1398)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0991  data: 2.6943  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3118  data: 0.8982  max mem: 39406
Test: Total time: 0:00:04 (1.3600 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7886 Acc: 0.9043 Recall_macro: 0.7886 Recall_weighted: 0.9043 AUC-ROC: 0.9736 Weighted F1-score: 0.9141
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 32, 'Val Loss': 0.33631712198257446, 'Val BAcc': np.float64(0.7886067983812345), 'Val Acc': 0.9043478260869565, 'Val ROC': np.float64(0.9736022511400615), 'Val W_F1': 0.9141239692884306, 'Val Recall_macro': 0.7886067983812345, 'Val Recall_weighted': 0.9043478260869565}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [33]  [ 0/64]  eta: 0:04:10  lr: 0.000192  min_lr: 0.000000  loss: 0.7072 (0.7072)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6005 (2.6005)  time: 3.9114  data: 3.0954  max mem: 39406
Epoch: [33]  [10/64]  eta: 0:00:59  lr: 0.000189  min_lr: 0.000000  loss: 1.1206 (1.0290)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0473 (1.9639)  time: 1.1105  data: 0.2818  max mem: 39406
Epoch: [33]  [20/64]  eta: 0:00:43  lr: 0.000186  min_lr: 0.000000  loss: 1.1432 (1.0814)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9055 (2.0441)  time: 0.8321  data: 0.0004  max mem: 39406
Epoch: [33]  [30/64]  eta: 0:00:31  lr: 0.000183  min_lr: 0.000000  loss: 1.1988 (1.0935)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2572 (2.1472)  time: 0.8355  data: 0.0004  max mem: 39406
Epoch: [33]  [40/64]  eta: 0:00:21  lr: 0.000180  min_lr: 0.000000  loss: 1.0847 (1.0800)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1018 (2.1035)  time: 0.8380  data: 0.0004  max mem: 39406
Epoch: [33]  [50/64]  eta: 0:00:12  lr: 0.000177  min_lr: 0.000000  loss: 1.1664 (1.0875)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1018 (2.1792)  time: 0.8396  data: 0.0003  max mem: 39406
Epoch: [33]  [60/64]  eta: 0:00:03  lr: 0.000175  min_lr: 0.000000  loss: 1.1498 (1.0789)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2116 (2.1546)  time: 0.8410  data: 0.0002  max mem: 39406
Epoch: [33]  [63/64]  eta: 0:00:00  lr: 0.000174  min_lr: 0.000000  loss: 1.0556 (1.0768)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1500 (2.1415)  time: 0.8411  data: 0.0001  max mem: 39406
Epoch: [33] Total time: 0:00:56 (0.8878 s / it)
2025-04-28 17:00:30 Averaged stats: lr: 0.000174  min_lr: 0.000000  loss: 1.0556 (1.0768)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1500 (2.1415)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1118  data: 2.7090  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3164  data: 0.9031  max mem: 39406
Test: Total time: 0:00:04 (1.3646 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7836 Acc: 0.8939 Recall_macro: 0.7836 Recall_weighted: 0.8939 AUC-ROC: 0.9516 Weighted F1-score: 0.9067
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 33, 'Val Loss': 0.38247376680374146, 'Val BAcc': np.float64(0.783554774381842), 'Val Acc': 0.8939130434782608, 'Val ROC': np.float64(0.9515576579058517), 'Val W_F1': 0.906748339117591, 'Val Recall_macro': 0.783554774381842, 'Val Recall_weighted': 0.8939130434782608}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [34]  [ 0/64]  eta: 0:04:00  lr: 0.000173  min_lr: 0.000000  loss: 1.1009 (1.1009)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1402 (2.1402)  time: 3.7518  data: 2.9337  max mem: 39406
Epoch: [34]  [10/64]  eta: 0:00:59  lr: 0.000170  min_lr: 0.000000  loss: 1.1590 (1.0963)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1402 (2.2353)  time: 1.0960  data: 0.2671  max mem: 39406
Epoch: [34]  [20/64]  eta: 0:00:42  lr: 0.000168  min_lr: 0.000000  loss: 1.1442 (1.0671)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0980 (2.2501)  time: 0.8321  data: 0.0004  max mem: 39406
Epoch: [34]  [30/64]  eta: 0:00:31  lr: 0.000165  min_lr: 0.000000  loss: 1.0031 (1.0399)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1976 (2.2858)  time: 0.8350  data: 0.0004  max mem: 39406
Epoch: [34]  [40/64]  eta: 0:00:21  lr: 0.000162  min_lr: 0.000000  loss: 1.0643 (1.0458)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2340 (2.3194)  time: 0.8377  data: 0.0004  max mem: 39406
Epoch: [34]  [50/64]  eta: 0:00:12  lr: 0.000159  min_lr: 0.000000  loss: 0.9472 (1.0164)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0435 (2.2342)  time: 0.8394  data: 0.0003  max mem: 39406
Epoch: [34]  [60/64]  eta: 0:00:03  lr: 0.000156  min_lr: 0.000000  loss: 0.9472 (1.0058)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8461 (2.1710)  time: 0.8410  data: 0.0002  max mem: 39406
Epoch: [34]  [63/64]  eta: 0:00:00  lr: 0.000155  min_lr: 0.000000  loss: 0.9472 (1.0057)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8461 (2.1605)  time: 0.8417  data: 0.0001  max mem: 39406
Epoch: [34] Total time: 0:00:56 (0.8853 s / it)
2025-04-28 17:01:31 Averaged stats: lr: 0.000155  min_lr: 0.000000  loss: 0.9472 (1.0057)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8461 (2.1605)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0421  data: 2.6434  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.2937  data: 0.8812  max mem: 39406
Test: Total time: 0:00:04 (1.3456 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7922 Acc: 0.9043 Recall_macro: 0.7922 Recall_weighted: 0.9043 AUC-ROC: 0.9716 Weighted F1-score: 0.9129
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 34, 'Val Loss': 0.3424471616744995, 'Val BAcc': np.float64(0.7922431620175981), 'Val Acc': 0.9043478260869565, 'Val ROC': np.float64(0.9715904520486329), 'Val W_F1': 0.9128649082389816, 'Val Recall_macro': 0.7922431620175981, 'Val Recall_weighted': 0.9043478260869565}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [35]  [ 0/64]  eta: 0:03:55  lr: 0.000155  min_lr: 0.000000  loss: 1.0603 (1.0603)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8409 (1.8409)  time: 3.6734  data: 2.8543  max mem: 39406
Epoch: [35]  [10/64]  eta: 0:00:58  lr: 0.000152  min_lr: 0.000000  loss: 0.9196 (0.9569)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8409 (1.9019)  time: 1.0889  data: 0.2598  max mem: 39406
Epoch: [35]  [20/64]  eta: 0:00:42  lr: 0.000149  min_lr: 0.000000  loss: 0.8544 (0.9852)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7265 (1.9030)  time: 0.8330  data: 0.0004  max mem: 39406
Epoch: [35]  [30/64]  eta: 0:00:31  lr: 0.000147  min_lr: 0.000000  loss: 1.0499 (1.0081)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8180 (1.9504)  time: 0.8368  data: 0.0004  max mem: 39406
Epoch: [35]  [40/64]  eta: 0:00:21  lr: 0.000144  min_lr: 0.000000  loss: 1.0842 (1.0382)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0063 (1.9643)  time: 0.8391  data: 0.0004  max mem: 39406
Epoch: [35]  [50/64]  eta: 0:00:12  lr: 0.000141  min_lr: 0.000000  loss: 1.0773 (1.0301)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0915 (1.9759)  time: 0.8403  data: 0.0004  max mem: 39406
Epoch: [35]  [60/64]  eta: 0:00:03  lr: 0.000138  min_lr: 0.000000  loss: 0.9816 (1.0181)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1845 (2.0433)  time: 0.8418  data: 0.0002  max mem: 39406
Epoch: [35]  [63/64]  eta: 0:00:00  lr: 0.000138  min_lr: 0.000000  loss: 1.0390 (1.0171)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2139 (2.0606)  time: 0.8425  data: 0.0001  max mem: 39406
Epoch: [35] Total time: 0:00:56 (0.8854 s / it)
2025-04-28 17:02:32 Averaged stats: lr: 0.000138  min_lr: 0.000000  loss: 1.0390 (1.0171)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2139 (2.0606)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0190  data: 2.6229  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.2863  data: 0.8744  max mem: 39406
Test: Total time: 0:00:04 (1.3406 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7892 Acc: 0.9113 Recall_macro: 0.7892 Recall_weighted: 0.9113 AUC-ROC: 0.9644 Weighted F1-score: 0.9162
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 35, 'Val Loss': 0.2954983413219452, 'Val BAcc': np.float64(0.7891976694984212), 'Val Acc': 0.9113043478260869, 'Val ROC': np.float64(0.9644382121273566), 'Val W_F1': 0.9161749576906396, 'Val Recall_macro': 0.7891976694984212, 'Val Recall_weighted': 0.9113043478260869}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [36]  [ 0/64]  eta: 0:03:58  lr: 0.000137  min_lr: 0.000000  loss: 0.7062 (0.7062)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6105 (2.6105)  time: 3.7297  data: 2.9081  max mem: 39406
Epoch: [36]  [10/64]  eta: 0:00:59  lr: 0.000135  min_lr: 0.000000  loss: 1.0676 (0.9967)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5323 (2.3224)  time: 1.0956  data: 0.2648  max mem: 39406
Epoch: [36]  [20/64]  eta: 0:00:42  lr: 0.000132  min_lr: 0.000000  loss: 1.1082 (1.0638)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2505 (2.2498)  time: 0.8339  data: 0.0004  max mem: 39406
Epoch: [36]  [30/64]  eta: 0:00:31  lr: 0.000129  min_lr: 0.000000  loss: 1.1470 (1.0755)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3168 (2.2959)  time: 0.8368  data: 0.0004  max mem: 39406
Epoch: [36]  [40/64]  eta: 0:00:21  lr: 0.000126  min_lr: 0.000000  loss: 1.0877 (1.0610)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3380 (2.2883)  time: 0.8390  data: 0.0004  max mem: 39406
Epoch: [36]  [50/64]  eta: 0:00:12  lr: 0.000124  min_lr: 0.000000  loss: 1.0370 (1.0518)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2291 (2.3014)  time: 0.8404  data: 0.0003  max mem: 39406
Epoch: [36]  [60/64]  eta: 0:00:03  lr: 0.000121  min_lr: 0.000000  loss: 1.0032 (1.0394)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0285 (2.2406)  time: 0.8422  data: 0.0002  max mem: 39406
Epoch: [36]  [63/64]  eta: 0:00:00  lr: 0.000120  min_lr: 0.000000  loss: 0.9765 (1.0325)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9300 (2.2211)  time: 0.8427  data: 0.0001  max mem: 39406
Epoch: [36] Total time: 0:00:56 (0.8863 s / it)
2025-04-28 17:03:33 Averaged stats: lr: 0.000120  min_lr: 0.000000  loss: 0.9765 (1.0325)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9300 (2.2211)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1329  data: 2.7359  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3255  data: 0.9121  max mem: 39406
Test: Total time: 0:00:04 (1.3779 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7975 Acc: 0.9148 Recall_macro: 0.7975 Recall_weighted: 0.9148 AUC-ROC: 0.9663 Weighted F1-score: 0.9208
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 36, 'Val Loss': 0.3134743273258209, 'Val BAcc': np.float64(0.7975230283500959), 'Val Acc': 0.9147826086956522, 'Val ROC': np.float64(0.9663357872909142), 'Val W_F1': 0.9207955843346305, 'Val Recall_macro': 0.7975230283500959, 'Val Recall_weighted': 0.9147826086956522}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [37]  [ 0/64]  eta: 0:04:03  lr: 0.000120  min_lr: 0.000000  loss: 0.6733 (0.6733)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.2792 (1.2792)  time: 3.8115  data: 2.9939  max mem: 39406
Epoch: [37]  [10/64]  eta: 0:00:59  lr: 0.000118  min_lr: 0.000000  loss: 1.0788 (1.0067)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8350 (1.9478)  time: 1.1023  data: 0.2725  max mem: 39406
Epoch: [37]  [20/64]  eta: 0:00:42  lr: 0.000115  min_lr: 0.000000  loss: 1.1305 (1.0205)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0204 (2.1579)  time: 0.8332  data: 0.0004  max mem: 39406
Epoch: [37]  [30/64]  eta: 0:00:31  lr: 0.000112  min_lr: 0.000000  loss: 1.0351 (1.0062)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0204 (2.1381)  time: 0.8361  data: 0.0004  max mem: 39406
Epoch: [37]  [40/64]  eta: 0:00:21  lr: 0.000110  min_lr: 0.000000  loss: 1.1527 (1.0589)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1346 (2.1684)  time: 0.8384  data: 0.0004  max mem: 39406
Epoch: [37]  [50/64]  eta: 0:00:12  lr: 0.000107  min_lr: 0.000000  loss: 0.9553 (1.0170)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1053 (2.1234)  time: 0.8396  data: 0.0003  max mem: 39406
Epoch: [37]  [60/64]  eta: 0:00:03  lr: 0.000105  min_lr: 0.000000  loss: 0.9334 (1.0234)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7974 (2.0989)  time: 0.8415  data: 0.0001  max mem: 39406
Epoch: [37]  [63/64]  eta: 0:00:00  lr: 0.000104  min_lr: 0.000000  loss: 1.0471 (1.0249)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0865 (2.1003)  time: 0.8424  data: 0.0001  max mem: 39406
Epoch: [37] Total time: 0:00:56 (0.8870 s / it)
2025-04-28 17:04:34 Averaged stats: lr: 0.000104  min_lr: 0.000000  loss: 1.0471 (1.0249)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0865 (2.1003)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1028  data: 2.7037  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3141  data: 0.9014  max mem: 39406
Test: Total time: 0:00:04 (1.3712 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8087 Acc: 0.8852 Recall_macro: 0.8087 Recall_weighted: 0.8852 AUC-ROC: 0.9540 Weighted F1-score: 0.9004
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 37, 'Val Loss': 0.3877995014190674, 'Val BAcc': np.float64(0.8086508478989681), 'Val Acc': 0.8852173913043478, 'Val ROC': np.float64(0.9539810398807788), 'Val W_F1': 0.9003915520863354, 'Val Recall_macro': 0.8086508478989681, 'Val Recall_weighted': 0.8852173913043478}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [38]  [ 0/64]  eta: 0:03:59  lr: 0.000104  min_lr: 0.000000  loss: 1.0556 (1.0556)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4966 (1.4966)  time: 3.7384  data: 2.9165  max mem: 39406
Epoch: [38]  [10/64]  eta: 0:00:59  lr: 0.000101  min_lr: 0.000000  loss: 1.0952 (1.0556)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8764 (1.8732)  time: 1.0967  data: 0.2655  max mem: 39406
Epoch: [38]  [20/64]  eta: 0:00:42  lr: 0.000099  min_lr: 0.000000  loss: 1.1417 (1.1047)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1002 (2.0020)  time: 0.8345  data: 0.0004  max mem: 39406
Epoch: [38]  [30/64]  eta: 0:00:31  lr: 0.000097  min_lr: 0.000000  loss: 1.1907 (1.0562)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2710 (2.0786)  time: 0.8375  data: 0.0004  max mem: 39406
Epoch: [38]  [40/64]  eta: 0:00:21  lr: 0.000094  min_lr: 0.000000  loss: 0.9051 (1.0435)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1057 (2.0499)  time: 0.8393  data: 0.0004  max mem: 39406
Epoch: [38]  [50/64]  eta: 0:00:12  lr: 0.000092  min_lr: 0.000000  loss: 1.0350 (1.0307)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8525 (2.0058)  time: 0.8415  data: 0.0003  max mem: 39406
Epoch: [38]  [60/64]  eta: 0:00:03  lr: 0.000089  min_lr: 0.000000  loss: 1.0350 (1.0412)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0525 (2.0205)  time: 0.8437  data: 0.0001  max mem: 39406
Epoch: [38]  [63/64]  eta: 0:00:00  lr: 0.000089  min_lr: 0.000000  loss: 1.0321 (1.0352)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0590 (2.0298)  time: 0.8439  data: 0.0001  max mem: 39406
Epoch: [38] Total time: 0:00:56 (0.8873 s / it)
2025-04-28 17:05:34 Averaged stats: lr: 0.000089  min_lr: 0.000000  loss: 1.0321 (1.0352)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0590 (2.0298)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1420  data: 2.7449  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3277  data: 0.9151  max mem: 39406
Test: Total time: 0:00:04 (1.3879 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8036 Acc: 0.8991 Recall_macro: 0.8036 Recall_weighted: 0.8991 AUC-ROC: 0.9613 Weighted F1-score: 0.9106
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 38, 'Val Loss': 0.3543621301651001, 'Val BAcc': np.float64(0.8035714828196031), 'Val Acc': 0.8991304347826087, 'Val ROC': np.float64(0.9612660447372656), 'Val W_F1': 0.9106164908644094, 'Val Recall_macro': 0.8035714828196031, 'Val Recall_weighted': 0.8991304347826087}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [39]  [ 0/64]  eta: 0:03:57  lr: 0.000088  min_lr: 0.000000  loss: 0.8361 (0.8361)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7878 (1.7878)  time: 3.7041  data: 2.8822  max mem: 39406
Epoch: [39]  [10/64]  eta: 0:00:59  lr: 0.000086  min_lr: 0.000000  loss: 0.8491 (0.9443)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9197 (2.1440)  time: 1.0926  data: 0.2624  max mem: 39406
Epoch: [39]  [20/64]  eta: 0:00:42  lr: 0.000084  min_lr: 0.000000  loss: 1.0390 (1.0098)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1157 (2.1700)  time: 0.8335  data: 0.0004  max mem: 39406
Epoch: [39]  [30/64]  eta: 0:00:31  lr: 0.000082  min_lr: 0.000000  loss: 1.0870 (1.0299)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3888 (2.1949)  time: 0.8371  data: 0.0004  max mem: 39406
Epoch: [39]  [40/64]  eta: 0:00:21  lr: 0.000079  min_lr: 0.000000  loss: 1.0870 (1.0310)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1757 (2.1081)  time: 0.8394  data: 0.0004  max mem: 39406
Epoch: [39]  [50/64]  eta: 0:00:12  lr: 0.000077  min_lr: 0.000000  loss: 0.9830 (1.0244)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8156 (2.1043)  time: 0.8411  data: 0.0003  max mem: 39406
Epoch: [39]  [60/64]  eta: 0:00:03  lr: 0.000075  min_lr: 0.000000  loss: 1.1343 (1.0499)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1539 (2.1441)  time: 0.8431  data: 0.0002  max mem: 39406
Epoch: [39]  [63/64]  eta: 0:00:00  lr: 0.000074  min_lr: 0.000000  loss: 1.1771 (1.0605)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1675 (2.1587)  time: 0.8435  data: 0.0001  max mem: 39406
Epoch: [39] Total time: 0:00:56 (0.8862 s / it)
2025-04-28 17:06:35 Averaged stats: lr: 0.000074  min_lr: 0.000000  loss: 1.1771 (1.0605)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1675 (2.1587)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0955  data: 2.7001  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3116  data: 0.9002  max mem: 39406
Test: Total time: 0:00:04 (1.3647 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8077 Acc: 0.9078 Recall_macro: 0.8077 Recall_weighted: 0.9078 AUC-ROC: 0.9626 Weighted F1-score: 0.9175
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 39, 'Val Loss': 0.3463904857635498, 'Val BAcc': np.float64(0.8076984669465872), 'Val Acc': 0.9078260869565218, 'Val ROC': np.float64(0.962640245691719), 'Val W_F1': 0.9175000218521661, 'Val Recall_macro': 0.8076984669465872, 'Val Recall_weighted': 0.9078260869565218}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [40]  [ 0/64]  eta: 0:04:30  lr: 0.000074  min_lr: 0.000000  loss: 1.2551 (1.2551)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9394 (1.9394)  time: 4.2305  data: 3.3295  max mem: 39406
Epoch: [40]  [10/64]  eta: 0:01:01  lr: 0.000072  min_lr: 0.000000  loss: 1.0644 (1.1331)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9394 (2.0819)  time: 1.1389  data: 0.3030  max mem: 39406
Epoch: [40]  [20/64]  eta: 0:00:43  lr: 0.000070  min_lr: 0.000000  loss: 1.0313 (1.0800)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9145 (2.0693)  time: 0.8319  data: 0.0004  max mem: 39406
Epoch: [40]  [30/64]  eta: 0:00:32  lr: 0.000068  min_lr: 0.000000  loss: 1.0621 (1.0601)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1874 (2.1364)  time: 0.8363  data: 0.0003  max mem: 39406
Epoch: [40]  [40/64]  eta: 0:00:22  lr: 0.000066  min_lr: 0.000000  loss: 1.0621 (1.0628)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1880 (2.1840)  time: 0.8392  data: 0.0004  max mem: 39406
Epoch: [40]  [50/64]  eta: 0:00:12  lr: 0.000064  min_lr: 0.000000  loss: 0.9807 (1.0310)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0381 (2.1016)  time: 0.8406  data: 0.0003  max mem: 39406
Epoch: [40]  [60/64]  eta: 0:00:03  lr: 0.000062  min_lr: 0.000000  loss: 0.9298 (1.0344)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8191 (2.0753)  time: 0.8419  data: 0.0002  max mem: 39406
Epoch: [40]  [63/64]  eta: 0:00:00  lr: 0.000061  min_lr: 0.000000  loss: 0.9298 (1.0281)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8191 (2.0727)  time: 0.8426  data: 0.0001  max mem: 39406
Epoch: [40] Total time: 0:00:57 (0.8941 s / it)
2025-04-28 17:07:37 Averaged stats: lr: 0.000061  min_lr: 0.000000  loss: 0.9298 (1.0281)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8191 (2.0727)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1006  data: 2.6980  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3139  data: 0.8995  max mem: 39406
Test: Total time: 0:00:04 (1.3654 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8153 Acc: 0.9217 Recall_macro: 0.8153 Recall_weighted: 0.9217 AUC-ROC: 0.9696 Weighted F1-score: 0.9261
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 40, 'Val Loss': 0.31768670678138733, 'Val BAcc': np.float64(0.8153175145656348), 'Val Acc': 0.9217391304347826, 'Val ROC': np.float64(0.9695520194955269), 'Val W_F1': 0.9261350541007691, 'Val Recall_macro': 0.8153175145656348, 'Val Recall_weighted': 0.9217391304347826}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [41]  [ 0/64]  eta: 0:03:49  lr: 0.000061  min_lr: 0.000000  loss: 1.0254 (1.0254)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5793 (1.5793)  time: 3.5797  data: 2.7632  max mem: 39406
Epoch: [41]  [10/64]  eta: 0:00:58  lr: 0.000059  min_lr: 0.000000  loss: 1.1344 (1.0834)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2559 (2.3192)  time: 1.0808  data: 0.2516  max mem: 39406
Epoch: [41]  [20/64]  eta: 0:00:42  lr: 0.000057  min_lr: 0.000000  loss: 1.1541 (1.1156)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2661 (2.3353)  time: 0.8332  data: 0.0004  max mem: 39406
Epoch: [41]  [30/64]  eta: 0:00:31  lr: 0.000055  min_lr: 0.000000  loss: 1.1506 (1.0547)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3850 (2.2986)  time: 0.8366  data: 0.0004  max mem: 39406
Epoch: [41]  [40/64]  eta: 0:00:21  lr: 0.000053  min_lr: 0.000000  loss: 1.1239 (1.0644)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0343 (2.2734)  time: 0.8389  data: 0.0004  max mem: 39406
Epoch: [41]  [50/64]  eta: 0:00:12  lr: 0.000051  min_lr: 0.000000  loss: 1.1239 (1.0626)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0333 (2.2122)  time: 0.8412  data: 0.0003  max mem: 39406
Epoch: [41]  [60/64]  eta: 0:00:03  lr: 0.000049  min_lr: 0.000000  loss: 1.1306 (1.0620)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2368 (2.2347)  time: 0.8420  data: 0.0001  max mem: 39406
Epoch: [41]  [63/64]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000000  loss: 1.1821 (1.0717)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4146 (2.2534)  time: 0.8419  data: 0.0001  max mem: 39406
Epoch: [41] Total time: 0:00:56 (0.8836 s / it)
2025-04-28 17:08:38 Averaged stats: lr: 0.000049  min_lr: 0.000000  loss: 1.1821 (1.0717)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4146 (2.2534)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1826  data: 2.7803  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3410  data: 0.9269  max mem: 39406
Test: Total time: 0:00:04 (1.3940 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8090 Acc: 0.9148 Recall_macro: 0.8090 Recall_weighted: 0.9148 AUC-ROC: 0.9656 Weighted F1-score: 0.9213
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 41, 'Val Loss': 0.33293813467025757, 'Val BAcc': np.float64(0.8089683082164284), 'Val Acc': 0.9147826086956522, 'Val ROC': np.float64(0.9656356323202118), 'Val W_F1': 0.9213053292742254, 'Val Recall_macro': 0.8089683082164284, 'Val Recall_weighted': 0.9147826086956522}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [42]  [ 0/64]  eta: 0:03:50  lr: 0.000049  min_lr: 0.000000  loss: 1.2402 (1.2402)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8068 (2.8068)  time: 3.6067  data: 2.7865  max mem: 39406
Epoch: [42]  [10/64]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 1.1517 (1.1003)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1412 (2.0620)  time: 1.0839  data: 0.2537  max mem: 39406
Epoch: [42]  [20/64]  eta: 0:00:42  lr: 0.000045  min_lr: 0.000000  loss: 1.1013 (1.1095)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1412 (2.1263)  time: 0.8337  data: 0.0004  max mem: 39406
Epoch: [42]  [30/64]  eta: 0:00:31  lr: 0.000043  min_lr: 0.000000  loss: 1.0944 (1.0973)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0916 (2.0886)  time: 0.8377  data: 0.0005  max mem: 39406
Epoch: [42]  [40/64]  eta: 0:00:21  lr: 0.000042  min_lr: 0.000000  loss: 1.0944 (1.0940)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0916 (2.1038)  time: 0.8402  data: 0.0006  max mem: 39406
Epoch: [42]  [50/64]  eta: 0:00:12  lr: 0.000040  min_lr: 0.000000  loss: 1.0397 (1.0552)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0200 (2.0838)  time: 0.8410  data: 0.0004  max mem: 39406
Epoch: [42]  [60/64]  eta: 0:00:03  lr: 0.000038  min_lr: 0.000000  loss: 0.9456 (1.0411)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9281 (2.0555)  time: 0.8417  data: 0.0002  max mem: 39406
Epoch: [42]  [63/64]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000000  loss: 0.9823 (1.0391)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9281 (2.0339)  time: 0.8420  data: 0.0001  max mem: 39406
Epoch: [42] Total time: 0:00:56 (0.8846 s / it)
2025-04-28 17:09:38 Averaged stats: lr: 0.000038  min_lr: 0.000000  loss: 0.9823 (1.0391)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9281 (2.0339)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0693  data: 2.6653  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3011  data: 0.8886  max mem: 39406
Test: Total time: 0:00:04 (1.3576 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8084 Acc: 0.9096 Recall_macro: 0.8084 Recall_weighted: 0.9096 AUC-ROC: 0.9626 Weighted F1-score: 0.9175
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 42, 'Val Loss': 0.33471646904945374, 'Val BAcc': np.float64(0.8084336382080742), 'Val Acc': 0.9095652173913044, 'Val ROC': np.float64(0.9625563055529254), 'Val W_F1': 0.9174684930664105, 'Val Recall_macro': 0.8084336382080742, 'Val Recall_weighted': 0.9095652173913044}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [43]  [ 0/64]  eta: 0:03:56  lr: 0.000038  min_lr: 0.000000  loss: 0.6598 (0.6598)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4735 (2.4735)  time: 3.6892  data: 2.8744  max mem: 39406
Epoch: [43]  [10/64]  eta: 0:00:58  lr: 0.000036  min_lr: 0.000000  loss: 1.0838 (0.9719)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2377 (2.1482)  time: 1.0917  data: 0.2617  max mem: 39406
Epoch: [43]  [20/64]  eta: 0:00:42  lr: 0.000035  min_lr: 0.000000  loss: 1.1692 (1.0469)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3057 (2.2967)  time: 0.8333  data: 0.0004  max mem: 39406
Epoch: [43]  [30/64]  eta: 0:00:31  lr: 0.000033  min_lr: 0.000000  loss: 1.1371 (1.0312)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3647 (2.2185)  time: 0.8362  data: 0.0004  max mem: 39406
Epoch: [43]  [40/64]  eta: 0:00:21  lr: 0.000032  min_lr: 0.000000  loss: 0.9261 (0.9908)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7382 (2.1235)  time: 0.8387  data: 0.0006  max mem: 39406
Epoch: [43]  [50/64]  eta: 0:00:12  lr: 0.000030  min_lr: 0.000000  loss: 0.8789 (0.9934)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0571 (2.0950)  time: 0.8407  data: 0.0005  max mem: 39406
Epoch: [43]  [60/64]  eta: 0:00:03  lr: 0.000029  min_lr: 0.000000  loss: 1.1089 (1.0007)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9348 (2.0896)  time: 0.8414  data: 0.0003  max mem: 39406
Epoch: [43]  [63/64]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000000  loss: 1.1131 (1.0064)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0834 (2.1152)  time: 0.8414  data: 0.0001  max mem: 39406
Epoch: [43] Total time: 0:00:56 (0.8854 s / it)
2025-04-28 17:10:39 Averaged stats: lr: 0.000028  min_lr: 0.000000  loss: 1.1131 (1.0064)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0834 (2.1152)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1118  data: 2.7091  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3171  data: 0.9032  max mem: 39406
Test: Total time: 0:00:04 (1.3699 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8007 Acc: 0.9148 Recall_macro: 0.8007 Recall_weighted: 0.9148 AUC-ROC: 0.9678 Weighted F1-score: 0.9210
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 43, 'Val Loss': 0.32036906480789185, 'Val BAcc': np.float64(0.8006702904447265), 'Val Acc': 0.9147826086956522, 'Val ROC': np.float64(0.9678203807110533), 'Val W_F1': 0.9209726833151914, 'Val Recall_macro': 0.8006702904447265, 'Val Recall_weighted': 0.9147826086956522}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [44]  [ 0/64]  eta: 0:03:58  lr: 0.000028  min_lr: 0.000000  loss: 1.3271 (1.3271)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7808 (2.7808)  time: 3.7320  data: 2.9074  max mem: 39406
Epoch: [44]  [10/64]  eta: 0:00:59  lr: 0.000027  min_lr: 0.000000  loss: 1.0230 (0.9630)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1381 (2.0304)  time: 1.0958  data: 0.2648  max mem: 39406
Epoch: [44]  [20/64]  eta: 0:00:42  lr: 0.000025  min_lr: 0.000000  loss: 1.0230 (0.9963)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0906 (2.0494)  time: 0.8340  data: 0.0006  max mem: 39406
Epoch: [44]  [30/64]  eta: 0:00:31  lr: 0.000024  min_lr: 0.000000  loss: 0.9574 (0.9670)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8322 (1.9735)  time: 0.8366  data: 0.0006  max mem: 39406
Epoch: [44]  [40/64]  eta: 0:00:21  lr: 0.000023  min_lr: 0.000000  loss: 0.8766 (0.9539)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7146 (1.9286)  time: 0.8388  data: 0.0004  max mem: 39406
Epoch: [44]  [50/64]  eta: 0:00:12  lr: 0.000022  min_lr: 0.000000  loss: 0.8490 (0.9335)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7146 (1.9568)  time: 0.8405  data: 0.0003  max mem: 39406
Epoch: [44]  [60/64]  eta: 0:00:03  lr: 0.000020  min_lr: 0.000000  loss: 0.9289 (0.9487)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8826 (1.9653)  time: 0.8418  data: 0.0002  max mem: 39406
Epoch: [44]  [63/64]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.0214 (0.9407)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8826 (1.9479)  time: 0.8423  data: 0.0001  max mem: 39406
Epoch: [44] Total time: 0:00:56 (0.8865 s / it)
2025-04-28 17:11:40 Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.0214 (0.9407)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8826 (1.9479)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0554  data: 2.6522  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.2981  data: 0.8842  max mem: 39406
Test: Total time: 0:00:04 (1.3451 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8062 Acc: 0.9113 Recall_macro: 0.8062 Recall_weighted: 0.9113 AUC-ROC: 0.9688 Weighted F1-score: 0.9199
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 44, 'Val Loss': 0.32625752687454224, 'Val BAcc': np.float64(0.8062114159858521), 'Val Acc': 0.9113043478260869, 'Val ROC': np.float64(0.9687995885059391), 'Val W_F1': 0.9199445955115012, 'Val Recall_macro': 0.8062114159858521, 'Val Recall_weighted': 0.9113043478260869}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [45]  [ 0/64]  eta: 0:03:49  lr: 0.000020  min_lr: 0.000000  loss: 1.0633 (1.0633)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9708 (1.9708)  time: 3.5927  data: 2.7733  max mem: 39406
Epoch: [45]  [10/64]  eta: 0:00:58  lr: 0.000019  min_lr: 0.000000  loss: 0.8826 (0.9267)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2151 (2.0763)  time: 1.0831  data: 0.2525  max mem: 39406
Epoch: [45]  [20/64]  eta: 0:00:42  lr: 0.000018  min_lr: 0.000000  loss: 1.0195 (1.0033)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1753 (2.1458)  time: 0.8349  data: 0.0004  max mem: 39406
Epoch: [45]  [30/64]  eta: 0:00:31  lr: 0.000017  min_lr: 0.000000  loss: 1.0811 (0.9665)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9848 (2.0569)  time: 0.8378  data: 0.0004  max mem: 39406
Epoch: [45]  [40/64]  eta: 0:00:21  lr: 0.000016  min_lr: 0.000000  loss: 0.9396 (0.9817)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8751 (2.0563)  time: 0.8387  data: 0.0004  max mem: 39406
Epoch: [45]  [50/64]  eta: 0:00:12  lr: 0.000015  min_lr: 0.000000  loss: 1.1490 (1.0097)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0555 (2.0537)  time: 0.8399  data: 0.0003  max mem: 39406
Epoch: [45]  [60/64]  eta: 0:00:03  lr: 0.000014  min_lr: 0.000000  loss: 1.1542 (1.0302)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0310 (2.0389)  time: 0.8413  data: 0.0001  max mem: 39406
Epoch: [45]  [63/64]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.1490 (1.0260)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0310 (2.0523)  time: 0.8419  data: 0.0001  max mem: 39406
Epoch: [45] Total time: 0:00:56 (0.8843 s / it)
2025-04-28 17:12:41 Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.1490 (1.0260)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0310 (2.0523)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.3436  data: 2.9406  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3932  data: 0.9803  max mem: 39406
Test: Total time: 0:00:04 (1.4458 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8062 Acc: 0.9113 Recall_macro: 0.8062 Recall_weighted: 0.9113 AUC-ROC: 0.9658 Weighted F1-score: 0.9195
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 45, 'Val Loss': 0.3342256247997284, 'Val BAcc': np.float64(0.8062114159858521), 'Val Acc': 0.9113043478260869, 'Val ROC': np.float64(0.9657752579381444), 'Val W_F1': 0.9195075797304783, 'Val Recall_macro': 0.8062114159858521, 'Val Recall_weighted': 0.9113043478260869}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [46]  [ 0/64]  eta: 0:03:53  lr: 0.000013  min_lr: 0.000000  loss: 1.0989 (1.0989)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4306 (2.4306)  time: 3.6562  data: 2.8341  max mem: 39406
Epoch: [46]  [10/64]  eta: 0:00:58  lr: 0.000012  min_lr: 0.000000  loss: 1.0160 (0.9363)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4085 (2.1682)  time: 1.0869  data: 0.2580  max mem: 39406
Epoch: [46]  [20/64]  eta: 0:00:42  lr: 0.000011  min_lr: 0.000000  loss: 0.9667 (0.9570)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6735 (1.9632)  time: 0.8323  data: 0.0004  max mem: 39406
Epoch: [46]  [30/64]  eta: 0:00:31  lr: 0.000011  min_lr: 0.000000  loss: 1.0626 (1.0020)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8029 (1.9527)  time: 0.8359  data: 0.0004  max mem: 39406
Epoch: [46]  [40/64]  eta: 0:00:21  lr: 0.000010  min_lr: 0.000000  loss: 1.0626 (0.9768)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9442 (1.9597)  time: 0.8375  data: 0.0004  max mem: 39406
Epoch: [46]  [50/64]  eta: 0:00:12  lr: 0.000009  min_lr: 0.000000  loss: 1.0493 (1.0047)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9508 (2.0215)  time: 0.8387  data: 0.0003  max mem: 39406
Epoch: [46]  [60/64]  eta: 0:00:03  lr: 0.000008  min_lr: 0.000000  loss: 1.0826 (1.0040)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9367 (1.9903)  time: 0.8407  data: 0.0002  max mem: 39406
Epoch: [46]  [63/64]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 0.9988 (0.9965)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8937 (1.9803)  time: 0.8413  data: 0.0001  max mem: 39406
Epoch: [46] Total time: 0:00:56 (0.8838 s / it)
2025-04-28 17:13:42 Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 0.9988 (0.9965)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8937 (1.9803)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.1759  data: 2.7763  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3378  data: 0.9255  max mem: 39406
Test: Total time: 0:00:04 (1.3962 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8072 Acc: 0.9165 Recall_macro: 0.8072 Recall_weighted: 0.9165 AUC-ROC: 0.9668 Weighted F1-score: 0.9236
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 46, 'Val Loss': 0.32801029086112976, 'Val BAcc': np.float64(0.807163796938233), 'Val Acc': 0.9165217391304348, 'Val ROC': np.float64(0.966770953014117), 'Val W_F1': 0.9236352901942964, 'Val Recall_macro': 0.807163796938233, 'Val Recall_weighted': 0.9165217391304348}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [47]  [ 0/64]  eta: 0:03:56  lr: 0.000008  min_lr: 0.000000  loss: 0.9458 (0.9458)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6379 (1.6379)  time: 3.7023  data: 2.8853  max mem: 39406
Epoch: [47]  [10/64]  eta: 0:00:58  lr: 0.000007  min_lr: 0.000000  loss: 1.1054 (1.0907)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3268 (2.1859)  time: 1.0916  data: 0.2626  max mem: 39406
Epoch: [47]  [20/64]  eta: 0:00:42  lr: 0.000007  min_lr: 0.000000  loss: 1.0489 (1.0102)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0999 (2.1237)  time: 0.8319  data: 0.0004  max mem: 39406
Epoch: [47]  [30/64]  eta: 0:00:31  lr: 0.000006  min_lr: 0.000000  loss: 1.0594 (1.0442)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9940 (2.1370)  time: 0.8350  data: 0.0004  max mem: 39406
Epoch: [47]  [40/64]  eta: 0:00:21  lr: 0.000005  min_lr: 0.000000  loss: 1.1107 (1.0165)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9129 (2.0769)  time: 0.8375  data: 0.0004  max mem: 39406
Epoch: [47]  [50/64]  eta: 0:00:12  lr: 0.000005  min_lr: 0.000000  loss: 1.0150 (1.0176)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9658 (2.0418)  time: 0.8395  data: 0.0003  max mem: 39406
Epoch: [47]  [60/64]  eta: 0:00:03  lr: 0.000004  min_lr: 0.000000  loss: 1.0147 (0.9991)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8181 (1.9847)  time: 0.8408  data: 0.0002  max mem: 39406
Epoch: [47]  [63/64]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 0.9936 (1.0035)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8122 (1.9810)  time: 0.8412  data: 0.0001  max mem: 39406
Epoch: [47] Total time: 0:00:56 (0.8846 s / it)
2025-04-28 17:14:43 Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 0.9936 (1.0035)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8122 (1.9810)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.2082  data: 2.8074  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3475  data: 0.9359  max mem: 39406
Test: Total time: 0:00:04 (1.3927 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8068 Acc: 0.9148 Recall_macro: 0.8068 Recall_weighted: 0.9148 AUC-ROC: 0.9660 Weighted F1-score: 0.9224
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 47, 'Val Loss': 0.3285870850086212, 'Val BAcc': np.float64(0.8068463366207727), 'Val Acc': 0.9147826086956522, 'Val ROC': np.float64(0.9660174579177234), 'Val W_F1': 0.9223977007591053, 'Val Recall_macro': 0.8068463366207727, 'Val Recall_weighted': 0.9147826086956522}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [48]  [ 0/64]  eta: 0:04:02  lr: 0.000004  min_lr: 0.000000  loss: 1.0299 (1.0299)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2671 (2.2671)  time: 3.7964  data: 2.9771  max mem: 39406
Epoch: [48]  [10/64]  eta: 0:00:59  lr: 0.000004  min_lr: 0.000000  loss: 1.0072 (0.9144)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2052 (2.1068)  time: 1.0992  data: 0.2710  max mem: 39406
Epoch: [48]  [20/64]  eta: 0:00:42  lr: 0.000003  min_lr: 0.000000  loss: 1.0072 (0.9636)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1392 (2.0825)  time: 0.8312  data: 0.0004  max mem: 39406
Epoch: [48]  [30/64]  eta: 0:00:31  lr: 0.000003  min_lr: 0.000000  loss: 1.0954 (0.9806)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1099 (2.0256)  time: 0.8347  data: 0.0004  max mem: 39406
Epoch: [48]  [40/64]  eta: 0:00:21  lr: 0.000002  min_lr: 0.000000  loss: 1.0792 (0.9936)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2066 (2.0799)  time: 0.8378  data: 0.0004  max mem: 39406
Epoch: [48]  [50/64]  eta: 0:00:12  lr: 0.000002  min_lr: 0.000000  loss: 1.0631 (1.0099)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1707 (2.0929)  time: 0.8395  data: 0.0003  max mem: 39406
Epoch: [48]  [60/64]  eta: 0:00:03  lr: 0.000002  min_lr: 0.000000  loss: 1.0631 (1.0152)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1707 (2.1177)  time: 0.8402  data: 0.0001  max mem: 39406
Epoch: [48]  [63/64]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.0527 (1.0152)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1208 (2.1076)  time: 0.8407  data: 0.0001  max mem: 39406
Epoch: [48] Total time: 0:00:56 (0.8856 s / it)
2025-04-28 17:15:43 Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.0527 (1.0152)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1208 (2.1076)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0592  data: 2.6613  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.2990  data: 0.8872  max mem: 39406
Test: Total time: 0:00:04 (1.3504 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8065 Acc: 0.9130 Recall_macro: 0.8065 Recall_weighted: 0.9130 AUC-ROC: 0.9658 Weighted F1-score: 0.9212
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 48, 'Val Loss': 0.32933229207992554, 'Val BAcc': np.float64(0.8065288763033124), 'Val Acc': 0.9130434782608695, 'Val ROC': np.float64(0.9657691903647304), 'Val W_F1': 0.9211676125118231, 'Val Recall_macro': 0.8065288763033124, 'Val Recall_weighted': 0.9130434782608695}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [49]  [ 0/64]  eta: 0:03:49  lr: 0.000002  min_lr: 0.000000  loss: 0.6128 (0.6128)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1662 (2.1662)  time: 3.5887  data: 2.7684  max mem: 39406
Epoch: [49]  [10/64]  eta: 0:00:58  lr: 0.000002  min_lr: 0.000000  loss: 0.8232 (0.8812)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4310 (1.6581)  time: 1.0806  data: 0.2521  max mem: 39406
Epoch: [49]  [20/64]  eta: 0:00:42  lr: 0.000001  min_lr: 0.000000  loss: 1.0072 (0.9419)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7341 (1.8403)  time: 0.8318  data: 0.0004  max mem: 39406
Epoch: [49]  [30/64]  eta: 0:00:31  lr: 0.000001  min_lr: 0.000000  loss: 1.0072 (0.9470)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0759 (1.9932)  time: 0.8351  data: 0.0004  max mem: 39406
Epoch: [49]  [40/64]  eta: 0:00:21  lr: 0.000001  min_lr: 0.000000  loss: 1.0452 (0.9638)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8960 (1.9511)  time: 0.8374  data: 0.0004  max mem: 39406
Epoch: [49]  [50/64]  eta: 0:00:12  lr: 0.000001  min_lr: 0.000000  loss: 1.0452 (0.9597)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9615 (2.0113)  time: 0.8394  data: 0.0003  max mem: 39406
Epoch: [49]  [60/64]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000000  loss: 1.0434 (0.9742)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2132 (2.0087)  time: 0.8411  data: 0.0002  max mem: 39406
Epoch: [49]  [63/64]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.0434 (0.9798)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1190 (2.0236)  time: 0.8416  data: 0.0001  max mem: 39406
Epoch: [49] Total time: 0:00:56 (0.8829 s / it)
2025-04-28 17:16:44 Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.0434 (0.9798)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1190 (2.0236)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.0376  data: 2.6396  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.2910  data: 0.8800  max mem: 39406
Test: Total time: 0:00:04 (1.3422 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8065 Acc: 0.9130 Recall_macro: 0.8065 Recall_weighted: 0.9130 AUC-ROC: 0.9655 Weighted F1-score: 0.9212
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/val.csv
-------------------------- {'Epoch': 49, 'Val Loss': 0.3293744623661041, 'Val BAcc': np.float64(0.8065288763033124), 'Val Acc': 0.9130434782608695, 'Val ROC': np.float64(0.9655170176381749), 'Val W_F1': 0.9211676125118231, 'Val Recall_macro': 0.8065288763033124, 'Val Recall_weighted': 0.9130434782608695}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:723: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_dict = torch.load(model_weight)
Starting test without tta
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [ 0/10]  eta: 0:00:28    time: 2.8887  data: 2.6409  max mem: 39406
Test:  [ 9/10]  eta: 0:00:00    time: 0.5476  data: 0.2642  max mem: 39406
Test: Total time: 0:00:05 (0.5704 s / it)
------------- test -------------
Sklearn Metrics - BAcc: 0.8325 Acc: 0.8539 Recall_macro: 0.8325 Recall_weighted: 0.8539 AUC-ROC: 0.9739 Weighted F1-score: 0.8672
{'balanced_accuracy': np.float64(0.832508282172398), 'accuracy': 0.8538961038961039, 'top3 accuracy': np.float64(0.9748376623376623), 'top5 accuracy': np.float64(0.9959415584415584), 'sensitivity': np.float64(0.832508282172398), 'specificity': np.float64(0.9745358153177908), 'auc_roc': np.float64(0.9739036318350232), 'weighted_f1': 0.867224272251137, 'recall_macro': 0.832508282172398, 'recall_weighted': 0.8538961038961039}
Predictions for test saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting2_large/test.csv
Training time 0:51:36
