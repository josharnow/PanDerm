Not using distributed mode
Namespace(mode='train', batch_size=128, epochs=50, update_freq=1, save_ckpt_freq=5, model='PanDerm_Large_FT', rel_pos_bias=True, sin_pos_emb=True, layer_scale_init_value=0.1, ood_eval=False, input_size=224, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, weights=True, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, percent_data=1.0, TTA=False, monitor='recall', opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.65, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=10, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', pretrained_checkpoint='/home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth', model_key='model|module|state_dict', model_prefix='', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, data_path='/datasets01/imagenet_full_size/061417/', eval_data_path=None, test_csv_path=None, image_key='image', nb_classes=7, imagenet_default_mean_and_std=True, data_set='IMNET', csv_path='/home/syyan/XJ/PanDerm-open_source/data/linear_probing/HAM-official-7-lp.csv', root_path='/home/share/Uni_Eval/ISIC2018_reader/images/', output_dir='/home/share/FM_Code/PanDerm/HAM_Res/', log_dir=None, device='cuda', seed=122, resume='', auto_resume=False, wandb_name='Reproduce_HAM_FT__122', save_ckpt=True, start_epoch=0, eval=False, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, enable_linear_eval=False, enable_multi_print=False, exp_name='ham finetune and eval', distributed=False)
Label distribution:
Label 0: 262
Label 1: 411
Label 2: 879
Label 3: 92
Label 4: 890
Label 5: 5364
Label 6: 114
Using WeightedRandomSampler
train size: 8012 ,val size: 500 ,test size: 1503
Mixup is activated!
/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=7, bias=True)
)
Patch size = (16, 16)
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained_checkpoint, map_location='cpu')
Load ckpt from /home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth
Load state_dict by model_key = model
all keys: ['encoder.cls_token', 'encoder.pos_embed', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.blocks.0.gamma_1', 'encoder.blocks.0.gamma_2', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.q_bias', 'encoder.blocks.0.attn.v_bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.gamma_1', 'encoder.blocks.1.gamma_2', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.q_bias', 'encoder.blocks.1.attn.v_bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.gamma_1', 'encoder.blocks.2.gamma_2', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.q_bias', 'encoder.blocks.2.attn.v_bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.gamma_1', 'encoder.blocks.3.gamma_2', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.q_bias', 'encoder.blocks.3.attn.v_bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.gamma_1', 'encoder.blocks.4.gamma_2', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.q_bias', 'encoder.blocks.4.attn.v_bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.gamma_1', 'encoder.blocks.5.gamma_2', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.q_bias', 'encoder.blocks.5.attn.v_bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.gamma_1', 'encoder.blocks.6.gamma_2', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.q_bias', 'encoder.blocks.6.attn.v_bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.gamma_1', 'encoder.blocks.7.gamma_2', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.q_bias', 'encoder.blocks.7.attn.v_bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.blocks.8.gamma_1', 'encoder.bl
##############new keys: 454 odict_keys(['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight', 'regresser.regressor_
Weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_bias_table', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.3.attn.relative_position_bias_table', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.6.attn.relative_position_bias_table', 'blocks.7.attn.relative_position_bias_table', 'blocks.8.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.10.attn.relative_position_bias_table', 'blocks.11.attn.relative_position_bias_table', 'blocks.12.attn.relative_position_bias_table', 'blocks.13.attn.relative_position_bias_table', 'blocks.14.attn.relative_position_bias_table', 'blocks.15.attn.relative_position_bias_table', 'blocks.16.attn.relative_position_bias_table', 'blocks.17.attn.relative_position_bias_table', 'blocks.18.attn.relative_position_bias_table', 'blocks.19.attn.relative_position_bias_table', 'blocks.20.attn.relative_position_bias_table', 'blocks.21.attn.relative_position_bias_table', 'blocks.22.attn.relative_position_bias_table', 'blocks.23.attn.relative_position_bias_table', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight',
Ignored weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_index', 'blocks.1.attn.relative_position_index', 'blocks.2.attn.relative_position_index', 'blocks.3.attn.relative_position_index', 'blocks.4.attn.relative_position_index', 'blocks.5.attn.relative_position_index', 'blocks.6.attn.relative_position_index', 'blocks.7.attn.relative_position_index', 'blocks.8.attn.relative_position_index', 'blocks.9.attn.relative_position_index', 'blocks.10.attn.relative_position_index', 'blocks.11.attn.relative_position_index', 'blocks.12.attn.relative_position_index', 'blocks.13.attn.relative_position_index', 'blocks.14.attn.relative_position_index', 'blocks.15.attn.relative_position_index', 'blocks.16.attn.relative_position_index', 'blocks.17.attn.relative_position_index', 'blocks.18.attn.relative_position_index', 'blocks.19.attn.relative_position_index', 'blocks.20.attn.relative_position_index', 'blocks.21.attn.relative_position_index', 'blocks.22.attn.relative_position_index', 'blocks.23.attn.relative_position_index']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=7, bias=True)
)
number of params: 303412743
LR = 0.00050000
Batch size = 128
Update frequent = 1
Number of training examples = 8012
Number of training training per epoch = 62
Assigned values = [2.1029740616282293e-05, 3.2353447101972754e-05, 4.977453400303501e-05, 7.65762061585154e-05, 0.00011780954793617752, 0.00018124545836335003, 0.0002788391667128462, 0.0004289833334043787, 0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "patch_embed.proj.bias"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.gamma_1",
      "blocks.0.gamma_2",
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.relative_position_bias_table",
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.gamma_1",
      "blocks.1.gamma_2",
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.relative_position_bias_table",
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.gamma_1",
      "blocks.2.gamma_2",
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.relative_position_bias_table",
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.gamma_1",
      "blocks.3.gamma_2",
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.relative_position_bias_table",
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.gamma_1",
      "blocks.4.gamma_2",
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.relative_position_bias_table",
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.gamma_1",
      "blocks.5.gamma_2",
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.relative_position_bias_table",
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.gamma_1",
      "blocks.6.gamma_2",
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.relative_position_bias_table",
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.gamma_1",
      "blocks.7.gamma_2",
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.relative_position_bias_table",
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.gamma_1",
      "blocks.8.gamma_2",
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.relative_position_bias_table",
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.gamma_1",
      "blocks.9.gamma_2",
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.relative_position_bias_table",
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.gamma_1",
      "blocks.10.gamma_2",
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.relative_position_bias_table",
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.gamma_1",
      "blocks.11.gamma_2",
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.relative_position_bias_table",
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.12.gamma_1",
      "blocks.12.gamma_2",
      "blocks.12.norm1.weight",
      "blocks.12.norm1.bias",
      "blocks.12.attn.q_bias",
      "blocks.12.attn.v_bias",
      "blocks.12.attn.proj.bias",
      "blocks.12.norm2.weight",
      "blocks.12.norm2.bias",
      "blocks.12.mlp.fc1.bias",
      "blocks.12.mlp.fc2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.12.attn.relative_position_bias_table",
      "blocks.12.attn.qkv.weight",
      "blocks.12.attn.proj.weight",
      "blocks.12.mlp.fc1.weight",
      "blocks.12.mlp.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_14_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.13.gamma_1",
      "blocks.13.gamma_2",
      "blocks.13.norm1.weight",
      "blocks.13.norm1.bias",
      "blocks.13.attn.q_bias",
      "blocks.13.attn.v_bias",
      "blocks.13.attn.proj.bias",
      "blocks.13.norm2.weight",
      "blocks.13.norm2.bias",
      "blocks.13.mlp.fc1.bias",
      "blocks.13.mlp.fc2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_14_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.13.attn.relative_position_bias_table",
      "blocks.13.attn.qkv.weight",
      "blocks.13.attn.proj.weight",
      "blocks.13.mlp.fc1.weight",
      "blocks.13.mlp.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_15_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.14.gamma_1",
      "blocks.14.gamma_2",
      "blocks.14.norm1.weight",
      "blocks.14.norm1.bias",
      "blocks.14.attn.q_bias",
      "blocks.14.attn.v_bias",
      "blocks.14.attn.proj.bias",
      "blocks.14.norm2.weight",
      "blocks.14.norm2.bias",
      "blocks.14.mlp.fc1.bias",
      "blocks.14.mlp.fc2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_15_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.14.attn.relative_position_bias_table",
      "blocks.14.attn.qkv.weight",
      "blocks.14.attn.proj.weight",
      "blocks.14.mlp.fc1.weight",
      "blocks.14.mlp.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_16_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.15.gamma_1",
      "blocks.15.gamma_2",
      "blocks.15.norm1.weight",
      "blocks.15.norm1.bias",
      "blocks.15.attn.q_bias",
      "blocks.15.attn.v_bias",
      "blocks.15.attn.proj.bias",
      "blocks.15.norm2.weight",
      "blocks.15.norm2.bias",
      "blocks.15.mlp.fc1.bias",
      "blocks.15.mlp.fc2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_16_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.15.attn.relative_position_bias_table",
      "blocks.15.attn.qkv.weight",
      "blocks.15.attn.proj.weight",
      "blocks.15.mlp.fc1.weight",
      "blocks.15.mlp.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.16.gamma_1",
      "blocks.16.gamma_2",
      "blocks.16.norm1.weight",
      "blocks.16.norm1.bias",
      "blocks.16.attn.q_bias",
      "blocks.16.attn.v_bias",
      "blocks.16.attn.proj.bias",
      "blocks.16.norm2.weight",
      "blocks.16.norm2.bias",
      "blocks.16.mlp.fc1.bias",
      "blocks.16.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.16.attn.relative_position_bias_table",
      "blocks.16.attn.qkv.weight",
      "blocks.16.attn.proj.weight",
      "blocks.16.mlp.fc1.weight",
      "blocks.16.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_18_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.17.gamma_1",
      "blocks.17.gamma_2",
      "blocks.17.norm1.weight",
      "blocks.17.norm1.bias",
      "blocks.17.attn.q_bias",
      "blocks.17.attn.v_bias",
      "blocks.17.attn.proj.bias",
      "blocks.17.norm2.weight",
      "blocks.17.norm2.bias",
      "blocks.17.mlp.fc1.bias",
      "blocks.17.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_18_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.17.attn.relative_position_bias_table",
      "blocks.17.attn.qkv.weight",
      "blocks.17.attn.proj.weight",
      "blocks.17.mlp.fc1.weight",
      "blocks.17.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_19_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.18.gamma_1",
      "blocks.18.gamma_2",
      "blocks.18.norm1.weight",
      "blocks.18.norm1.bias",
      "blocks.18.attn.q_bias",
      "blocks.18.attn.v_bias",
      "blocks.18.attn.proj.bias",
      "blocks.18.norm2.weight",
      "blocks.18.norm2.bias",
      "blocks.18.mlp.fc1.bias",
      "blocks.18.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_19_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.18.attn.relative_position_bias_table",
      "blocks.18.attn.qkv.weight",
      "blocks.18.attn.proj.weight",
      "blocks.18.mlp.fc1.weight",
      "blocks.18.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_20_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.19.gamma_1",
      "blocks.19.gamma_2",
      "blocks.19.norm1.weight",
      "blocks.19.norm1.bias",
      "blocks.19.attn.q_bias",
      "blocks.19.attn.v_bias",
      "blocks.19.attn.proj.bias",
      "blocks.19.norm2.weight",
      "blocks.19.norm2.bias",
      "blocks.19.mlp.fc1.bias",
      "blocks.19.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_20_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.19.attn.relative_position_bias_table",
      "blocks.19.attn.qkv.weight",
      "blocks.19.attn.proj.weight",
      "blocks.19.mlp.fc1.weight",
      "blocks.19.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_21_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.20.gamma_1",
      "blocks.20.gamma_2",
      "blocks.20.norm1.weight",
      "blocks.20.norm1.bias",
      "blocks.20.attn.q_bias",
      "blocks.20.attn.v_bias",
      "blocks.20.attn.proj.bias",
      "blocks.20.norm2.weight",
      "blocks.20.norm2.bias",
      "blocks.20.mlp.fc1.bias",
      "blocks.20.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_21_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.20.attn.relative_position_bias_table",
      "blocks.20.attn.qkv.weight",
      "blocks.20.attn.proj.weight",
      "blocks.20.mlp.fc1.weight",
      "blocks.20.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_22_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.21.gamma_1",
      "blocks.21.gamma_2",
      "blocks.21.norm1.weight",
      "blocks.21.norm1.bias",
      "blocks.21.attn.q_bias",
      "blocks.21.attn.v_bias",
      "blocks.21.attn.proj.bias",
      "blocks.21.norm2.weight",
      "blocks.21.norm2.bias",
      "blocks.21.mlp.fc1.bias",
      "blocks.21.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_22_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.21.attn.relative_position_bias_table",
      "blocks.21.attn.qkv.weight",
      "blocks.21.attn.proj.weight",
      "blocks.21.mlp.fc1.weight",
      "blocks.21.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_23_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.22.gamma_1",
      "blocks.22.gamma_2",
      "blocks.22.norm1.weight",
      "blocks.22.norm1.bias",
      "blocks.22.attn.q_bias",
      "blocks.22.attn.v_bias",
      "blocks.22.attn.proj.bias",
      "blocks.22.norm2.weight",
      "blocks.22.norm2.bias",
      "blocks.22.mlp.fc1.bias",
      "blocks.22.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_23_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.22.attn.relative_position_bias_table",
      "blocks.22.attn.qkv.weight",
      "blocks.22.attn.proj.weight",
      "blocks.22.mlp.fc1.weight",
      "blocks.22.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_24_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.23.gamma_1",
      "blocks.23.gamma_2",
      "blocks.23.norm1.weight",
      "blocks.23.norm1.bias",
      "blocks.23.attn.q_bias",
      "blocks.23.attn.v_bias",
      "blocks.23.attn.proj.bias",
      "blocks.23.norm2.weight",
      "blocks.23.norm2.bias",
      "blocks.23.mlp.fc1.bias",
      "blocks.23.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_24_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.23.attn.relative_position_bias_table",
      "blocks.23.attn.qkv.weight",
      "blocks.23.attn.proj.weight",
      "blocks.23.mlp.fc1.weight",
      "blocks.23.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_25_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_25_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
/home/share/FM_Code/PanDerm/classification/furnace/utils.py:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
Use step level LR scheduler!
Set warmup steps = 620
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 50 epochs
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [0]  [ 0/62]  eta: 0:06:33  lr: 0.000000  min_lr: 0.000000  loss: 1.9459 (1.9459)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7611 (0.7611)  time: 6.3431  data: 3.4066  max mem: 37088
Epoch: [0]  [10/62]  eta: 0:01:07  lr: 0.000008  min_lr: 0.000000  loss: 1.9459 (1.9458)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8677 (0.8111)  time: 1.3046  data: 0.3104  max mem: 39406
Epoch: [0]  [20/62]  eta: 0:00:44  lr: 0.000016  min_lr: 0.000000  loss: 1.9455 (1.9454)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8596 (0.8246)  time: 0.7988  data: 0.0006  max mem: 39406
Epoch: [0]  [30/62]  eta: 0:00:31  lr: 0.000024  min_lr: 0.000000  loss: 1.9444 (1.9448)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8475 (0.8520)  time: 0.8050  data: 0.0004  max mem: 39406
Epoch: [0]  [40/62]  eta: 0:00:20  lr: 0.000032  min_lr: 0.000000  loss: 1.9415 (1.9427)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.9447 (0.9903)  time: 0.8206  data: 0.0004  max mem: 39406
Epoch: [0]  [50/62]  eta: 0:00:11  lr: 0.000040  min_lr: 0.000000  loss: 1.9237 (1.9374)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5856 (1.1360)  time: 0.8339  data: 0.0003  max mem: 39406
Epoch: [0]  [60/62]  eta: 0:00:01  lr: 0.000048  min_lr: 0.000000  loss: 1.9062 (1.9281)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7245 (1.2500)  time: 0.8457  data: 0.0001  max mem: 39406
Epoch: [0]  [61/62]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000000  loss: 1.8974 (1.9274)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7245 (1.2565)  time: 0.8469  data: 0.0001  max mem: 39406
Epoch: [0] Total time: 0:00:56 (0.9151 s / it)
2025-04-28 18:51:05 Averaged stats: lr: 0.000049  min_lr: 0.000000  loss: 1.8974 (1.9274)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7245 (1.2565)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:11    time: 3.8945  data: 3.0135  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.5660  data: 1.0046  max mem: 39406
Test: Total time: 0:00:04 (1.6320 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5139 Acc: 0.2920 Recall_macro: 0.5139 Recall_weighted: 0.2920 AUC-ROC: 0.8696 Weighted F1-score: 0.3572
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 0, 'Val Loss': 1.8790241479873657, 'Val BAcc': np.float64(0.513854926168359), 'Val Acc': 0.292, 'Val ROC': np.float64(0.8696082575063212), 'Val W_F1': 0.35719896011517893, 'Val Recall_macro': 0.513854926168359, 'Val Recall_weighted': 0.292}
Max val mean recall: 0.51%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [1]  [ 0/62]  eta: 0:04:10  lr: 0.000050  min_lr: 0.000000  loss: 1.8663 (1.8663)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8245 (1.8245)  time: 4.0364  data: 3.2108  max mem: 39406
Epoch: [1]  [10/62]  eta: 0:00:58  lr: 0.000058  min_lr: 0.000000  loss: 1.8210 (1.8060)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0148 (2.0163)  time: 1.1327  data: 0.2923  max mem: 39406
Epoch: [1]  [20/62]  eta: 0:00:42  lr: 0.000066  min_lr: 0.000000  loss: 1.7798 (1.7751)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9479 (1.9737)  time: 0.8483  data: 0.0005  max mem: 39406
Epoch: [1]  [30/62]  eta: 0:00:30  lr: 0.000074  min_lr: 0.000000  loss: 1.7174 (1.7511)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8585 (1.9626)  time: 0.8577  data: 0.0004  max mem: 39406
Epoch: [1]  [40/62]  eta: 0:00:20  lr: 0.000082  min_lr: 0.000000  loss: 1.6836 (1.7294)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9596 (1.9847)  time: 0.8658  data: 0.0005  max mem: 39406
Epoch: [1]  [50/62]  eta: 0:00:11  lr: 0.000090  min_lr: 0.000000  loss: 1.6158 (1.7040)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8973 (1.9429)  time: 0.8737  data: 0.0004  max mem: 39406
Epoch: [1]  [60/62]  eta: 0:00:01  lr: 0.000099  min_lr: 0.000000  loss: 1.6427 (1.6938)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8005 (1.9685)  time: 0.8792  data: 0.0001  max mem: 39406
Epoch: [1]  [61/62]  eta: 0:00:00  lr: 0.000099  min_lr: 0.000000  loss: 1.6427 (1.6884)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8330 (1.9786)  time: 0.8797  data: 0.0001  max mem: 39406
Epoch: [1] Total time: 0:00:57 (0.9202 s / it)
2025-04-28 18:52:13 Averaged stats: lr: 0.000099  min_lr: 0.000000  loss: 1.6427 (1.6884)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8330 (1.9786)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:11    time: 3.9637  data: 3.5286  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.5617  data: 1.1764  max mem: 39406
Test: Total time: 0:00:04 (1.6364 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6717 Acc: 0.5340 Recall_macro: 0.6717 Recall_weighted: 0.5340 AUC-ROC: 0.8986 Weighted F1-score: 0.5794
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 1, 'Val Loss': 1.5018079280853271, 'Val BAcc': np.float64(0.6717270291897158), 'Val Acc': 0.534, 'Val ROC': np.float64(0.8986067763332602), 'Val W_F1': 0.5793997446134403, 'Val Recall_macro': 0.6717270291897158, 'Val Recall_weighted': 0.534}
Max val mean recall: 0.67%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [2]  [ 0/62]  eta: 0:04:40  lr: 0.000100  min_lr: 0.000000  loss: 1.4911 (1.4911)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8744 (1.8744)  time: 4.5219  data: 3.6794  max mem: 39406
Epoch: [2]  [10/62]  eta: 0:01:02  lr: 0.000108  min_lr: 0.000000  loss: 1.6138 (1.5730)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9401 (1.9338)  time: 1.1928  data: 0.3353  max mem: 39406
Epoch: [2]  [20/62]  eta: 0:00:43  lr: 0.000116  min_lr: 0.000000  loss: 1.5545 (1.5117)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9721 (1.9769)  time: 0.8628  data: 0.0008  max mem: 39406
Epoch: [2]  [30/62]  eta: 0:00:31  lr: 0.000124  min_lr: 0.000000  loss: 1.4161 (1.5008)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0151 (1.9972)  time: 0.8720  data: 0.0008  max mem: 39406
Epoch: [2]  [40/62]  eta: 0:00:21  lr: 0.000132  min_lr: 0.000000  loss: 1.3982 (1.4723)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2065 (2.1435)  time: 0.8790  data: 0.0007  max mem: 39406
Epoch: [2]  [50/62]  eta: 0:00:11  lr: 0.000141  min_lr: 0.000000  loss: 1.6050 (1.5037)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3081 (2.1503)  time: 0.8815  data: 0.0003  max mem: 39406
Epoch: [2]  [60/62]  eta: 0:00:01  lr: 0.000149  min_lr: 0.000000  loss: 1.6178 (1.5004)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1793 (2.1525)  time: 0.8863  data: 0.0002  max mem: 39406
Epoch: [2]  [61/62]  eta: 0:00:00  lr: 0.000149  min_lr: 0.000000  loss: 1.6178 (1.5031)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1208 (2.1471)  time: 0.8868  data: 0.0001  max mem: 39406
Epoch: [2] Total time: 0:00:58 (0.9393 s / it)
2025-04-28 18:53:23 Averaged stats: lr: 0.000149  min_lr: 0.000000  loss: 1.6178 (1.5031)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1208 (2.1471)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:12    time: 4.1754  data: 3.7351  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.6301  data: 1.2451  max mem: 39406
Test: Total time: 0:00:05 (1.6918 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6845 Acc: 0.6020 Recall_macro: 0.6845 Recall_weighted: 0.6020 AUC-ROC: 0.9408 Weighted F1-score: 0.6478
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 2, 'Val Loss': 1.2256637811660767, 'Val BAcc': np.float64(0.6845450569331166), 'Val Acc': 0.602, 'Val ROC': np.float64(0.9408369174357681), 'Val W_F1': 0.647772083690877, 'Val Recall_macro': 0.6845450569331166, 'Val Recall_weighted': 0.602}
Max val mean recall: 0.68%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [3]  [ 0/62]  eta: 0:04:22  lr: 0.000150  min_lr: 0.000000  loss: 1.6123 (1.6123)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3183 (2.3183)  time: 4.2267  data: 3.3834  max mem: 39406
