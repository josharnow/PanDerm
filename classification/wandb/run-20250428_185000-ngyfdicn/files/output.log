Not using distributed mode
Namespace(mode='train', batch_size=128, epochs=50, update_freq=1, save_ckpt_freq=5, model='PanDerm_Large_FT', rel_pos_bias=True, sin_pos_emb=True, layer_scale_init_value=0.1, ood_eval=False, input_size=224, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, weights=True, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, percent_data=1.0, TTA=False, monitor='recall', opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.65, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=10, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', pretrained_checkpoint='/home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth', model_key='model|module|state_dict', model_prefix='', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, data_path='/datasets01/imagenet_full_size/061417/', eval_data_path=None, test_csv_path=None, image_key='image', nb_classes=7, imagenet_default_mean_and_std=True, data_set='IMNET', csv_path='/home/syyan/XJ/PanDerm-open_source/data/linear_probing/HAM-official-7-lp.csv', root_path='/home/share/Uni_Eval/ISIC2018_reader/images/', output_dir='/home/share/FM_Code/PanDerm/HAM_Res/', log_dir=None, device='cuda', seed=122, resume='', auto_resume=False, wandb_name='Reproduce_HAM_FT__122', save_ckpt=True, start_epoch=0, eval=False, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, enable_linear_eval=False, enable_multi_print=False, exp_name='ham finetune and eval', distributed=False)
Label distribution:
Label 0: 262
Label 1: 411
Label 2: 879
Label 3: 92
Label 4: 890
Label 5: 5364
Label 6: 114
Using WeightedRandomSampler
train size: 8012 ,val size: 500 ,test size: 1503
Mixup is activated!
/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=7, bias=True)
)
Patch size = (16, 16)
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained_checkpoint, map_location='cpu')
Load ckpt from /home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth
Load state_dict by model_key = model
all keys: ['encoder.cls_token', 'encoder.pos_embed', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.blocks.0.gamma_1', 'encoder.blocks.0.gamma_2', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.q_bias', 'encoder.blocks.0.attn.v_bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.gamma_1', 'encoder.blocks.1.gamma_2', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.q_bias', 'encoder.blocks.1.attn.v_bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.gamma_1', 'encoder.blocks.2.gamma_2', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.q_bias', 'encoder.blocks.2.attn.v_bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.gamma_1', 'encoder.blocks.3.gamma_2', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.q_bias', 'encoder.blocks.3.attn.v_bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.gamma_1', 'encoder.blocks.4.gamma_2', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.q_bias', 'encoder.blocks.4.attn.v_bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.gamma_1', 'encoder.blocks.5.gamma_2', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.q_bias', 'encoder.blocks.5.attn.v_bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.gamma_1', 'encoder.blocks.6.gamma_2', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.q_bias', 'encoder.blocks.6.attn.v_bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.gamma_1', 'encoder.blocks.7.gamma_2', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.q_bias', 'encoder.blocks.7.attn.v_bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.blocks.8.gamma_1', 'encoder.bl
##############new keys: 454 odict_keys(['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight', 'regresser.regressor_
Weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_bias_table', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.3.attn.relative_position_bias_table', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.6.attn.relative_position_bias_table', 'blocks.7.attn.relative_position_bias_table', 'blocks.8.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.10.attn.relative_position_bias_table', 'blocks.11.attn.relative_position_bias_table', 'blocks.12.attn.relative_position_bias_table', 'blocks.13.attn.relative_position_bias_table', 'blocks.14.attn.relative_position_bias_table', 'blocks.15.attn.relative_position_bias_table', 'blocks.16.attn.relative_position_bias_table', 'blocks.17.attn.relative_position_bias_table', 'blocks.18.attn.relative_position_bias_table', 'blocks.19.attn.relative_position_bias_table', 'blocks.20.attn.relative_position_bias_table', 'blocks.21.attn.relative_position_bias_table', 'blocks.22.attn.relative_position_bias_table', 'blocks.23.attn.relative_position_bias_table', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight',
Ignored weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_index', 'blocks.1.attn.relative_position_index', 'blocks.2.attn.relative_position_index', 'blocks.3.attn.relative_position_index', 'blocks.4.attn.relative_position_index', 'blocks.5.attn.relative_position_index', 'blocks.6.attn.relative_position_index', 'blocks.7.attn.relative_position_index', 'blocks.8.attn.relative_position_index', 'blocks.9.attn.relative_position_index', 'blocks.10.attn.relative_position_index', 'blocks.11.attn.relative_position_index', 'blocks.12.attn.relative_position_index', 'blocks.13.attn.relative_position_index', 'blocks.14.attn.relative_position_index', 'blocks.15.attn.relative_position_index', 'blocks.16.attn.relative_position_index', 'blocks.17.attn.relative_position_index', 'blocks.18.attn.relative_position_index', 'blocks.19.attn.relative_position_index', 'blocks.20.attn.relative_position_index', 'blocks.21.attn.relative_position_index', 'blocks.22.attn.relative_position_index', 'blocks.23.attn.relative_position_index']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=7, bias=True)
)
number of params: 303412743
LR = 0.00050000
Batch size = 128
Update frequent = 1
Number of training examples = 8012
Number of training training per epoch = 62
Assigned values = [2.1029740616282293e-05, 3.2353447101972754e-05, 4.977453400303501e-05, 7.65762061585154e-05, 0.00011780954793617752, 0.00018124545836335003, 0.0002788391667128462, 0.0004289833334043787, 0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "patch_embed.proj.bias"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.gamma_1",
      "blocks.0.gamma_2",
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.relative_position_bias_table",
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.gamma_1",
      "blocks.1.gamma_2",
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.relative_position_bias_table",
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.gamma_1",
      "blocks.2.gamma_2",
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.relative_position_bias_table",
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.gamma_1",
      "blocks.3.gamma_2",
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.relative_position_bias_table",
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.gamma_1",
      "blocks.4.gamma_2",
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.relative_position_bias_table",
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.gamma_1",
      "blocks.5.gamma_2",
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.relative_position_bias_table",
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.gamma_1",
      "blocks.6.gamma_2",
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.relative_position_bias_table",
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.gamma_1",
      "blocks.7.gamma_2",
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.relative_position_bias_table",
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.gamma_1",
      "blocks.8.gamma_2",
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.relative_position_bias_table",
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.gamma_1",
      "blocks.9.gamma_2",
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.relative_position_bias_table",
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.gamma_1",
      "blocks.10.gamma_2",
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.relative_position_bias_table",
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.gamma_1",
      "blocks.11.gamma_2",
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.relative_position_bias_table",
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.12.gamma_1",
      "blocks.12.gamma_2",
      "blocks.12.norm1.weight",
      "blocks.12.norm1.bias",
      "blocks.12.attn.q_bias",
      "blocks.12.attn.v_bias",
      "blocks.12.attn.proj.bias",
      "blocks.12.norm2.weight",
      "blocks.12.norm2.bias",
      "blocks.12.mlp.fc1.bias",
      "blocks.12.mlp.fc2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.12.attn.relative_position_bias_table",
      "blocks.12.attn.qkv.weight",
      "blocks.12.attn.proj.weight",
      "blocks.12.mlp.fc1.weight",
      "blocks.12.mlp.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_14_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.13.gamma_1",
      "blocks.13.gamma_2",
      "blocks.13.norm1.weight",
      "blocks.13.norm1.bias",
      "blocks.13.attn.q_bias",
      "blocks.13.attn.v_bias",
      "blocks.13.attn.proj.bias",
      "blocks.13.norm2.weight",
      "blocks.13.norm2.bias",
      "blocks.13.mlp.fc1.bias",
      "blocks.13.mlp.fc2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_14_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.13.attn.relative_position_bias_table",
      "blocks.13.attn.qkv.weight",
      "blocks.13.attn.proj.weight",
      "blocks.13.mlp.fc1.weight",
      "blocks.13.mlp.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_15_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.14.gamma_1",
      "blocks.14.gamma_2",
      "blocks.14.norm1.weight",
      "blocks.14.norm1.bias",
      "blocks.14.attn.q_bias",
      "blocks.14.attn.v_bias",
      "blocks.14.attn.proj.bias",
      "blocks.14.norm2.weight",
      "blocks.14.norm2.bias",
      "blocks.14.mlp.fc1.bias",
      "blocks.14.mlp.fc2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_15_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.14.attn.relative_position_bias_table",
      "blocks.14.attn.qkv.weight",
      "blocks.14.attn.proj.weight",
      "blocks.14.mlp.fc1.weight",
      "blocks.14.mlp.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_16_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.15.gamma_1",
      "blocks.15.gamma_2",
      "blocks.15.norm1.weight",
      "blocks.15.norm1.bias",
      "blocks.15.attn.q_bias",
      "blocks.15.attn.v_bias",
      "blocks.15.attn.proj.bias",
      "blocks.15.norm2.weight",
      "blocks.15.norm2.bias",
      "blocks.15.mlp.fc1.bias",
      "blocks.15.mlp.fc2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_16_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.15.attn.relative_position_bias_table",
      "blocks.15.attn.qkv.weight",
      "blocks.15.attn.proj.weight",
      "blocks.15.mlp.fc1.weight",
      "blocks.15.mlp.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.16.gamma_1",
      "blocks.16.gamma_2",
      "blocks.16.norm1.weight",
      "blocks.16.norm1.bias",
      "blocks.16.attn.q_bias",
      "blocks.16.attn.v_bias",
      "blocks.16.attn.proj.bias",
      "blocks.16.norm2.weight",
      "blocks.16.norm2.bias",
      "blocks.16.mlp.fc1.bias",
      "blocks.16.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.16.attn.relative_position_bias_table",
      "blocks.16.attn.qkv.weight",
      "blocks.16.attn.proj.weight",
      "blocks.16.mlp.fc1.weight",
      "blocks.16.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_18_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.17.gamma_1",
      "blocks.17.gamma_2",
      "blocks.17.norm1.weight",
      "blocks.17.norm1.bias",
      "blocks.17.attn.q_bias",
      "blocks.17.attn.v_bias",
      "blocks.17.attn.proj.bias",
      "blocks.17.norm2.weight",
      "blocks.17.norm2.bias",
      "blocks.17.mlp.fc1.bias",
      "blocks.17.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_18_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.17.attn.relative_position_bias_table",
      "blocks.17.attn.qkv.weight",
      "blocks.17.attn.proj.weight",
      "blocks.17.mlp.fc1.weight",
      "blocks.17.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_19_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.18.gamma_1",
      "blocks.18.gamma_2",
      "blocks.18.norm1.weight",
      "blocks.18.norm1.bias",
      "blocks.18.attn.q_bias",
      "blocks.18.attn.v_bias",
      "blocks.18.attn.proj.bias",
      "blocks.18.norm2.weight",
      "blocks.18.norm2.bias",
      "blocks.18.mlp.fc1.bias",
      "blocks.18.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_19_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.18.attn.relative_position_bias_table",
      "blocks.18.attn.qkv.weight",
      "blocks.18.attn.proj.weight",
      "blocks.18.mlp.fc1.weight",
      "blocks.18.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_20_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.19.gamma_1",
      "blocks.19.gamma_2",
      "blocks.19.norm1.weight",
      "blocks.19.norm1.bias",
      "blocks.19.attn.q_bias",
      "blocks.19.attn.v_bias",
      "blocks.19.attn.proj.bias",
      "blocks.19.norm2.weight",
      "blocks.19.norm2.bias",
      "blocks.19.mlp.fc1.bias",
      "blocks.19.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_20_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.19.attn.relative_position_bias_table",
      "blocks.19.attn.qkv.weight",
      "blocks.19.attn.proj.weight",
      "blocks.19.mlp.fc1.weight",
      "blocks.19.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_21_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.20.gamma_1",
      "blocks.20.gamma_2",
      "blocks.20.norm1.weight",
      "blocks.20.norm1.bias",
      "blocks.20.attn.q_bias",
      "blocks.20.attn.v_bias",
      "blocks.20.attn.proj.bias",
      "blocks.20.norm2.weight",
      "blocks.20.norm2.bias",
      "blocks.20.mlp.fc1.bias",
      "blocks.20.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_21_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.20.attn.relative_position_bias_table",
      "blocks.20.attn.qkv.weight",
      "blocks.20.attn.proj.weight",
      "blocks.20.mlp.fc1.weight",
      "blocks.20.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_22_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.21.gamma_1",
      "blocks.21.gamma_2",
      "blocks.21.norm1.weight",
      "blocks.21.norm1.bias",
      "blocks.21.attn.q_bias",
      "blocks.21.attn.v_bias",
      "blocks.21.attn.proj.bias",
      "blocks.21.norm2.weight",
      "blocks.21.norm2.bias",
      "blocks.21.mlp.fc1.bias",
      "blocks.21.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_22_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.21.attn.relative_position_bias_table",
      "blocks.21.attn.qkv.weight",
      "blocks.21.attn.proj.weight",
      "blocks.21.mlp.fc1.weight",
      "blocks.21.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_23_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.22.gamma_1",
      "blocks.22.gamma_2",
      "blocks.22.norm1.weight",
      "blocks.22.norm1.bias",
      "blocks.22.attn.q_bias",
      "blocks.22.attn.v_bias",
      "blocks.22.attn.proj.bias",
      "blocks.22.norm2.weight",
      "blocks.22.norm2.bias",
      "blocks.22.mlp.fc1.bias",
      "blocks.22.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_23_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.22.attn.relative_position_bias_table",
      "blocks.22.attn.qkv.weight",
      "blocks.22.attn.proj.weight",
      "blocks.22.mlp.fc1.weight",
      "blocks.22.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_24_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.23.gamma_1",
      "blocks.23.gamma_2",
      "blocks.23.norm1.weight",
      "blocks.23.norm1.bias",
      "blocks.23.attn.q_bias",
      "blocks.23.attn.v_bias",
      "blocks.23.attn.proj.bias",
      "blocks.23.norm2.weight",
      "blocks.23.norm2.bias",
      "blocks.23.mlp.fc1.bias",
      "blocks.23.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_24_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.23.attn.relative_position_bias_table",
      "blocks.23.attn.qkv.weight",
      "blocks.23.attn.proj.weight",
      "blocks.23.mlp.fc1.weight",
      "blocks.23.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_25_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_25_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
/home/share/FM_Code/PanDerm/classification/furnace/utils.py:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
Use step level LR scheduler!
Set warmup steps = 620
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 50 epochs
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [0]  [ 0/62]  eta: 0:06:33  lr: 0.000000  min_lr: 0.000000  loss: 1.9459 (1.9459)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7611 (0.7611)  time: 6.3431  data: 3.4066  max mem: 37088
Epoch: [0]  [10/62]  eta: 0:01:07  lr: 0.000008  min_lr: 0.000000  loss: 1.9459 (1.9458)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8677 (0.8111)  time: 1.3046  data: 0.3104  max mem: 39406
Epoch: [0]  [20/62]  eta: 0:00:44  lr: 0.000016  min_lr: 0.000000  loss: 1.9455 (1.9454)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8596 (0.8246)  time: 0.7988  data: 0.0006  max mem: 39406
Epoch: [0]  [30/62]  eta: 0:00:31  lr: 0.000024  min_lr: 0.000000  loss: 1.9444 (1.9448)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8475 (0.8520)  time: 0.8050  data: 0.0004  max mem: 39406
Epoch: [0]  [40/62]  eta: 0:00:20  lr: 0.000032  min_lr: 0.000000  loss: 1.9415 (1.9427)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.9447 (0.9903)  time: 0.8206  data: 0.0004  max mem: 39406
Epoch: [0]  [50/62]  eta: 0:00:11  lr: 0.000040  min_lr: 0.000000  loss: 1.9237 (1.9374)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5856 (1.1360)  time: 0.8339  data: 0.0003  max mem: 39406
Epoch: [0]  [60/62]  eta: 0:00:01  lr: 0.000048  min_lr: 0.000000  loss: 1.9062 (1.9281)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7245 (1.2500)  time: 0.8457  data: 0.0001  max mem: 39406
Epoch: [0]  [61/62]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000000  loss: 1.8974 (1.9274)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7245 (1.2565)  time: 0.8469  data: 0.0001  max mem: 39406
Epoch: [0] Total time: 0:00:56 (0.9151 s / it)
2025-04-28 18:51:05 Averaged stats: lr: 0.000049  min_lr: 0.000000  loss: 1.8974 (1.9274)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7245 (1.2565)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:11    time: 3.8945  data: 3.0135  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.5660  data: 1.0046  max mem: 39406
Test: Total time: 0:00:04 (1.6320 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5139 Acc: 0.2920 Recall_macro: 0.5139 Recall_weighted: 0.2920 AUC-ROC: 0.8696 Weighted F1-score: 0.3572
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 0, 'Val Loss': 1.8790241479873657, 'Val BAcc': np.float64(0.513854926168359), 'Val Acc': 0.292, 'Val ROC': np.float64(0.8696082575063212), 'Val W_F1': 0.35719896011517893, 'Val Recall_macro': 0.513854926168359, 'Val Recall_weighted': 0.292}
Max val mean recall: 0.51%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [1]  [ 0/62]  eta: 0:04:10  lr: 0.000050  min_lr: 0.000000  loss: 1.8663 (1.8663)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8245 (1.8245)  time: 4.0364  data: 3.2108  max mem: 39406
Epoch: [1]  [10/62]  eta: 0:00:58  lr: 0.000058  min_lr: 0.000000  loss: 1.8210 (1.8060)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0148 (2.0163)  time: 1.1327  data: 0.2923  max mem: 39406
Epoch: [1]  [20/62]  eta: 0:00:42  lr: 0.000066  min_lr: 0.000000  loss: 1.7798 (1.7751)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9479 (1.9737)  time: 0.8483  data: 0.0005  max mem: 39406
Epoch: [1]  [30/62]  eta: 0:00:30  lr: 0.000074  min_lr: 0.000000  loss: 1.7174 (1.7511)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8585 (1.9626)  time: 0.8577  data: 0.0004  max mem: 39406
Epoch: [1]  [40/62]  eta: 0:00:20  lr: 0.000082  min_lr: 0.000000  loss: 1.6836 (1.7294)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9596 (1.9847)  time: 0.8658  data: 0.0005  max mem: 39406
Epoch: [1]  [50/62]  eta: 0:00:11  lr: 0.000090  min_lr: 0.000000  loss: 1.6158 (1.7040)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8973 (1.9429)  time: 0.8737  data: 0.0004  max mem: 39406
Epoch: [1]  [60/62]  eta: 0:00:01  lr: 0.000099  min_lr: 0.000000  loss: 1.6427 (1.6938)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8005 (1.9685)  time: 0.8792  data: 0.0001  max mem: 39406
Epoch: [1]  [61/62]  eta: 0:00:00  lr: 0.000099  min_lr: 0.000000  loss: 1.6427 (1.6884)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8330 (1.9786)  time: 0.8797  data: 0.0001  max mem: 39406
Epoch: [1] Total time: 0:00:57 (0.9202 s / it)
2025-04-28 18:52:13 Averaged stats: lr: 0.000099  min_lr: 0.000000  loss: 1.6427 (1.6884)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8330 (1.9786)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:11    time: 3.9637  data: 3.5286  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.5617  data: 1.1764  max mem: 39406
Test: Total time: 0:00:04 (1.6364 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6717 Acc: 0.5340 Recall_macro: 0.6717 Recall_weighted: 0.5340 AUC-ROC: 0.8986 Weighted F1-score: 0.5794
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 1, 'Val Loss': 1.5018079280853271, 'Val BAcc': np.float64(0.6717270291897158), 'Val Acc': 0.534, 'Val ROC': np.float64(0.8986067763332602), 'Val W_F1': 0.5793997446134403, 'Val Recall_macro': 0.6717270291897158, 'Val Recall_weighted': 0.534}
Max val mean recall: 0.67%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [2]  [ 0/62]  eta: 0:04:40  lr: 0.000100  min_lr: 0.000000  loss: 1.4911 (1.4911)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8744 (1.8744)  time: 4.5219  data: 3.6794  max mem: 39406
Epoch: [2]  [10/62]  eta: 0:01:02  lr: 0.000108  min_lr: 0.000000  loss: 1.6138 (1.5730)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9401 (1.9338)  time: 1.1928  data: 0.3353  max mem: 39406
Epoch: [2]  [20/62]  eta: 0:00:43  lr: 0.000116  min_lr: 0.000000  loss: 1.5545 (1.5117)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9721 (1.9769)  time: 0.8628  data: 0.0008  max mem: 39406
Epoch: [2]  [30/62]  eta: 0:00:31  lr: 0.000124  min_lr: 0.000000  loss: 1.4161 (1.5008)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0151 (1.9972)  time: 0.8720  data: 0.0008  max mem: 39406
Epoch: [2]  [40/62]  eta: 0:00:21  lr: 0.000132  min_lr: 0.000000  loss: 1.3982 (1.4723)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2065 (2.1435)  time: 0.8790  data: 0.0007  max mem: 39406
Epoch: [2]  [50/62]  eta: 0:00:11  lr: 0.000141  min_lr: 0.000000  loss: 1.6050 (1.5037)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3081 (2.1503)  time: 0.8815  data: 0.0003  max mem: 39406
Epoch: [2]  [60/62]  eta: 0:00:01  lr: 0.000149  min_lr: 0.000000  loss: 1.6178 (1.5004)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1793 (2.1525)  time: 0.8863  data: 0.0002  max mem: 39406
Epoch: [2]  [61/62]  eta: 0:00:00  lr: 0.000149  min_lr: 0.000000  loss: 1.6178 (1.5031)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1208 (2.1471)  time: 0.8868  data: 0.0001  max mem: 39406
Epoch: [2] Total time: 0:00:58 (0.9393 s / it)
2025-04-28 18:53:23 Averaged stats: lr: 0.000149  min_lr: 0.000000  loss: 1.6178 (1.5031)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1208 (2.1471)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:12    time: 4.1754  data: 3.7351  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.6301  data: 1.2451  max mem: 39406
Test: Total time: 0:00:05 (1.6918 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6845 Acc: 0.6020 Recall_macro: 0.6845 Recall_weighted: 0.6020 AUC-ROC: 0.9408 Weighted F1-score: 0.6478
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 2, 'Val Loss': 1.2256637811660767, 'Val BAcc': np.float64(0.6845450569331166), 'Val Acc': 0.602, 'Val ROC': np.float64(0.9408369174357681), 'Val W_F1': 0.647772083690877, 'Val Recall_macro': 0.6845450569331166, 'Val Recall_weighted': 0.602}
Max val mean recall: 0.68%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [3]  [ 0/62]  eta: 0:04:22  lr: 0.000150  min_lr: 0.000000  loss: 1.6123 (1.6123)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3183 (2.3183)  time: 4.2267  data: 3.3834  max mem: 39406
Epoch: [3]  [10/62]  eta: 0:01:00  lr: 0.000158  min_lr: 0.000000  loss: 1.5208 (1.4753)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9031 (2.0203)  time: 1.1635  data: 0.3079  max mem: 39406
Epoch: [3]  [20/62]  eta: 0:00:42  lr: 0.000166  min_lr: 0.000000  loss: 1.5208 (1.5100)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9349 (2.1035)  time: 0.8627  data: 0.0005  max mem: 39406
Epoch: [3]  [30/62]  eta: 0:00:31  lr: 0.000174  min_lr: 0.000000  loss: 1.5520 (1.5064)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1774 (2.2094)  time: 0.8712  data: 0.0005  max mem: 39406
Epoch: [3]  [40/62]  eta: 0:00:20  lr: 0.000183  min_lr: 0.000000  loss: 1.5389 (1.4885)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2556 (2.2177)  time: 0.8781  data: 0.0005  max mem: 39406
Epoch: [3]  [50/62]  eta: 0:00:11  lr: 0.000191  min_lr: 0.000000  loss: 1.4370 (1.4773)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2500 (2.2800)  time: 0.8850  data: 0.0003  max mem: 39406
Epoch: [3]  [60/62]  eta: 0:00:01  lr: 0.000199  min_lr: 0.000000  loss: 1.3653 (1.4502)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3658 (2.3347)  time: 0.8907  data: 0.0001  max mem: 39406
Epoch: [3]  [61/62]  eta: 0:00:00  lr: 0.000200  min_lr: 0.000000  loss: 1.3653 (1.4507)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3658 (2.3295)  time: 0.8913  data: 0.0001  max mem: 39406
Epoch: [3] Total time: 0:00:57 (0.9355 s / it)
2025-04-28 18:54:32 Averaged stats: lr: 0.000200  min_lr: 0.000000  loss: 1.3653 (1.4507)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3658 (2.3295)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4032  data: 2.9670  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3749  data: 0.9891  max mem: 39406
Test: Total time: 0:00:04 (1.4360 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7455 Acc: 0.6400 Recall_macro: 0.7455 Recall_weighted: 0.6400 AUC-ROC: 0.9454 Weighted F1-score: 0.6807
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 3, 'Val Loss': 1.1081711053848267, 'Val BAcc': np.float64(0.7455310237026654), 'Val Acc': 0.64, 'Val ROC': np.float64(0.945364618049853), 'Val W_F1': 0.6807307175383939, 'Val Recall_macro': 0.7455310237026654, 'Val Recall_weighted': 0.64}
Max val mean recall: 0.75%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [4]  [ 0/62]  eta: 0:04:18  lr: 0.000200  min_lr: 0.000000  loss: 1.5614 (1.5614)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5414 (2.5414)  time: 4.1636  data: 3.3153  max mem: 39406
Epoch: [4]  [10/62]  eta: 0:01:00  lr: 0.000208  min_lr: 0.000000  loss: 1.5147 (1.3895)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3110 (2.2316)  time: 1.1619  data: 0.3017  max mem: 39406
Epoch: [4]  [20/62]  eta: 0:00:42  lr: 0.000216  min_lr: 0.000000  loss: 1.4260 (1.3927)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9732 (2.0965)  time: 0.8657  data: 0.0004  max mem: 39406
Epoch: [4]  [30/62]  eta: 0:00:31  lr: 0.000225  min_lr: 0.000000  loss: 1.3215 (1.3587)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0504 (2.2532)  time: 0.8735  data: 0.0004  max mem: 39406
Epoch: [4]  [40/62]  eta: 0:00:20  lr: 0.000233  min_lr: 0.000000  loss: 1.2564 (1.3452)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3153 (2.3093)  time: 0.8806  data: 0.0004  max mem: 39406
Epoch: [4]  [50/62]  eta: 0:00:11  lr: 0.000241  min_lr: 0.000000  loss: 1.3778 (1.3422)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3220 (2.3065)  time: 0.8871  data: 0.0003  max mem: 39406
Epoch: [4]  [60/62]  eta: 0:00:01  lr: 0.000249  min_lr: 0.000000  loss: 1.3763 (1.3352)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3220 (2.3407)  time: 0.8926  data: 0.0001  max mem: 39406
Epoch: [4]  [61/62]  eta: 0:00:00  lr: 0.000250  min_lr: 0.000000  loss: 1.3267 (1.3333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3905 (2.3513)  time: 0.8930  data: 0.0001  max mem: 39406
Epoch: [4] Total time: 0:00:58 (0.9367 s / it)
2025-04-28 18:55:40 Averaged stats: lr: 0.000250  min_lr: 0.000000  loss: 1.3267 (1.3333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3905 (2.3513)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.3900  data: 2.9516  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3718  data: 0.9840  max mem: 39406
Test: Total time: 0:00:04 (1.4317 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7680 Acc: 0.6660 Recall_macro: 0.7680 Recall_weighted: 0.6660 AUC-ROC: 0.9497 Weighted F1-score: 0.7045
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 4, 'Val Loss': 1.0623295307159424, 'Val BAcc': np.float64(0.7680395475171595), 'Val Acc': 0.666, 'Val ROC': np.float64(0.9496516778785457), 'Val W_F1': 0.7045427526516307, 'Val Recall_macro': 0.7680395475171595, 'Val Recall_weighted': 0.666}
Max val mean recall: 0.77%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [5]  [ 0/62]  eta: 0:04:13  lr: 0.000250  min_lr: 0.000000  loss: 1.2524 (1.2524)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8720 (1.8720)  time: 4.0811  data: 3.2375  max mem: 39406
Epoch: [5]  [10/62]  eta: 0:01:00  lr: 0.000258  min_lr: 0.000000  loss: 1.3476 (1.3009)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3245 (2.4169)  time: 1.1541  data: 0.2947  max mem: 39406
Epoch: [5]  [20/62]  eta: 0:00:42  lr: 0.000267  min_lr: 0.000000  loss: 1.3912 (1.3248)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4879 (2.4339)  time: 0.8690  data: 0.0004  max mem: 39406
Epoch: [5]  [30/62]  eta: 0:00:31  lr: 0.000275  min_lr: 0.000000  loss: 1.4553 (1.3325)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4358 (2.4720)  time: 0.8776  data: 0.0004  max mem: 39406
Epoch: [5]  [40/62]  eta: 0:00:20  lr: 0.000283  min_lr: 0.000000  loss: 1.4659 (1.3641)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2198 (2.4236)  time: 0.8814  data: 0.0004  max mem: 39406
Epoch: [5]  [50/62]  eta: 0:00:11  lr: 0.000291  min_lr: 0.000000  loss: 1.4407 (1.3793)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1699 (2.4231)  time: 0.8877  data: 0.0003  max mem: 39406
Epoch: [5]  [60/62]  eta: 0:00:01  lr: 0.000299  min_lr: 0.000000  loss: 1.3271 (1.3522)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3159 (2.4033)  time: 0.8928  data: 0.0001  max mem: 39406
Epoch: [5]  [61/62]  eta: 0:00:00  lr: 0.000300  min_lr: 0.000000  loss: 1.3868 (1.3541)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3159 (2.4018)  time: 0.8930  data: 0.0001  max mem: 39406
Epoch: [5] Total time: 0:00:58 (0.9366 s / it)
2025-04-28 18:56:49 Averaged stats: lr: 0.000300  min_lr: 0.000000  loss: 1.3868 (1.3541)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3159 (2.4018)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.3598  data: 2.9271  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3622  data: 0.9758  max mem: 39406
Test: Total time: 0:00:04 (1.4205 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8190 Acc: 0.7060 Recall_macro: 0.8190 Recall_weighted: 0.7060 AUC-ROC: 0.9638 Weighted F1-score: 0.7406
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 5, 'Val Loss': 0.8842025995254517, 'Val BAcc': np.float64(0.8189925745895896), 'Val Acc': 0.706, 'Val ROC': np.float64(0.9638499129453242), 'Val W_F1': 0.7405573940765118, 'Val Recall_macro': 0.8189925745895896, 'Val Recall_weighted': 0.706}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [6]  [ 0/62]  eta: 0:04:12  lr: 0.000300  min_lr: 0.000000  loss: 1.5738 (1.5738)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0094 (2.0094)  time: 4.0681  data: 3.2259  max mem: 39406
Epoch: [6]  [10/62]  eta: 0:00:59  lr: 0.000309  min_lr: 0.000000  loss: 1.4377 (1.4316)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4705 (2.4544)  time: 1.1530  data: 0.2937  max mem: 39406
Epoch: [6]  [20/62]  eta: 0:00:42  lr: 0.000317  min_lr: 0.000000  loss: 1.4028 (1.3591)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5719 (2.5442)  time: 0.8657  data: 0.0005  max mem: 39406
Epoch: [6]  [30/62]  eta: 0:00:31  lr: 0.000325  min_lr: 0.000000  loss: 1.4064 (1.3718)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7651 (2.5845)  time: 0.8733  data: 0.0005  max mem: 39406
Epoch: [6]  [40/62]  eta: 0:00:20  lr: 0.000333  min_lr: 0.000000  loss: 1.2522 (1.3203)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3291 (2.4988)  time: 0.8804  data: 0.0005  max mem: 39406
Epoch: [6]  [50/62]  eta: 0:00:11  lr: 0.000341  min_lr: 0.000000  loss: 1.1943 (1.3152)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3114 (2.5167)  time: 0.8872  data: 0.0004  max mem: 39406
Epoch: [6]  [60/62]  eta: 0:00:01  lr: 0.000349  min_lr: 0.000000  loss: 1.3544 (1.3107)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5054 (2.4800)  time: 0.8925  data: 0.0001  max mem: 39406
Epoch: [6]  [61/62]  eta: 0:00:00  lr: 0.000350  min_lr: 0.000000  loss: 1.3261 (1.3097)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3895 (2.4684)  time: 0.8930  data: 0.0001  max mem: 39406
Epoch: [6] Total time: 0:00:57 (0.9350 s / it)
2025-04-28 18:57:57 Averaged stats: lr: 0.000350  min_lr: 0.000000  loss: 1.3261 (1.3097)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3895 (2.4684)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:09    time: 3.3272  data: 2.8883  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3500  data: 0.9629  max mem: 39406
Test: Total time: 0:00:04 (1.4154 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7989 Acc: 0.6740 Recall_macro: 0.7989 Recall_weighted: 0.6740 AUC-ROC: 0.9607 Weighted F1-score: 0.7084
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 6, 'Val Loss': 0.9894893169403076, 'Val BAcc': np.float64(0.7989225948181172), 'Val Acc': 0.674, 'Val ROC': np.float64(0.9607257684885501), 'Val W_F1': 0.7084109478929217, 'Val Recall_macro': 0.7989225948181172, 'Val Recall_weighted': 0.674}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [7]  [ 0/62]  eta: 0:04:17  lr: 0.000351  min_lr: 0.000000  loss: 1.0589 (1.0589)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2794 (2.2794)  time: 4.1591  data: 3.2968  max mem: 39406
Epoch: [7]  [10/62]  eta: 0:01:00  lr: 0.000359  min_lr: 0.000000  loss: 1.3200 (1.2556)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3179 (2.4324)  time: 1.1727  data: 0.3001  max mem: 39406
Epoch: [7]  [20/62]  eta: 0:00:43  lr: 0.000367  min_lr: 0.000000  loss: 1.1948 (1.2395)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6225 (2.7140)  time: 0.8773  data: 0.0004  max mem: 39406
Epoch: [7]  [30/62]  eta: 0:00:31  lr: 0.000375  min_lr: 0.000000  loss: 1.3069 (1.2640)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6553 (2.6787)  time: 0.8824  data: 0.0004  max mem: 39406
Epoch: [7]  [40/62]  eta: 0:00:21  lr: 0.000383  min_lr: 0.000000  loss: 1.3069 (1.2648)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3682 (2.6352)  time: 0.8868  data: 0.0004  max mem: 39406
Epoch: [7]  [50/62]  eta: 0:00:11  lr: 0.000391  min_lr: 0.000000  loss: 1.2568 (1.2727)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4345 (2.6574)  time: 0.8912  data: 0.0003  max mem: 39406
Epoch: [7]  [60/62]  eta: 0:00:01  lr: 0.000399  min_lr: 0.000000  loss: 1.2793 (1.2719)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2537 (2.5660)  time: 0.8942  data: 0.0001  max mem: 39406
Epoch: [7]  [61/62]  eta: 0:00:00  lr: 0.000400  min_lr: 0.000000  loss: 1.3017 (1.2723)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1966 (2.5596)  time: 0.8947  data: 0.0001  max mem: 39406
Epoch: [7] Total time: 0:00:58 (0.9430 s / it)
2025-04-28 18:59:00 Averaged stats: lr: 0.000400  min_lr: 0.000000  loss: 1.3017 (1.2723)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1966 (2.5596)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4169  data: 2.9842  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3827  data: 0.9949  max mem: 39406
Test: Total time: 0:00:04 (1.4622 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7758 Acc: 0.7020 Recall_macro: 0.7758 Recall_weighted: 0.7020 AUC-ROC: 0.9647 Weighted F1-score: 0.7351
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 7, 'Val Loss': 0.9128928184509277, 'Val BAcc': np.float64(0.7758168182208695), 'Val Acc': 0.702, 'Val ROC': np.float64(0.9646845069876196), 'Val W_F1': 0.7351159293565134, 'Val Recall_macro': 0.7758168182208695, 'Val Recall_weighted': 0.702}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [8]  [ 0/62]  eta: 0:04:11  lr: 0.000401  min_lr: 0.000000  loss: 1.0064 (1.0064)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7710 (2.7710)  time: 4.0550  data: 3.1930  max mem: 39406
Epoch: [8]  [10/62]  eta: 0:01:00  lr: 0.000409  min_lr: 0.000000  loss: 1.3306 (1.2860)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6741 (2.6767)  time: 1.1652  data: 0.2907  max mem: 39406
Epoch: [8]  [20/62]  eta: 0:00:43  lr: 0.000417  min_lr: 0.000000  loss: 1.3074 (1.2539)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2858 (2.4515)  time: 0.8787  data: 0.0004  max mem: 39406
Epoch: [8]  [30/62]  eta: 0:00:31  lr: 0.000425  min_lr: 0.000000  loss: 1.1225 (1.2097)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0311 (2.4239)  time: 0.8836  data: 0.0005  max mem: 39406
Epoch: [8]  [40/62]  eta: 0:00:21  lr: 0.000433  min_lr: 0.000000  loss: 1.1493 (1.2146)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1001 (2.4236)  time: 0.8875  data: 0.0005  max mem: 39406
Epoch: [8]  [50/62]  eta: 0:00:11  lr: 0.000441  min_lr: 0.000000  loss: 1.3223 (1.2462)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3139 (2.4271)  time: 0.8911  data: 0.0003  max mem: 39406
Epoch: [8]  [60/62]  eta: 0:00:01  lr: 0.000449  min_lr: 0.000000  loss: 1.3324 (1.2527)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2652 (2.4216)  time: 0.8946  data: 0.0002  max mem: 39406
Epoch: [8]  [61/62]  eta: 0:00:00  lr: 0.000450  min_lr: 0.000000  loss: 1.3589 (1.2595)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2652 (2.4411)  time: 0.8949  data: 0.0002  max mem: 39406
Epoch: [8] Total time: 0:00:58 (0.9423 s / it)
2025-04-28 19:00:03 Averaged stats: lr: 0.000450  min_lr: 0.000000  loss: 1.3589 (1.2595)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2652 (2.4411)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:11    time: 3.9478  data: 3.5121  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.5617  data: 1.1708  max mem: 39406
Test: Total time: 0:00:04 (1.6386 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8123 Acc: 0.7500 Recall_macro: 0.8123 Recall_weighted: 0.7500 AUC-ROC: 0.9676 Weighted F1-score: 0.7720
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 8, 'Val Loss': 0.8363831639289856, 'Val BAcc': np.float64(0.8123315472729118), 'Val Acc': 0.75, 'Val ROC': np.float64(0.9676274348683489), 'Val W_F1': 0.7720007961880523, 'Val Recall_macro': 0.8123315472729118, 'Val Recall_weighted': 0.75}
Max val mean recall: 0.82%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [9]  [ 0/62]  eta: 0:04:57  lr: 0.000451  min_lr: 0.000000  loss: 1.3142 (1.3142)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0997 (2.0997)  time: 4.7906  data: 3.9364  max mem: 39406
Epoch: [9]  [10/62]  eta: 0:01:03  lr: 0.000459  min_lr: 0.000000  loss: 1.3142 (1.2270)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4139 (2.6090)  time: 1.2281  data: 0.3583  max mem: 39406
Epoch: [9]  [20/62]  eta: 0:00:44  lr: 0.000467  min_lr: 0.000000  loss: 1.2078 (1.2108)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2198 (2.4047)  time: 0.8748  data: 0.0004  max mem: 39406
Epoch: [9]  [30/62]  eta: 0:00:32  lr: 0.000475  min_lr: 0.000000  loss: 1.2263 (1.2258)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3919 (2.4926)  time: 0.8802  data: 0.0004  max mem: 39406
Epoch: [9]  [40/62]  eta: 0:00:21  lr: 0.000483  min_lr: 0.000000  loss: 1.3131 (1.2360)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4639 (2.4856)  time: 0.8852  data: 0.0005  max mem: 39406
Epoch: [9]  [50/62]  eta: 0:00:11  lr: 0.000491  min_lr: 0.000000  loss: 1.3963 (1.2538)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1953 (2.4234)  time: 0.8895  data: 0.0003  max mem: 39406
Epoch: [9]  [60/62]  eta: 0:00:01  lr: 0.000499  min_lr: 0.000000  loss: 1.3174 (1.2516)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1702 (2.4038)  time: 0.8926  data: 0.0001  max mem: 39406
Epoch: [9]  [61/62]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000000  loss: 1.3491 (1.2549)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1702 (2.4075)  time: 0.8928  data: 0.0001  max mem: 39406
Epoch: [9] Total time: 0:00:58 (0.9514 s / it)
2025-04-28 19:01:07 Averaged stats: lr: 0.000500  min_lr: 0.000000  loss: 1.3491 (1.2549)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1702 (2.4075)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4118  data: 2.9745  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3793  data: 0.9916  max mem: 39406
Test: Total time: 0:00:04 (1.4477 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8372 Acc: 0.7220 Recall_macro: 0.8372 Recall_weighted: 0.7220 AUC-ROC: 0.9654 Weighted F1-score: 0.7549
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 9, 'Val Loss': 0.8624188899993896, 'Val BAcc': np.float64(0.8371979388770434), 'Val Acc': 0.722, 'Val ROC': np.float64(0.9653558687647321), 'Val W_F1': 0.7548541380566335, 'Val Recall_macro': 0.8371979388770434, 'Val Recall_weighted': 0.722}
Max val mean recall: 0.84%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [10]  [ 0/62]  eta: 0:04:19  lr: 0.000500  min_lr: 0.000000  loss: 1.5630 (1.5630)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5438 (3.5438)  time: 4.1913  data: 3.3452  max mem: 39406
Epoch: [10]  [10/62]  eta: 0:01:00  lr: 0.000500  min_lr: 0.000000  loss: 1.3113 (1.2271)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2679 (2.3346)  time: 1.1635  data: 0.3046  max mem: 39406
Epoch: [10]  [20/62]  eta: 0:00:43  lr: 0.000500  min_lr: 0.000000  loss: 1.3113 (1.2437)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2705 (2.5242)  time: 0.8656  data: 0.0004  max mem: 39406
Epoch: [10]  [30/62]  eta: 0:00:31  lr: 0.000500  min_lr: 0.000000  loss: 1.3161 (1.2165)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4889 (2.4752)  time: 0.8742  data: 0.0004  max mem: 39406
Epoch: [10]  [40/62]  eta: 0:00:20  lr: 0.000500  min_lr: 0.000000  loss: 1.3059 (1.2252)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4422 (2.4719)  time: 0.8815  data: 0.0004  max mem: 39406
Epoch: [10]  [50/62]  eta: 0:00:11  lr: 0.000499  min_lr: 0.000000  loss: 1.3059 (1.2238)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3535 (2.4607)  time: 0.8880  data: 0.0003  max mem: 39406
Epoch: [10]  [60/62]  eta: 0:00:01  lr: 0.000499  min_lr: 0.000000  loss: 1.3473 (1.2290)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3055 (2.4239)  time: 0.8933  data: 0.0001  max mem: 39406
Epoch: [10]  [61/62]  eta: 0:00:00  lr: 0.000499  min_lr: 0.000000  loss: 1.3473 (1.2296)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3055 (2.4211)  time: 0.8939  data: 0.0001  max mem: 39406
Epoch: [10] Total time: 0:00:58 (0.9377 s / it)
2025-04-28 19:02:15 Averaged stats: lr: 0.000499  min_lr: 0.000000  loss: 1.3473 (1.2296)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3055 (2.4211)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4363  data: 2.9977  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3867  data: 0.9993  max mem: 39406
Test: Total time: 0:00:04 (1.4579 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8176 Acc: 0.7200 Recall_macro: 0.8176 Recall_weighted: 0.7200 AUC-ROC: 0.9729 Weighted F1-score: 0.7543
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 10, 'Val Loss': 0.8774797916412354, 'Val BAcc': np.float64(0.8175888166559808), 'Val Acc': 0.72, 'Val ROC': np.float64(0.9728825231829029), 'Val W_F1': 0.7543418138278077, 'Val Recall_macro': 0.8175888166559808, 'Val Recall_weighted': 0.72}
Max val mean recall: 0.84%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [11]  [ 0/62]  eta: 0:04:08  lr: 0.000499  min_lr: 0.000000  loss: 1.3830 (1.3830)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1402 (3.1402)  time: 4.0104  data: 3.1491  max mem: 39406
Epoch: [11]  [10/62]  eta: 0:01:00  lr: 0.000499  min_lr: 0.000000  loss: 1.4076 (1.2896)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3165 (2.5155)  time: 1.1606  data: 0.2867  max mem: 39406
Epoch: [11]  [20/62]  eta: 0:00:43  lr: 0.000499  min_lr: 0.000000  loss: 1.3219 (1.2687)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3061 (2.3899)  time: 0.8784  data: 0.0004  max mem: 39406
Epoch: [11]  [30/62]  eta: 0:00:31  lr: 0.000498  min_lr: 0.000000  loss: 1.2779 (1.2878)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3048 (2.3775)  time: 0.8833  data: 0.0004  max mem: 39406
Epoch: [11]  [40/62]  eta: 0:00:21  lr: 0.000498  min_lr: 0.000000  loss: 1.3824 (1.2897)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2694 (2.3813)  time: 0.8875  data: 0.0004  max mem: 39406
Epoch: [11]  [50/62]  eta: 0:00:11  lr: 0.000497  min_lr: 0.000000  loss: 1.3225 (1.2733)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2694 (2.3752)  time: 0.8914  data: 0.0003  max mem: 39406
Epoch: [11]  [60/62]  eta: 0:00:01  lr: 0.000497  min_lr: 0.000000  loss: 1.2793 (1.2546)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4332 (2.3902)  time: 0.8944  data: 0.0001  max mem: 39406
Epoch: [11]  [61/62]  eta: 0:00:00  lr: 0.000497  min_lr: 0.000000  loss: 1.2403 (1.2473)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4332 (2.3923)  time: 0.8947  data: 0.0001  max mem: 39406
Epoch: [11] Total time: 0:00:58 (0.9414 s / it)
2025-04-28 19:03:18 Averaged stats: lr: 0.000497  min_lr: 0.000000  loss: 1.2403 (1.2473)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4332 (2.3923)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.5705  data: 3.1435  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4323  data: 1.0480  max mem: 39406
Test: Total time: 0:00:04 (1.5084 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7953 Acc: 0.7960 Recall_macro: 0.7953 Recall_weighted: 0.7960 AUC-ROC: 0.9716 Weighted F1-score: 0.8111
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 11, 'Val Loss': 0.7785354852676392, 'Val BAcc': np.float64(0.7953477492656598), 'Val Acc': 0.796, 'Val ROC': np.float64(0.9716153060066791), 'Val W_F1': 0.8111033853487413, 'Val Recall_macro': 0.7953477492656598, 'Val Recall_weighted': 0.796}
Max val mean recall: 0.84%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [12]  [ 0/62]  eta: 0:04:15  lr: 0.000497  min_lr: 0.000000  loss: 1.1013 (1.1013)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9026 (2.9026)  time: 4.1277  data: 3.2675  max mem: 39406
Epoch: [12]  [10/62]  eta: 0:01:00  lr: 0.000496  min_lr: 0.000000  loss: 1.3290 (1.2587)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4688 (2.5344)  time: 1.1700  data: 0.2974  max mem: 39406
Epoch: [12]  [20/62]  eta: 0:00:43  lr: 0.000496  min_lr: 0.000000  loss: 1.2853 (1.2196)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2274 (2.4151)  time: 0.8768  data: 0.0004  max mem: 39406
Epoch: [12]  [30/62]  eta: 0:00:31  lr: 0.000495  min_lr: 0.000000  loss: 1.1738 (1.2092)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3263 (2.5085)  time: 0.8819  data: 0.0004  max mem: 39406
Epoch: [12]  [40/62]  eta: 0:00:21  lr: 0.000495  min_lr: 0.000000  loss: 1.2800 (1.2165)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4001 (2.4745)  time: 0.8866  data: 0.0004  max mem: 39406
Epoch: [12]  [50/62]  eta: 0:00:11  lr: 0.000494  min_lr: 0.000000  loss: 1.2513 (1.2001)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1097 (2.4414)  time: 0.8905  data: 0.0003  max mem: 39406
Epoch: [12]  [60/62]  eta: 0:00:01  lr: 0.000493  min_lr: 0.000000  loss: 1.2047 (1.2068)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3628 (2.4772)  time: 0.8935  data: 0.0001  max mem: 39406
Epoch: [12]  [61/62]  eta: 0:00:00  lr: 0.000493  min_lr: 0.000000  loss: 1.2047 (1.2108)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3628 (2.4749)  time: 0.8936  data: 0.0001  max mem: 39406
Epoch: [12] Total time: 0:00:58 (0.9422 s / it)
2025-04-28 19:04:21 Averaged stats: lr: 0.000493  min_lr: 0.000000  loss: 1.2047 (1.2108)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3628 (2.4749)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.3753  data: 2.9402  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3676  data: 0.9802  max mem: 39406
Test: Total time: 0:00:04 (1.4353 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8090 Acc: 0.6780 Recall_macro: 0.8090 Recall_weighted: 0.6780 AUC-ROC: 0.9657 Weighted F1-score: 0.7172
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 12, 'Val Loss': 0.9459312558174133, 'Val BAcc': np.float64(0.8089962897798718), 'Val Acc': 0.678, 'Val ROC': np.float64(0.96570823626365), 'Val W_F1': 0.7171791622097685, 'Val Recall_macro': 0.8089962897798718, 'Val Recall_weighted': 0.678}
Max val mean recall: 0.84%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [13]  [ 0/62]  eta: 0:04:04  lr: 0.000493  min_lr: 0.000000  loss: 1.0877 (1.0877)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6148 (2.6148)  time: 3.9392  data: 3.0791  max mem: 39406
Epoch: [13]  [10/62]  eta: 0:00:59  lr: 0.000492  min_lr: 0.000000  loss: 1.2554 (1.2293)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2323 (2.3141)  time: 1.1533  data: 0.2803  max mem: 39406
Epoch: [13]  [20/62]  eta: 0:00:42  lr: 0.000492  min_lr: 0.000000  loss: 1.2055 (1.2109)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2219 (2.2951)  time: 0.8778  data: 0.0004  max mem: 39406
Epoch: [13]  [30/62]  eta: 0:00:31  lr: 0.000491  min_lr: 0.000000  loss: 1.2037 (1.1848)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2202 (2.3252)  time: 0.8836  data: 0.0004  max mem: 39406
Epoch: [13]  [40/62]  eta: 0:00:21  lr: 0.000490  min_lr: 0.000000  loss: 1.2298 (1.1988)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2631 (2.3764)  time: 0.8879  data: 0.0004  max mem: 39406
Epoch: [13]  [50/62]  eta: 0:00:11  lr: 0.000489  min_lr: 0.000000  loss: 1.2919 (1.2010)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4788 (2.4067)  time: 0.8916  data: 0.0003  max mem: 39406
Epoch: [13]  [60/62]  eta: 0:00:01  lr: 0.000488  min_lr: 0.000000  loss: 1.2169 (1.1878)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3441 (2.3625)  time: 0.8945  data: 0.0001  max mem: 39406
Epoch: [13]  [61/62]  eta: 0:00:00  lr: 0.000488  min_lr: 0.000000  loss: 1.2169 (1.1929)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3441 (2.3721)  time: 0.8947  data: 0.0001  max mem: 39406
Epoch: [13] Total time: 0:00:58 (0.9402 s / it)
2025-04-28 19:05:24 Averaged stats: lr: 0.000488  min_lr: 0.000000  loss: 1.2169 (1.1929)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3441 (2.3721)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4216  data: 2.9821  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.3809  data: 0.9942  max mem: 39406
Test: Total time: 0:00:04 (1.4472 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8599 Acc: 0.8080 Recall_macro: 0.8599 Recall_weighted: 0.8080 AUC-ROC: 0.9734 Weighted F1-score: 0.8221
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 13, 'Val Loss': 0.7207189202308655, 'Val BAcc': np.float64(0.8598767526006331), 'Val Acc': 0.808, 'Val ROC': np.float64(0.9734074614360323), 'Val W_F1': 0.8220780814810571, 'Val Recall_macro': 0.8598767526006331, 'Val Recall_weighted': 0.808}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [14]  [ 0/62]  eta: 0:04:18  lr: 0.000488  min_lr: 0.000000  loss: 1.3614 (1.3614)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3072 (2.3072)  time: 4.1770  data: 3.3299  max mem: 39406
Epoch: [14]  [10/62]  eta: 0:01:00  lr: 0.000487  min_lr: 0.000000  loss: 1.0188 (1.0180)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3666 (2.4011)  time: 1.1625  data: 0.3031  max mem: 39406
Epoch: [14]  [20/62]  eta: 0:00:42  lr: 0.000486  min_lr: 0.000000  loss: 1.0622 (1.0627)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4070 (2.4576)  time: 0.8654  data: 0.0004  max mem: 39406
Epoch: [14]  [30/62]  eta: 0:00:31  lr: 0.000485  min_lr: 0.000000  loss: 1.2307 (1.1012)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4696 (2.4448)  time: 0.8737  data: 0.0004  max mem: 39406
Epoch: [14]  [40/62]  eta: 0:00:20  lr: 0.000484  min_lr: 0.000000  loss: 1.1651 (1.0938)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4344 (2.4265)  time: 0.8807  data: 0.0004  max mem: 39406
Epoch: [14]  [50/62]  eta: 0:00:11  lr: 0.000482  min_lr: 0.000000  loss: 1.1515 (1.1082)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2053 (2.3771)  time: 0.8866  data: 0.0002  max mem: 39406
Epoch: [14]  [60/62]  eta: 0:00:01  lr: 0.000481  min_lr: 0.000000  loss: 1.1676 (1.1277)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1424 (2.3557)  time: 0.8918  data: 0.0001  max mem: 39406
Epoch: [14]  [61/62]  eta: 0:00:00  lr: 0.000481  min_lr: 0.000000  loss: 1.1676 (1.1326)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1424 (2.3592)  time: 0.8922  data: 0.0001  max mem: 39406
Epoch: [14] Total time: 0:00:58 (0.9360 s / it)
2025-04-28 19:06:32 Averaged stats: lr: 0.000481  min_lr: 0.000000  loss: 1.1676 (1.1326)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1424 (2.3592)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:10    time: 3.4844  data: 3.0502  max mem: 39406
Test:  [2/3]  eta: 0:00:01    time: 1.4039  data: 1.0168  max mem: 39406
Test: Total time: 0:00:04 (1.4668 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8102 Acc: 0.6940 Recall_macro: 0.8102 Recall_weighted: 0.6940 AUC-ROC: 0.9694 Weighted F1-score: 0.7302
Predictions for val saved to /home/share/FM_Code/PanDerm/HAM_Res/val.csv
-------------------------- {'Epoch': 14, 'Val Loss': 0.9575327038764954, 'Val BAcc': np.float64(0.8101973896750015), 'Val Acc': 0.694, 'Val ROC': np.float64(0.9693935875596474), 'Val W_F1': 0.7302458112698462, 'Val Recall_macro': 0.8101973896750015, 'Val Recall_weighted': 0.694}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [15]  [ 0/62]  eta: 0:04:22  lr: 0.000481  min_lr: 0.000000  loss: 1.0886 (1.0886)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9345 (1.9345)  time: 4.2277  data: 3.3649  max mem: 39406
Epoch: [15]  [10/62]  eta: 0:01:01  lr: 0.000480  min_lr: 0.000000  loss: 0.9266 (0.9649)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4819 (2.5360)  time: 1.1795  data: 0.3063  max mem: 39406
