Not using distributed mode
Namespace(mode='train', batch_size=128, epochs=50, update_freq=1, save_ckpt_freq=5, model='PanDerm_Large_FT', rel_pos_bias=True, sin_pos_emb=True, layer_scale_init_value=0.1, ood_eval=False, input_size=224, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, weights=True, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, percent_data=1.0, TTA=False, monitor='acc', opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.65, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=10, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', pretrained_checkpoint='/home/share/FM_Code/Stage1/PanDerm/Model_Weights/panderm_ll_data6_checkpoint-499.pth', model_key='model|module|state_dict', model_prefix='', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, data_path='/datasets01/imagenet_full_size/061417/', eval_data_path=None, test_csv_path=None, image_key='image', nb_classes=6, imagenet_default_mean_and_std=True, data_set='IMNET', csv_path='/home/share/Uni_Eval/pad-ufes/2000.csv', root_path='/home/share/Uni_Eval/pad-ufes/images/', output_dir='/home/share/FM_Code/PanDerm/PAD_Res/', log_dir=None, device='cuda', seed=122, resume='', auto_resume=False, wandb_name='Reproduce_PAD_FT_122', save_ckpt=True, start_epoch=0, eval=False, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, enable_linear_eval=False, enable_multi_print=False, exp_name='pad finetune and eval', distributed=False)
Label distribution:
Label 0: 174
Label 1: 543
Label 2: 464
Label 3: 154
Label 4: 123
Label 5: 35
Using WeightedRandomSampler
train size: 1493 ,val size: 344 ,test size: 461
Mixup is activated!
/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=6, bias=True)
)
Patch size = (16, 16)
Load ckpt from /home/share/FM_Code/Stage1/PanDerm/Model_Weights/panderm_ll_data6_checkpoint-499.pth
Load state_dict by model_key = model
all keys: ['encoder.cls_token', 'encoder.pos_embed', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.blocks.0.gamma_1', 'encoder.blocks.0.gamma_2', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.q_bias', 'encoder.blocks.0.attn.v_bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.gamma_1', 'encoder.blocks.1.gamma_2', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.q_bias', 'encoder.blocks.1.attn.v_bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.gamma_1', 'encoder.blocks.2.gamma_2', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.q_bias', 'encoder.blocks.2.attn.v_bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.gamma_1', 'encoder.blocks.3.gamma_2', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.q_bias', 'encoder.blocks.3.attn.v_bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.gamma_1', 'encoder.blocks.4.gamma_2', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.q_bias', 'encoder.blocks.4.attn.v_bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.gamma_1', 'encoder.blocks.5.gamma_2', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.q_bias', 'encoder.blocks.5.attn.v_bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.gamma_1', 'encoder.blocks.6.gamma_2', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.q_bias', 'encoder.blocks.6.attn.v_bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.gamma_1', 'encoder.blocks.7.gamma_2', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.q_bias', 'encoder.blocks.7.attn.v_bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.blocks.8.gamma_1', 'encoder.bl
##############new keys: 454 odict_keys(['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight', 'regresser.regressor_
Weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_bias_table', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.3.attn.relative_position_bias_table', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.6.attn.relative_position_bias_table', 'blocks.7.attn.relative_position_bias_table', 'blocks.8.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.10.attn.relative_position_bias_table', 'blocks.11.attn.relative_position_bias_table', 'blocks.12.attn.relative_position_bias_table', 'blocks.13.attn.relative_position_bias_table', 'blocks.14.attn.relative_position_bias_table', 'blocks.15.attn.relative_position_bias_table', 'blocks.16.attn.relative_position_bias_table', 'blocks.17.attn.relative_position_bias_table', 'blocks.18.attn.relative_position_bias_table', 'blocks.19.attn.relative_position_bias_table', 'blocks.20.attn.relative_position_bias_table', 'blocks.21.attn.relative_position_bias_table', 'blocks.22.attn.relative_position_bias_table', 'blocks.23.attn.relative_position_bias_table', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight',
Ignored weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_index', 'blocks.1.attn.relative_position_index', 'blocks.2.attn.relative_position_index', 'blocks.3.attn.relative_position_index', 'blocks.4.attn.relative_position_index', 'blocks.5.attn.relative_position_index', 'blocks.6.attn.relative_position_index', 'blocks.7.attn.relative_position_index', 'blocks.8.attn.relative_position_index', 'blocks.9.attn.relative_position_index', 'blocks.10.attn.relative_position_index', 'blocks.11.attn.relative_position_index', 'blocks.12.attn.relative_position_index', 'blocks.13.attn.relative_position_index', 'blocks.14.attn.relative_position_index', 'blocks.15.attn.relative_position_index', 'blocks.16.attn.relative_position_index', 'blocks.17.attn.relative_position_index', 'blocks.18.attn.relative_position_index', 'blocks.19.attn.relative_position_index', 'blocks.20.attn.relative_position_index', 'blocks.21.attn.relative_position_index', 'blocks.22.attn.relative_position_index', 'blocks.23.attn.relative_position_index']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=6, bias=True)
)
number of params: 303411718
LR = 0.00050000
Batch size = 128
Update frequent = 1
Number of training examples = 1493
Number of training training per epoch = 11
Assigned values = [2.1029740616282293e-05, 3.2353447101972754e-05, 4.977453400303501e-05, 7.65762061585154e-05, 0.00011780954793617752, 0.00018124545836335003, 0.0002788391667128462, 0.0004289833334043787, 0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "patch_embed.proj.bias"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.gamma_1",
      "blocks.0.gamma_2",
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.relative_position_bias_table",
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.gamma_1",
      "blocks.1.gamma_2",
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.relative_position_bias_table",
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.gamma_1",
      "blocks.2.gamma_2",
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.relative_position_bias_table",
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.gamma_1",
      "blocks.3.gamma_2",
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.relative_position_bias_table",
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.gamma_1",
      "blocks.4.gamma_2",
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.relative_position_bias_table",
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.gamma_1",
      "blocks.5.gamma_2",
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.relative_position_bias_table",
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.gamma_1",
      "blocks.6.gamma_2",
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.relative_position_bias_table",
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.gamma_1",
      "blocks.7.gamma_2",
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.relative_position_bias_table",
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.gamma_1",
      "blocks.8.gamma_2",
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.relative_position_bias_table",
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.gamma_1",
      "blocks.9.gamma_2",
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.relative_position_bias_table",
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.gamma_1",
      "blocks.10.gamma_2",
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.relative_position_bias_table",
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.gamma_1",
      "blocks.11.gamma_2",
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.relative_position_bias_table",
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.12.gamma_1",
      "blocks.12.gamma_2",
      "blocks.12.norm1.weight",
      "blocks.12.norm1.bias",
      "blocks.12.attn.q_bias",
      "blocks.12.attn.v_bias",
      "blocks.12.attn.proj.bias",
      "blocks.12.norm2.weight",
      "blocks.12.norm2.bias",
      "blocks.12.mlp.fc1.bias",
      "blocks.12.mlp.fc2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.12.attn.relative_position_bias_table",
      "blocks.12.attn.qkv.weight",
      "blocks.12.attn.proj.weight",
      "blocks.12.mlp.fc1.weight",
      "blocks.12.mlp.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_14_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.13.gamma_1",
      "blocks.13.gamma_2",
      "blocks.13.norm1.weight",
      "blocks.13.norm1.bias",
      "blocks.13.attn.q_bias",
      "blocks.13.attn.v_bias",
      "blocks.13.attn.proj.bias",
      "blocks.13.norm2.weight",
      "blocks.13.norm2.bias",
      "blocks.13.mlp.fc1.bias",
      "blocks.13.mlp.fc2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_14_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.13.attn.relative_position_bias_table",
      "blocks.13.attn.qkv.weight",
      "blocks.13.attn.proj.weight",
      "blocks.13.mlp.fc1.weight",
      "blocks.13.mlp.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_15_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.14.gamma_1",
      "blocks.14.gamma_2",
      "blocks.14.norm1.weight",
      "blocks.14.norm1.bias",
      "blocks.14.attn.q_bias",
      "blocks.14.attn.v_bias",
      "blocks.14.attn.proj.bias",
      "blocks.14.norm2.weight",
      "blocks.14.norm2.bias",
      "blocks.14.mlp.fc1.bias",
      "blocks.14.mlp.fc2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_15_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.14.attn.relative_position_bias_table",
      "blocks.14.attn.qkv.weight",
      "blocks.14.attn.proj.weight",
      "blocks.14.mlp.fc1.weight",
      "blocks.14.mlp.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_16_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.15.gamma_1",
      "blocks.15.gamma_2",
      "blocks.15.norm1.weight",
      "blocks.15.norm1.bias",
      "blocks.15.attn.q_bias",
      "blocks.15.attn.v_bias",
      "blocks.15.attn.proj.bias",
      "blocks.15.norm2.weight",
      "blocks.15.norm2.bias",
      "blocks.15.mlp.fc1.bias",
      "blocks.15.mlp.fc2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_16_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.15.attn.relative_position_bias_table",
      "blocks.15.attn.qkv.weight",
      "blocks.15.attn.proj.weight",
      "blocks.15.mlp.fc1.weight",
      "blocks.15.mlp.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.16.gamma_1",
      "blocks.16.gamma_2",
      "blocks.16.norm1.weight",
      "blocks.16.norm1.bias",
      "blocks.16.attn.q_bias",
      "blocks.16.attn.v_bias",
      "blocks.16.attn.proj.bias",
      "blocks.16.norm2.weight",
      "blocks.16.norm2.bias",
      "blocks.16.mlp.fc1.bias",
      "blocks.16.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.16.attn.relative_position_bias_table",
      "blocks.16.attn.qkv.weight",
      "blocks.16.attn.proj.weight",
      "blocks.16.mlp.fc1.weight",
      "blocks.16.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_18_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.17.gamma_1",
      "blocks.17.gamma_2",
      "blocks.17.norm1.weight",
      "blocks.17.norm1.bias",
      "blocks.17.attn.q_bias",
      "blocks.17.attn.v_bias",
      "blocks.17.attn.proj.bias",
      "blocks.17.norm2.weight",
      "blocks.17.norm2.bias",
      "blocks.17.mlp.fc1.bias",
      "blocks.17.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_18_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.17.attn.relative_position_bias_table",
      "blocks.17.attn.qkv.weight",
      "blocks.17.attn.proj.weight",
      "blocks.17.mlp.fc1.weight",
      "blocks.17.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_19_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.18.gamma_1",
      "blocks.18.gamma_2",
      "blocks.18.norm1.weight",
      "blocks.18.norm1.bias",
      "blocks.18.attn.q_bias",
      "blocks.18.attn.v_bias",
      "blocks.18.attn.proj.bias",
      "blocks.18.norm2.weight",
      "blocks.18.norm2.bias",
      "blocks.18.mlp.fc1.bias",
      "blocks.18.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_19_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.18.attn.relative_position_bias_table",
      "blocks.18.attn.qkv.weight",
      "blocks.18.attn.proj.weight",
      "blocks.18.mlp.fc1.weight",
      "blocks.18.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_20_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.19.gamma_1",
      "blocks.19.gamma_2",
      "blocks.19.norm1.weight",
      "blocks.19.norm1.bias",
      "blocks.19.attn.q_bias",
      "blocks.19.attn.v_bias",
      "blocks.19.attn.proj.bias",
      "blocks.19.norm2.weight",
      "blocks.19.norm2.bias",
      "blocks.19.mlp.fc1.bias",
      "blocks.19.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_20_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.19.attn.relative_position_bias_table",
      "blocks.19.attn.qkv.weight",
      "blocks.19.attn.proj.weight",
      "blocks.19.mlp.fc1.weight",
      "blocks.19.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_21_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.20.gamma_1",
      "blocks.20.gamma_2",
      "blocks.20.norm1.weight",
      "blocks.20.norm1.bias",
      "blocks.20.attn.q_bias",
      "blocks.20.attn.v_bias",
      "blocks.20.attn.proj.bias",
      "blocks.20.norm2.weight",
      "blocks.20.norm2.bias",
      "blocks.20.mlp.fc1.bias",
      "blocks.20.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_21_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.20.attn.relative_position_bias_table",
      "blocks.20.attn.qkv.weight",
      "blocks.20.attn.proj.weight",
      "blocks.20.mlp.fc1.weight",
      "blocks.20.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_22_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.21.gamma_1",
      "blocks.21.gamma_2",
      "blocks.21.norm1.weight",
      "blocks.21.norm1.bias",
      "blocks.21.attn.q_bias",
      "blocks.21.attn.v_bias",
      "blocks.21.attn.proj.bias",
      "blocks.21.norm2.weight",
      "blocks.21.norm2.bias",
      "blocks.21.mlp.fc1.bias",
      "blocks.21.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_22_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.21.attn.relative_position_bias_table",
      "blocks.21.attn.qkv.weight",
      "blocks.21.attn.proj.weight",
      "blocks.21.mlp.fc1.weight",
      "blocks.21.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_23_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.22.gamma_1",
      "blocks.22.gamma_2",
      "blocks.22.norm1.weight",
      "blocks.22.norm1.bias",
      "blocks.22.attn.q_bias",
      "blocks.22.attn.v_bias",
      "blocks.22.attn.proj.bias",
      "blocks.22.norm2.weight",
      "blocks.22.norm2.bias",
      "blocks.22.mlp.fc1.bias",
      "blocks.22.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_23_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.22.attn.relative_position_bias_table",
      "blocks.22.attn.qkv.weight",
      "blocks.22.attn.proj.weight",
      "blocks.22.mlp.fc1.weight",
      "blocks.22.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_24_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.23.gamma_1",
      "blocks.23.gamma_2",
      "blocks.23.norm1.weight",
      "blocks.23.norm1.bias",
      "blocks.23.attn.q_bias",
      "blocks.23.attn.v_bias",
      "blocks.23.attn.proj.bias",
      "blocks.23.norm2.weight",
      "blocks.23.norm2.bias",
      "blocks.23.mlp.fc1.bias",
      "blocks.23.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_24_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.23.attn.relative_position_bias_table",
      "blocks.23.attn.qkv.weight",
      "blocks.23.attn.proj.weight",
      "blocks.23.mlp.fc1.weight",
      "blocks.23.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_25_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_25_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
/home/share/FM_Code/PanDerm/classification/furnace/utils.py:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
Use step level LR scheduler!
Set warmup steps = 110
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 50 epochs
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [0]  [ 0/11]  eta: 0:00:58  lr: 0.000000  min_lr: 0.000000  loss: 1.7918 (1.7918)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.9695 (0.9695)  time: 5.3366  data: 3.0988  max mem: 37088
Epoch: [0]  [10/11]  eta: 0:00:01  lr: 0.000046  min_lr: 0.000000  loss: 1.7915 (1.7912)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8382 (0.7836)  time: 1.1715  data: 0.2818  max mem: 39406
Epoch: [0] Total time: 0:00:13 (1.1835 s / it)
2025-04-29 11:30:03 Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 1.7915 (1.7912)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8382 (0.7836)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3759  data: 2.5454  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8914  data: 1.2728  max mem: 39406
Test: Total time: 0:00:03 (1.9516 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.3729 Acc: 0.4302 Recall_macro: 0.3729 Recall_weighted: 0.4302 AUC-ROC: 0.8581 Weighted F1-score: 0.3526
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 0, 'Val Loss': 1.7878738641738892, 'Val BAcc': np.float64(0.3728675168675169), 'Val Acc': 0.43023255813953487, 'Val ROC': np.float64(0.8581267178292508), 'Val W_F1': 0.3525949651496472, 'Val Recall_macro': 0.3728675168675169, 'Val Recall_weighted': 0.43023255813953487}
Max val mean accuracy: 0.43%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [1]  [ 0/11]  eta: 0:00:41  lr: 0.000050  min_lr: 0.000000  loss: 1.7882 (1.7882)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1725 (1.1725)  time: 3.7759  data: 3.0138  max mem: 39406
Epoch: [1]  [10/11]  eta: 0:00:01  lr: 0.000096  min_lr: 0.000000  loss: 1.7833 (1.7823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1743 (1.2213)  time: 1.0367  data: 0.2741  max mem: 39406
Epoch: [1] Total time: 0:00:11 (1.0506 s / it)
2025-04-29 11:30:24 Averaged stats: lr: 0.000096  min_lr: 0.000000  loss: 1.7833 (1.7823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1743 (1.2213)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0040  data: 2.6360  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6469  data: 1.3181  max mem: 39406
Test: Total time: 0:00:03 (1.7057 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5025 Acc: 0.2878 Recall_macro: 0.5025 Recall_weighted: 0.2878 AUC-ROC: 0.8827 Weighted F1-score: 0.2858
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 1, 'Val Loss': 1.7634598016738892, 'Val BAcc': np.float64(0.5025387398290624), 'Val Acc': 0.2877906976744186, 'Val ROC': np.float64(0.8826518135622546), 'Val W_F1': 0.2857953195506727, 'Val Recall_macro': 0.5025387398290624, 'Val Recall_weighted': 0.2877906976744186}
Max val mean accuracy: 0.43%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [2]  [ 0/11]  eta: 0:00:39  lr: 0.000101  min_lr: 0.000000  loss: 1.7592 (1.7592)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3563 (1.3563)  time: 3.6251  data: 2.8600  max mem: 39406
Epoch: [2]  [10/11]  eta: 0:00:01  lr: 0.000147  min_lr: 0.000000  loss: 1.7333 (1.7135)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8357 (1.7744)  time: 1.0307  data: 0.2601  max mem: 39406
Epoch: [2] Total time: 0:00:11 (1.0457 s / it)
2025-04-29 11:30:39 Averaged stats: lr: 0.000147  min_lr: 0.000000  loss: 1.7333 (1.7135)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8357 (1.7744)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0147  data: 2.6472  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6539  data: 1.3237  max mem: 39406
Test: Total time: 0:00:03 (1.7176 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5252 Acc: 0.4709 Recall_macro: 0.5252 Recall_weighted: 0.4709 AUC-ROC: 0.8708 Weighted F1-score: 0.5020
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 2, 'Val Loss': 1.6007529497146606, 'Val BAcc': np.float64(0.5251813422136002), 'Val Acc': 0.47093023255813954, 'Val ROC': np.float64(0.8707628133600909), 'Val W_F1': 0.5019549227809496, 'Val Recall_macro': 0.5251813422136002, 'Val Recall_weighted': 0.47093023255813954}
Max val mean accuracy: 0.47%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [3]  [ 0/11]  eta: 0:00:39  lr: 0.000151  min_lr: 0.000000  loss: 1.6823 (1.6823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0131 (2.0131)  time: 3.5906  data: 2.8175  max mem: 39406
Epoch: [3]  [10/11]  eta: 0:00:01  lr: 0.000197  min_lr: 0.000000  loss: 1.4879 (1.5289)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9687 (1.9766)  time: 1.0327  data: 0.2562  max mem: 39406
Epoch: [3] Total time: 0:00:11 (1.0468 s / it)
2025-04-29 11:31:01 Averaged stats: lr: 0.000197  min_lr: 0.000000  loss: 1.4879 (1.5289)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9687 (1.9766)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0401  data: 2.6712  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6675  data: 1.3357  max mem: 39406
Test: Total time: 0:00:03 (1.7305 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5271 Acc: 0.3866 Recall_macro: 0.5271 Recall_weighted: 0.3866 AUC-ROC: 0.8943 Weighted F1-score: 0.3583
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 3, 'Val Loss': 1.322593331336975, 'Val BAcc': np.float64(0.5271137362750266), 'Val Acc': 0.3866279069767442, 'Val ROC': np.float64(0.8943233586196984), 'Val W_F1': 0.3583329386633155, 'Val Recall_macro': 0.5271137362750266, 'Val Recall_weighted': 0.3866279069767442}
Max val mean accuracy: 0.47%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [4]  [ 0/11]  eta: 0:00:40  lr: 0.000202  min_lr: 0.000000  loss: 1.4084 (1.4084)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5380 (1.5380)  time: 3.6731  data: 2.8964  max mem: 39406
Epoch: [4]  [10/11]  eta: 0:00:01  lr: 0.000248  min_lr: 0.000000  loss: 1.4401 (1.4568)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6765 (1.7491)  time: 1.0489  data: 0.2634  max mem: 39406
Epoch: [4] Total time: 0:00:11 (1.0631 s / it)
2025-04-29 11:31:16 Averaged stats: lr: 0.000248  min_lr: 0.000000  loss: 1.4401 (1.4568)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6765 (1.7491)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1461  data: 2.7719  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7219  data: 1.3860  max mem: 39406
Test: Total time: 0:00:03 (1.7807 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5882 Acc: 0.6570 Recall_macro: 0.5882 Recall_weighted: 0.6570 AUC-ROC: 0.8960 Weighted F1-score: 0.6450
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 4, 'Val Loss': 1.1074732542037964, 'Val BAcc': np.float64(0.5882122591800011), 'Val Acc': 0.6569767441860465, 'Val ROC': np.float64(0.8959738323252814), 'Val W_F1': 0.6449929296901872, 'Val Recall_macro': 0.5882122591800011, 'Val Recall_weighted': 0.6569767441860465}
Max val mean accuracy: 0.66%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [5]  [ 0/11]  eta: 0:00:41  lr: 0.000252  min_lr: 0.000000  loss: 1.2636 (1.2636)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8995 (1.8995)  time: 3.7898  data: 3.0063  max mem: 39406
Epoch: [5]  [10/11]  eta: 0:00:01  lr: 0.000298  min_lr: 0.000000  loss: 1.3655 (1.4171)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7757 (1.8367)  time: 1.0612  data: 0.2734  max mem: 39406
Epoch: [5] Total time: 0:00:11 (1.0747 s / it)
2025-04-29 11:31:38 Averaged stats: lr: 0.000298  min_lr: 0.000000  loss: 1.3655 (1.4171)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7757 (1.8367)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0429  data: 2.6630  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6718  data: 1.3316  max mem: 39406
Test: Total time: 0:00:03 (1.7326 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6685 Acc: 0.5959 Recall_macro: 0.6685 Recall_weighted: 0.5959 AUC-ROC: 0.9085 Weighted F1-score: 0.6223
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 5, 'Val Loss': 1.0368362665176392, 'Val BAcc': np.float64(0.6684628507854313), 'Val Acc': 0.5959302325581395, 'Val ROC': np.float64(0.908511645955956), 'Val W_F1': 0.6223315161050064, 'Val Recall_macro': 0.6684628507854313, 'Val Recall_weighted': 0.5959302325581395}
Max val mean accuracy: 0.66%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [6]  [ 0/11]  eta: 0:00:41  lr: 0.000303  min_lr: 0.000000  loss: 1.4280 (1.4280)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7663 (1.7663)  time: 3.7559  data: 2.9690  max mem: 39406
Epoch: [6]  [10/11]  eta: 0:00:01  lr: 0.000349  min_lr: 0.000000  loss: 1.3841 (1.3709)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1205 (2.3355)  time: 1.0638  data: 0.2700  max mem: 39406
Epoch: [6] Total time: 0:00:11 (1.0774 s / it)
2025-04-29 11:31:53 Averaged stats: lr: 0.000349  min_lr: 0.000000  loss: 1.3841 (1.3709)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1205 (2.3355)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0395  data: 2.6555  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6711  data: 1.3278  max mem: 39406
Test: Total time: 0:00:03 (1.7418 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6538 Acc: 0.7035 Recall_macro: 0.6538 Recall_weighted: 0.7035 AUC-ROC: 0.9231 Weighted F1-score: 0.6980
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 6, 'Val Loss': 0.9955396056175232, 'Val BAcc': np.float64(0.653765098152195), 'Val Acc': 0.7034883720930233, 'Val ROC': np.float64(0.9231356171124183), 'Val W_F1': 0.6980351482685704, 'Val Recall_macro': 0.653765098152195, 'Val Recall_weighted': 0.7034883720930233}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [7]  [ 0/11]  eta: 0:00:40  lr: 0.000353  min_lr: 0.000000  loss: 1.3784 (1.3784)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0507 (2.0507)  time: 3.6579  data: 2.8723  max mem: 39406
Epoch: [7]  [10/11]  eta: 0:00:01  lr: 0.000399  min_lr: 0.000000  loss: 1.3784 (1.3263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0507 (2.0922)  time: 1.0560  data: 0.2612  max mem: 39406
Epoch: [7] Total time: 0:00:11 (1.0689 s / it)
2025-04-29 11:32:15 Averaged stats: lr: 0.000399  min_lr: 0.000000  loss: 1.3784 (1.3263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0507 (2.0922)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1314  data: 2.7484  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7174  data: 1.3743  max mem: 39406
Test: Total time: 0:00:03 (1.7904 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6506 Acc: 0.5233 Recall_macro: 0.6506 Recall_weighted: 0.5233 AUC-ROC: 0.9311 Weighted F1-score: 0.5547
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 7, 'Val Loss': 1.0241409540176392, 'Val BAcc': np.float64(0.6506393850909981), 'Val Acc': 0.5232558139534884, 'Val ROC': np.float64(0.9310714787347059), 'Val W_F1': 0.5546605494033601, 'Val Recall_macro': 0.6506393850909981, 'Val Recall_weighted': 0.5232558139534884}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [8]  [ 0/11]  eta: 0:00:41  lr: 0.000404  min_lr: 0.000000  loss: 1.3378 (1.3378)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6778 (2.6778)  time: 3.7419  data: 2.9489  max mem: 39406
Epoch: [8]  [10/11]  eta: 0:00:01  lr: 0.000450  min_lr: 0.000000  loss: 1.3429 (1.3703)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8792 (1.8556)  time: 1.0676  data: 0.2682  max mem: 39406
Epoch: [8] Total time: 0:00:11 (1.0821 s / it)
2025-04-29 11:32:30 Averaged stats: lr: 0.000450  min_lr: 0.000000  loss: 1.3429 (1.3703)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8792 (1.8556)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1745  data: 2.7908  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7399  data: 1.3955  max mem: 39406
Test: Total time: 0:00:03 (1.8103 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7056 Acc: 0.7006 Recall_macro: 0.7056 Recall_weighted: 0.7006 AUC-ROC: 0.9314 Weighted F1-score: 0.7147
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 8, 'Val Loss': 0.8987619280815125, 'Val BAcc': np.float64(0.7055605769154156), 'Val Acc': 0.7005813953488372, 'Val ROC': np.float64(0.9313582178055532), 'Val W_F1': 0.7147298661895573, 'Val Recall_macro': 0.7055605769154156, 'Val Recall_weighted': 0.7005813953488372}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [9]  [ 0/11]  eta: 0:00:41  lr: 0.000454  min_lr: 0.000000  loss: 1.0986 (1.0986)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9033 (1.9033)  time: 3.7920  data: 3.0002  max mem: 39406
Epoch: [9]  [10/11]  eta: 0:00:01  lr: 0.000500  min_lr: 0.000000  loss: 1.3878 (1.3658)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0659 (1.9823)  time: 1.0759  data: 0.2728  max mem: 39406
Epoch: [9] Total time: 0:00:11 (1.0898 s / it)
2025-04-29 11:32:46 Averaged stats: lr: 0.000500  min_lr: 0.000000  loss: 1.3878 (1.3658)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0659 (1.9823)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1481  data: 2.7611  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7277  data: 1.3807  max mem: 39406
Test: Total time: 0:00:03 (1.7963 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6888 Acc: 0.6337 Recall_macro: 0.6888 Recall_weighted: 0.6337 AUC-ROC: 0.9250 Weighted F1-score: 0.6632
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 9, 'Val Loss': 0.9622971415519714, 'Val BAcc': np.float64(0.6888031055772991), 'Val Acc': 0.6337209302325582, 'Val ROC': np.float64(0.9250477914155603), 'Val W_F1': 0.663248867386957, 'Val Recall_macro': 0.6888031055772991, 'Val Recall_weighted': 0.6337209302325582}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [10]  [ 0/11]  eta: 0:00:41  lr: 0.000500  min_lr: 0.000000  loss: 1.1440 (1.1440)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5634 (1.5634)  time: 3.7578  data: 2.9603  max mem: 39406
Epoch: [10]  [10/11]  eta: 0:00:01  lr: 0.000499  min_lr: 0.000000  loss: 1.4338 (1.3995)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1147 (2.1905)  time: 1.0775  data: 0.2692  max mem: 39406
Epoch: [10] Total time: 0:00:12 (1.0924 s / it)
2025-04-29 11:33:02 Averaged stats: lr: 0.000499  min_lr: 0.000000  loss: 1.4338 (1.3995)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1147 (2.1905)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1673  data: 2.7827  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7387  data: 1.3915  max mem: 39406
Test: Total time: 0:00:03 (1.8104 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6943 Acc: 0.6744 Recall_macro: 0.6943 Recall_weighted: 0.6744 AUC-ROC: 0.9290 Weighted F1-score: 0.7018
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 10, 'Val Loss': 0.924159824848175, 'Val BAcc': np.float64(0.6943153190895126), 'Val Acc': 0.6744186046511628, 'Val ROC': np.float64(0.9290409925964176), 'Val W_F1': 0.701816811968726, 'Val Recall_macro': 0.6943153190895126, 'Val Recall_weighted': 0.6744186046511628}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [11]  [ 0/11]  eta: 0:00:41  lr: 0.000499  min_lr: 0.000000  loss: 1.4134 (1.4134)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5671 (1.5671)  time: 3.7665  data: 2.9718  max mem: 39406
Epoch: [11]  [10/11]  eta: 0:00:01  lr: 0.000497  min_lr: 0.000000  loss: 1.3955 (1.3253)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8032 (1.8095)  time: 1.0766  data: 0.2702  max mem: 39406
Epoch: [11] Total time: 0:00:11 (1.0893 s / it)
2025-04-29 11:33:17 Averaged stats: lr: 0.000497  min_lr: 0.000000  loss: 1.3955 (1.3253)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8032 (1.8095)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0392  data: 2.6558  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6742  data: 1.3280  max mem: 39406
Test: Total time: 0:00:03 (1.7429 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6719 Acc: 0.6831 Recall_macro: 0.6719 Recall_weighted: 0.6831 AUC-ROC: 0.9310 Weighted F1-score: 0.7085
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 11, 'Val Loss': 0.9239204525947571, 'Val BAcc': np.float64(0.6719041739686902), 'Val Acc': 0.6831395348837209, 'Val ROC': np.float64(0.930950787379995), 'Val W_F1': 0.7085497674534487, 'Val Recall_macro': 0.6719041739686902, 'Val Recall_weighted': 0.6831395348837209}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [12]  [ 0/11]  eta: 0:00:40  lr: 0.000497  min_lr: 0.000000  loss: 1.3502 (1.3502)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7790 (1.7790)  time: 3.6513  data: 2.8484  max mem: 39406
Epoch: [12]  [10/11]  eta: 0:00:01  lr: 0.000494  min_lr: 0.000000  loss: 1.1593 (1.1934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7790 (1.7291)  time: 1.0699  data: 0.2591  max mem: 39406
Epoch: [12] Total time: 0:00:11 (1.0826 s / it)
2025-04-29 11:33:33 Averaged stats: lr: 0.000494  min_lr: 0.000000  loss: 1.1593 (1.1934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7790 (1.7291)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.2467  data: 2.8609  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7780  data: 1.4305  max mem: 39406
Test: Total time: 0:00:03 (1.8379 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6625 Acc: 0.7209 Recall_macro: 0.6625 Recall_weighted: 0.7209 AUC-ROC: 0.9388 Weighted F1-score: 0.7366
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 12, 'Val Loss': 0.835507869720459, 'Val BAcc': np.float64(0.6624620565265728), 'Val Acc': 0.7209302325581395, 'Val ROC': np.float64(0.9388255475963021), 'Val W_F1': 0.7366263783434405, 'Val Recall_macro': 0.6624620565265728, 'Val Recall_weighted': 0.7209302325581395}
Max val mean accuracy: 0.72%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [13]  [ 0/11]  eta: 0:00:40  lr: 0.000493  min_lr: 0.000000  loss: 1.5605 (1.5605)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8822 (1.8822)  time: 3.7059  data: 2.9114  max mem: 39406
Epoch: [13]  [10/11]  eta: 0:00:01  lr: 0.000488  min_lr: 0.000000  loss: 1.3639 (1.2604)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9152 (1.8631)  time: 1.0675  data: 0.2648  max mem: 39406
Epoch: [13] Total time: 0:00:11 (1.0821 s / it)
2025-04-29 11:33:55 Averaged stats: lr: 0.000488  min_lr: 0.000000  loss: 1.3639 (1.2604)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9152 (1.8631)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1128  data: 2.7275  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7099  data: 1.3638  max mem: 39406
Test: Total time: 0:00:03 (1.7718 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6322 Acc: 0.7384 Recall_macro: 0.6322 Recall_weighted: 0.7384 AUC-ROC: 0.9365 Weighted F1-score: 0.7433
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 13, 'Val Loss': 0.7887508869171143, 'Val BAcc': np.float64(0.6322273408725022), 'Val Acc': 0.7383720930232558, 'Val ROC': np.float64(0.9365391549263555), 'Val W_F1': 0.7433490246517096, 'Val Recall_macro': 0.6322273408725022, 'Val Recall_weighted': 0.7383720930232558}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [14]  [ 0/11]  eta: 0:00:40  lr: 0.000488  min_lr: 0.000000  loss: 1.1635 (1.1635)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8561 (1.8561)  time: 3.7052  data: 2.9096  max mem: 39406
Epoch: [14]  [10/11]  eta: 0:00:01  lr: 0.000482  min_lr: 0.000000  loss: 1.1108 (1.1295)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9752 (2.1131)  time: 1.0654  data: 0.2646  max mem: 39406
Epoch: [14] Total time: 0:00:11 (1.0799 s / it)
2025-04-29 11:34:16 Averaged stats: lr: 0.000482  min_lr: 0.000000  loss: 1.1108 (1.1295)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9752 (2.1131)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0319  data: 2.6484  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6677  data: 1.3243  max mem: 39406
Test: Total time: 0:00:03 (1.7348 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6853 Acc: 0.6744 Recall_macro: 0.6853 Recall_weighted: 0.6744 AUC-ROC: 0.9387 Weighted F1-score: 0.7000
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 14, 'Val Loss': 0.8104360699653625, 'Val BAcc': np.float64(0.685323906227132), 'Val Acc': 0.6744186046511628, 'Val ROC': np.float64(0.9386737038252974), 'Val W_F1': 0.7000034559451316, 'Val Recall_macro': 0.685323906227132, 'Val Recall_weighted': 0.6744186046511628}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [15]  [ 0/11]  eta: 0:00:40  lr: 0.000481  min_lr: 0.000000  loss: 1.5588 (1.5588)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5062 (3.5062)  time: 3.6808  data: 2.8850  max mem: 39406
Epoch: [15]  [10/11]  eta: 0:00:01  lr: 0.000474  min_lr: 0.000000  loss: 1.5060 (1.4263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8664 (2.2226)  time: 1.0676  data: 0.2624  max mem: 39406
Epoch: [15] Total time: 0:00:11 (1.0806 s / it)
2025-04-29 11:34:31 Averaged stats: lr: 0.000474  min_lr: 0.000000  loss: 1.5060 (1.4263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8664 (2.2226)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0872  data: 2.7010  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6985  data: 1.3506  max mem: 39406
Test: Total time: 0:00:03 (1.7609 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7313 Acc: 0.6570 Recall_macro: 0.7313 Recall_weighted: 0.6570 AUC-ROC: 0.9397 Weighted F1-score: 0.6917
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 15, 'Val Loss': 0.8888662457466125, 'Val BAcc': np.float64(0.7313448110867466), 'Val Acc': 0.6569767441860465, 'Val ROC': np.float64(0.9396880354059344), 'Val W_F1': 0.6917080232738627, 'Val Recall_macro': 0.7313448110867466, 'Val Recall_weighted': 0.6569767441860465}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [16]  [ 0/11]  eta: 0:00:40  lr: 0.000473  min_lr: 0.000000  loss: 1.2632 (1.2632)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8135 (1.8135)  time: 3.6713  data: 2.8710  max mem: 39406
Epoch: [16]  [10/11]  eta: 0:00:01  lr: 0.000464  min_lr: 0.000000  loss: 1.3736 (1.3449)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7611 (1.8175)  time: 1.0706  data: 0.2611  max mem: 39406
Epoch: [16] Total time: 0:00:11 (1.0842 s / it)
2025-04-29 11:34:47 Averaged stats: lr: 0.000464  min_lr: 0.000000  loss: 1.3736 (1.3449)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7611 (1.8175)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:05    time: 2.9970  data: 2.6079  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6528  data: 1.3040  max mem: 39406
Test: Total time: 0:00:03 (1.7080 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6799 Acc: 0.6395 Recall_macro: 0.6799 Recall_weighted: 0.6395 AUC-ROC: 0.9391 Weighted F1-score: 0.6761
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 16, 'Val Loss': 0.88738614320755, 'Val BAcc': np.float64(0.6798857743373873), 'Val Acc': 0.6395348837209303, 'Val ROC': np.float64(0.9390851899969359), 'Val W_F1': 0.6761173159214264, 'Val Recall_macro': 0.6798857743373873, 'Val Recall_weighted': 0.6395348837209303}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [17]  [ 0/11]  eta: 0:00:41  lr: 0.000463  min_lr: 0.000000  loss: 1.3015 (1.3015)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8954 (1.8954)  time: 3.7556  data: 2.9533  max mem: 39406
Epoch: [17]  [10/11]  eta: 0:00:01  lr: 0.000453  min_lr: 0.000000  loss: 1.3224 (1.2776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8038 (1.7905)  time: 1.0869  data: 0.2686  max mem: 39406
Epoch: [17] Total time: 0:00:12 (1.1012 s / it)
2025-04-29 11:35:03 Averaged stats: lr: 0.000453  min_lr: 0.000000  loss: 1.3224 (1.2776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8038 (1.7905)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0479  data: 2.6655  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6778  data: 1.3328  max mem: 39406
Test: Total time: 0:00:03 (1.7489 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6744 Acc: 0.7238 Recall_macro: 0.6744 Recall_weighted: 0.7238 AUC-ROC: 0.9386 Weighted F1-score: 0.7358
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 17, 'Val Loss': 0.8087061643600464, 'Val BAcc': np.float64(0.6743970631712567), 'Val Acc': 0.7238372093023255, 'Val ROC': np.float64(0.9385927645832041), 'Val W_F1': 0.7358093357763097, 'Val Recall_macro': 0.6743970631712567, 'Val Recall_weighted': 0.7238372093023255}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [18]  [ 0/11]  eta: 0:00:41  lr: 0.000452  min_lr: 0.000000  loss: 1.4416 (1.4416)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3193 (2.3193)  time: 3.7902  data: 2.9868  max mem: 39406
Epoch: [18]  [10/11]  eta: 0:00:01  lr: 0.000441  min_lr: 0.000000  loss: 1.4136 (1.3456)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9698 (2.1563)  time: 1.0852  data: 0.2716  max mem: 39406
Epoch: [18] Total time: 0:00:12 (1.0989 s / it)
2025-04-29 11:35:18 Averaged stats: lr: 0.000441  min_lr: 0.000000  loss: 1.4136 (1.3456)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9698 (2.1563)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0583  data: 2.6718  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6837  data: 1.3360  max mem: 39406
Test: Total time: 0:00:03 (1.7524 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6275 Acc: 0.7442 Recall_macro: 0.6275 Recall_weighted: 0.7442 AUC-ROC: 0.9376 Weighted F1-score: 0.7352
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 18, 'Val Loss': 0.8054231405258179, 'Val BAcc': np.float64(0.6275151599022567), 'Val Acc': 0.7441860465116279, 'Val ROC': np.float64(0.93763740543713), 'Val W_F1': 0.7351785275774051, 'Val Recall_macro': 0.6275151599022567, 'Val Recall_weighted': 0.7441860465116279}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [19]  [ 0/11]  eta: 0:00:42  lr: 0.000440  min_lr: 0.000000  loss: 1.3818 (1.3818)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5830 (1.5830)  time: 3.8458  data: 3.0508  max mem: 39406
Epoch: [19]  [10/11]  eta: 0:00:01  lr: 0.000428  min_lr: 0.000000  loss: 1.3818 (1.3332)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8478 (1.8297)  time: 1.0846  data: 0.2774  max mem: 39406
Epoch: [19] Total time: 0:00:12 (1.0972 s / it)
2025-04-29 11:35:40 Averaged stats: lr: 0.000428  min_lr: 0.000000  loss: 1.3818 (1.3332)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8478 (1.8297)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0350  data: 2.6490  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6721  data: 1.3246  max mem: 39406
Test: Total time: 0:00:03 (1.7322 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6996 Acc: 0.7703 Recall_macro: 0.6996 Recall_weighted: 0.7703 AUC-ROC: 0.9374 Weighted F1-score: 0.7738
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 19, 'Val Loss': 0.7549920082092285, 'Val BAcc': np.float64(0.6995616362067976), 'Val Acc': 0.7703488372093024, 'Val ROC': np.float64(0.9373889869276826), 'Val W_F1': 0.7737630446913282, 'Val Recall_macro': 0.6995616362067976, 'Val Recall_weighted': 0.7703488372093024}
Max val mean accuracy: 0.77%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [20]  [ 0/11]  eta: 0:00:41  lr: 0.000427  min_lr: 0.000000  loss: 1.4756 (1.4756)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8850 (1.8850)  time: 3.7828  data: 2.9891  max mem: 39406
Epoch: [20]  [10/11]  eta: 0:00:01  lr: 0.000414  min_lr: 0.000000  loss: 1.2796 (1.2638)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8387 (1.9164)  time: 1.0758  data: 0.2718  max mem: 39406
Epoch: [20] Total time: 0:00:11 (1.0895 s / it)
2025-04-29 11:36:01 Averaged stats: lr: 0.000414  min_lr: 0.000000  loss: 1.2796 (1.2638)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8387 (1.9164)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0197  data: 2.6343  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6635  data: 1.3172  max mem: 39406
Test: Total time: 0:00:03 (1.7271 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7326 Acc: 0.7326 Recall_macro: 0.7326 Recall_weighted: 0.7326 AUC-ROC: 0.9338 Weighted F1-score: 0.7449
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 20, 'Val Loss': 0.7992039918899536, 'Val BAcc': np.float64(0.7325680468906276), 'Val Acc': 0.7325581395348837, 'Val ROC': np.float64(0.9337525065310094), 'Val W_F1': 0.74493990037605, 'Val Recall_macro': 0.7325680468906276, 'Val Recall_weighted': 0.7325581395348837}
Max val mean accuracy: 0.77%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [21]  [ 0/11]  eta: 0:00:39  lr: 0.000413  min_lr: 0.000000  loss: 1.3107 (1.3107)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5047 (1.5047)  time: 3.5478  data: 2.7492  max mem: 39406
Epoch: [21]  [10/11]  eta: 0:00:01  lr: 0.000399  min_lr: 0.000000  loss: 1.0313 (1.1168)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7237 (1.8153)  time: 1.0587  data: 0.2500  max mem: 39406
Epoch: [21] Total time: 0:00:11 (1.0725 s / it)
2025-04-29 11:36:17 Averaged stats: lr: 0.000399  min_lr: 0.000000  loss: 1.0313 (1.1168)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7237 (1.8153)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0077  data: 2.6229  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6582  data: 1.3115  max mem: 39406
Test: Total time: 0:00:03 (1.7152 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7153 Acc: 0.7587 Recall_macro: 0.7153 Recall_weighted: 0.7587 AUC-ROC: 0.9363 Weighted F1-score: 0.7656
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 21, 'Val Loss': 0.7212295532226562, 'Val BAcc': np.float64(0.7152841974777459), 'Val Acc': 0.7587209302325582, 'Val ROC': np.float64(0.9363279848658926), 'Val W_F1': 0.7656172933859684, 'Val Recall_macro': 0.7152841974777459, 'Val Recall_weighted': 0.7587209302325582}
Max val mean accuracy: 0.77%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [22]  [ 0/11]  eta: 0:00:39  lr: 0.000397  min_lr: 0.000000  loss: 1.1988 (1.1988)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0695 (2.0695)  time: 3.5981  data: 2.7972  max mem: 39406
Epoch: [22]  [10/11]  eta: 0:00:01  lr: 0.000382  min_lr: 0.000000  loss: 1.1983 (1.1596)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0695 (1.9769)  time: 1.0654  data: 0.2544  max mem: 39406
Epoch: [22] Total time: 0:00:11 (1.0801 s / it)
2025-04-29 11:36:32 Averaged stats: lr: 0.000382  min_lr: 0.000000  loss: 1.1983 (1.1596)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0695 (1.9769)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:05    time: 2.9311  data: 2.5417  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6207  data: 1.2709  max mem: 39406
Test: Total time: 0:00:03 (1.6884 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6863 Acc: 0.7849 Recall_macro: 0.6863 Recall_weighted: 0.7849 AUC-ROC: 0.9485 Weighted F1-score: 0.7774
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 22, 'Val Loss': 0.6851091980934143, 'Val BAcc': np.float64(0.6862663920728438), 'Val Acc': 0.7848837209302325, 'Val ROC': np.float64(0.9484976378320252), 'Val W_F1': 0.7774468717107201, 'Val Recall_macro': 0.6862663920728438, 'Val Recall_weighted': 0.7848837209302325}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [23]  [ 0/11]  eta: 0:00:43  lr: 0.000381  min_lr: 0.000000  loss: 1.4971 (1.4971)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3544 (2.3544)  time: 3.9323  data: 3.1394  max mem: 39406
Epoch: [23]  [10/11]  eta: 0:00:01  lr: 0.000365  min_lr: 0.000000  loss: 1.3837 (1.2844)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8268 (1.8887)  time: 1.0922  data: 0.2855  max mem: 39406
Epoch: [23] Total time: 0:00:12 (1.1061 s / it)
2025-04-29 11:36:54 Averaged stats: lr: 0.000365  min_lr: 0.000000  loss: 1.3837 (1.2844)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8268 (1.8887)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0270  data: 2.6385  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6675  data: 1.3193  max mem: 39406
Test: Total time: 0:00:03 (1.7216 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6835 Acc: 0.7762 Recall_macro: 0.6835 Recall_weighted: 0.7762 AUC-ROC: 0.9399 Weighted F1-score: 0.7711
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 23, 'Val Loss': 0.6952558755874634, 'Val BAcc': np.float64(0.683516265322717), 'Val Acc': 0.7761627906976745, 'Val ROC': np.float64(0.9398559944140779), 'Val W_F1': 0.7710503939542627, 'Val Recall_macro': 0.683516265322717, 'Val Recall_weighted': 0.7761627906976745}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [24]  [ 0/11]  eta: 0:00:39  lr: 0.000364  min_lr: 0.000000  loss: 0.9386 (0.9386)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7279 (1.7279)  time: 3.6190  data: 2.8176  max mem: 39406
Epoch: [24]  [10/11]  eta: 0:00:01  lr: 0.000348  min_lr: 0.000000  loss: 1.0771 (1.0887)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7160 (1.7608)  time: 1.0660  data: 0.2562  max mem: 39406
Epoch: [24] Total time: 0:00:11 (1.0801 s / it)
2025-04-29 11:37:09 Averaged stats: lr: 0.000348  min_lr: 0.000000  loss: 1.0771 (1.0887)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7160 (1.7608)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0623  data: 2.6698  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6858  data: 1.3350  max mem: 39406
Test: Total time: 0:00:03 (1.7542 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7140 Acc: 0.7762 Recall_macro: 0.7140 Recall_weighted: 0.7762 AUC-ROC: 0.9397 Weighted F1-score: 0.7772
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 24, 'Val Loss': 0.7086189389228821, 'Val BAcc': np.float64(0.713969297324136), 'Val Acc': 0.7761627906976745, 'Val ROC': np.float64(0.9397282866444181), 'Val W_F1': 0.7771665714502558, 'Val Recall_macro': 0.713969297324136, 'Val Recall_weighted': 0.7761627906976745}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [25]  [ 0/11]  eta: 0:00:39  lr: 0.000346  min_lr: 0.000000  loss: 0.8182 (0.8182)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8972 (1.8972)  time: 3.6045  data: 2.8040  max mem: 39406
Epoch: [25]  [10/11]  eta: 0:00:01  lr: 0.000329  min_lr: 0.000000  loss: 1.0938 (1.0399)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6716 (1.7994)  time: 1.0670  data: 0.2550  max mem: 39406
Epoch: [25] Total time: 0:00:11 (1.0811 s / it)
2025-04-29 11:37:24 Averaged stats: lr: 0.000329  min_lr: 0.000000  loss: 1.0938 (1.0399)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6716 (1.7994)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0079  data: 2.6175  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6592  data: 1.3088  max mem: 39406
Test: Total time: 0:00:03 (1.7228 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6739 Acc: 0.7558 Recall_macro: 0.6739 Recall_weighted: 0.7558 AUC-ROC: 0.9367 Weighted F1-score: 0.7572
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 25, 'Val Loss': 0.6925791501998901, 'Val BAcc': np.float64(0.6739151585603199), 'Val Acc': 0.7558139534883721, 'Val ROC': np.float64(0.9367436564245807), 'Val W_F1': 0.757245731468876, 'Val Recall_macro': 0.6739151585603199, 'Val Recall_weighted': 0.7558139534883721}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [26]  [ 0/11]  eta: 0:00:41  lr: 0.000328  min_lr: 0.000000  loss: 1.3804 (1.3804)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9095 (2.9095)  time: 3.7971  data: 2.9929  max mem: 39406
Epoch: [26]  [10/11]  eta: 0:00:01  lr: 0.000310  min_lr: 0.000000  loss: 1.2130 (1.1595)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0015 (2.0258)  time: 1.0868  data: 0.2721  max mem: 39406
Epoch: [26] Total time: 0:00:12 (1.0998 s / it)
2025-04-29 11:37:40 Averaged stats: lr: 0.000310  min_lr: 0.000000  loss: 1.2130 (1.1595)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0015 (2.0258)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:05    time: 2.9893  data: 2.5997  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6501  data: 1.2999  max mem: 39406
Test: Total time: 0:00:03 (1.7167 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7383 Acc: 0.7645 Recall_macro: 0.7383 Recall_weighted: 0.7645 AUC-ROC: 0.9365 Weighted F1-score: 0.7791
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 26, 'Val Loss': 0.6904481649398804, 'Val BAcc': np.float64(0.7383114777953487), 'Val Acc': 0.7645348837209303, 'Val ROC': np.float64(0.9364933558189342), 'Val W_F1': 0.7790835898428301, 'Val Recall_macro': 0.7383114777953487, 'Val Recall_weighted': 0.7645348837209303}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [27]  [ 0/11]  eta: 0:00:41  lr: 0.000309  min_lr: 0.000000  loss: 1.1197 (1.1197)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6262 (1.6262)  time: 3.7343  data: 2.9275  max mem: 39406
Epoch: [27]  [10/11]  eta: 0:00:01  lr: 0.000291  min_lr: 0.000000  loss: 1.0335 (1.0943)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7086 (1.7738)  time: 1.0820  data: 0.2662  max mem: 39406
Epoch: [27] Total time: 0:00:12 (1.0958 s / it)
2025-04-29 11:37:56 Averaged stats: lr: 0.000291  min_lr: 0.000000  loss: 1.0335 (1.0943)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7086 (1.7738)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:05    time: 2.9748  data: 2.5825  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6433  data: 1.2913  max mem: 39406
Test: Total time: 0:00:03 (1.7003 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6838 Acc: 0.7907 Recall_macro: 0.6838 Recall_weighted: 0.7907 AUC-ROC: 0.9426 Weighted F1-score: 0.7863
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 27, 'Val Loss': 0.6395235657691956, 'Val BAcc': np.float64(0.6838129256193772), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.942559748862303), 'Val W_F1': 0.7862672526406536, 'Val Recall_macro': 0.6838129256193772, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.79%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [28]  [ 0/11]  eta: 0:00:39  lr: 0.000290  min_lr: 0.000000  loss: 1.2026 (1.2026)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5280 (1.5280)  time: 3.5764  data: 2.7768  max mem: 39406
Epoch: [28]  [10/11]  eta: 0:00:01  lr: 0.000272  min_lr: 0.000000  loss: 1.1233 (1.1011)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8072 (1.9222)  time: 1.0600  data: 0.2525  max mem: 39406
Epoch: [28] Total time: 0:00:11 (1.0740 s / it)
2025-04-29 11:38:17 Averaged stats: lr: 0.000272  min_lr: 0.000000  loss: 1.1233 (1.1011)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8072 (1.9222)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0188  data: 2.6279  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6649  data: 1.3140  max mem: 39406
Test: Total time: 0:00:03 (1.7299 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6890 Acc: 0.7849 Recall_macro: 0.6890 Recall_weighted: 0.7849 AUC-ROC: 0.9420 Weighted F1-score: 0.7732
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 28, 'Val Loss': 0.6554075479507446, 'Val BAcc': np.float64(0.6890234147008342), 'Val Acc': 0.7848837209302325, 'Val ROC': np.float64(0.9419585557987924), 'Val W_F1': 0.7731530243492658, 'Val Recall_macro': 0.6890234147008342, 'Val Recall_weighted': 0.7848837209302325}
Max val mean accuracy: 0.79%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [29]  [ 0/11]  eta: 0:00:42  lr: 0.000270  min_lr: 0.000000  loss: 0.6663 (0.6663)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9647 (1.9647)  time: 3.8825  data: 3.0854  max mem: 39406
Epoch: [29]  [10/11]  eta: 0:00:01  lr: 0.000252  min_lr: 0.000000  loss: 1.1741 (1.0846)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9989 (2.0350)  time: 1.0917  data: 0.2805  max mem: 39406
Epoch: [29] Total time: 0:00:12 (1.1047 s / it)
2025-04-29 11:38:33 Averaged stats: lr: 0.000252  min_lr: 0.000000  loss: 1.1741 (1.0846)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9989 (2.0350)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1684  data: 2.7770  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7387  data: 1.3886  max mem: 39406
Test: Total time: 0:00:03 (1.8151 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6795 Acc: 0.7907 Recall_macro: 0.6795 Recall_weighted: 0.7907 AUC-ROC: 0.9413 Weighted F1-score: 0.7809
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 29, 'Val Loss': 0.6254999041557312, 'Val BAcc': np.float64(0.6794614438485406), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9413280332114375), 'Val W_F1': 0.7808573073623966, 'Val Recall_macro': 0.6794614438485406, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.79%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [30]  [ 0/11]  eta: 0:00:39  lr: 0.000251  min_lr: 0.000000  loss: 1.1917 (1.1917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7446 (1.7446)  time: 3.5805  data: 2.7767  max mem: 39406
Epoch: [30]  [10/11]  eta: 0:00:01  lr: 0.000233  min_lr: 0.000000  loss: 1.3071 (1.1499)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0973 (2.0359)  time: 1.0664  data: 0.2525  max mem: 39406
Epoch: [30] Total time: 0:00:11 (1.0802 s / it)
2025-04-29 11:38:48 Averaged stats: lr: 0.000233  min_lr: 0.000000  loss: 1.3071 (1.1499)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0973 (2.0359)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0102  data: 2.6198  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6603  data: 1.3100  max mem: 39406
Test: Total time: 0:00:03 (1.7174 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7019 Acc: 0.7965 Recall_macro: 0.7019 Recall_weighted: 0.7965 AUC-ROC: 0.9372 Weighted F1-score: 0.7926
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 30, 'Val Loss': 0.6306794881820679, 'Val BAcc': np.float64(0.7018968931226995), 'Val Acc': 0.7965116279069767, 'Val ROC': np.float64(0.9371971804947762), 'Val W_F1': 0.7926429658097575, 'Val Recall_macro': 0.7018968931226995, 'Val Recall_weighted': 0.7965116279069767}
Max val mean accuracy: 0.80%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [31]  [ 0/11]  eta: 0:00:40  lr: 0.000231  min_lr: 0.000000  loss: 1.5113 (1.5113)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3730 (2.3730)  time: 3.6591  data: 2.8635  max mem: 39406
Epoch: [31]  [10/11]  eta: 0:00:01  lr: 0.000213  min_lr: 0.000000  loss: 1.3063 (1.2372)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7165 (1.7257)  time: 1.0692  data: 0.2604  max mem: 39406
Epoch: [31] Total time: 0:00:11 (1.0832 s / it)
2025-04-29 11:39:10 Averaged stats: lr: 0.000213  min_lr: 0.000000  loss: 1.3063 (1.2372)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7165 (1.7257)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3527  data: 2.9677  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8304  data: 1.4839  max mem: 39406
Test: Total time: 0:00:03 (1.8848 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7110 Acc: 0.8081 Recall_macro: 0.7110 Recall_weighted: 0.8081 AUC-ROC: 0.9337 Weighted F1-score: 0.7957
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 31, 'Val Loss': 0.6454062461853027, 'Val BAcc': np.float64(0.711009764687184), 'Val Acc': 0.8081395348837209, 'Val ROC': np.float64(0.9336513520586244), 'Val W_F1': 0.7957070953709217, 'Val Recall_macro': 0.711009764687184, 'Val Recall_weighted': 0.8081395348837209}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [32]  [ 0/11]  eta: 0:00:40  lr: 0.000211  min_lr: 0.000000  loss: 1.2541 (1.2541)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8465 (1.8465)  time: 3.7108  data: 2.9139  max mem: 39406
Epoch: [32]  [10/11]  eta: 0:00:01  lr: 0.000194  min_lr: 0.000000  loss: 1.2426 (1.2282)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7692 (1.8360)  time: 1.0686  data: 0.2650  max mem: 39406
Epoch: [32] Total time: 0:00:11 (1.0818 s / it)
2025-04-29 11:39:31 Averaged stats: lr: 0.000194  min_lr: 0.000000  loss: 1.2426 (1.2282)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7692 (1.8360)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:05    time: 2.9940  data: 2.6138  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6513  data: 1.3070  max mem: 39406
Test: Total time: 0:00:03 (1.7143 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6920 Acc: 0.7849 Recall_macro: 0.6920 Recall_weighted: 0.7849 AUC-ROC: 0.9304 Weighted F1-score: 0.7784
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 32, 'Val Loss': 0.6590784788131714, 'Val BAcc': np.float64(0.6920370391338134), 'Val Acc': 0.7848837209302325, 'Val ROC': np.float64(0.9304332587199081), 'Val W_F1': 0.7783552792816296, 'Val Recall_macro': 0.6920370391338134, 'Val Recall_weighted': 0.7848837209302325}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [33]  [ 0/11]  eta: 0:00:39  lr: 0.000192  min_lr: 0.000000  loss: 0.7418 (0.7418)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9422 (1.9422)  time: 3.5962  data: 2.7995  max mem: 39406
Epoch: [33]  [10/11]  eta: 0:00:01  lr: 0.000175  min_lr: 0.000000  loss: 1.0138 (1.0363)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8277 (1.8309)  time: 1.0644  data: 0.2546  max mem: 39406
Epoch: [33] Total time: 0:00:11 (1.0764 s / it)
2025-04-29 11:39:47 Averaged stats: lr: 0.000175  min_lr: 0.000000  loss: 1.0138 (1.0363)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8277 (1.8309)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0497  data: 2.6626  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6805  data: 1.3314  max mem: 39406
Test: Total time: 0:00:03 (1.7413 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7025 Acc: 0.7878 Recall_macro: 0.7025 Recall_weighted: 0.7878 AUC-ROC: 0.9332 Weighted F1-score: 0.7782
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 33, 'Val Loss': 0.665116548538208, 'Val BAcc': np.float64(0.7024590388461357), 'Val Acc': 0.7877906976744186, 'Val ROC': np.float64(0.9331863204438404), 'Val W_F1': 0.7782051806129772, 'Val Recall_macro': 0.7024590388461357, 'Val Recall_weighted': 0.7877906976744186}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [34]  [ 0/11]  eta: 0:00:41  lr: 0.000173  min_lr: 0.000000  loss: 1.0709 (1.0709)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9357 (1.9357)  time: 3.7510  data: 2.9484  max mem: 39406
Epoch: [34]  [10/11]  eta: 0:00:01  lr: 0.000157  min_lr: 0.000000  loss: 1.1707 (1.0967)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7858 (1.7715)  time: 1.0802  data: 0.2681  max mem: 39406
Epoch: [34] Total time: 0:00:12 (1.0950 s / it)
2025-04-29 11:40:02 Averaged stats: lr: 0.000157  min_lr: 0.000000  loss: 1.1707 (1.0967)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7858 (1.7715)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:05    time: 2.9768  data: 2.5860  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6435  data: 1.2931  max mem: 39406
Test: Total time: 0:00:03 (1.7040 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7270 Acc: 0.7878 Recall_macro: 0.7270 Recall_weighted: 0.7878 AUC-ROC: 0.9349 Weighted F1-score: 0.7889
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 34, 'Val Loss': 0.6804496645927429, 'Val BAcc': np.float64(0.7270057036508649), 'Val Acc': 0.7877906976744186, 'Val ROC': np.float64(0.9349060569670399), 'Val W_F1': 0.7888657095351266, 'Val Recall_macro': 0.7270057036508649, 'Val Recall_weighted': 0.7877906976744186}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [35]  [ 0/11]  eta: 0:00:42  lr: 0.000155  min_lr: 0.000000  loss: 1.4333 (1.4333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5871 (2.5871)  time: 3.8321  data: 3.0252  max mem: 39406
Epoch: [35]  [10/11]  eta: 0:00:01  lr: 0.000139  min_lr: 0.000000  loss: 1.2324 (1.1625)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9368 (1.9269)  time: 1.0919  data: 0.2751  max mem: 39406
Epoch: [35] Total time: 0:00:12 (1.1042 s / it)
2025-04-29 11:40:18 Averaged stats: lr: 0.000139  min_lr: 0.000000  loss: 1.2324 (1.1625)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9368 (1.9269)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1851  data: 2.7951  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7490  data: 1.3976  max mem: 39406
Test: Total time: 0:00:03 (1.8098 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7102 Acc: 0.7965 Recall_macro: 0.7102 Recall_weighted: 0.7965 AUC-ROC: 0.9407 Weighted F1-score: 0.7863
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 35, 'Val Loss': 0.6628646850585938, 'Val BAcc': np.float64(0.7101762105633074), 'Val Acc': 0.7965116279069767, 'Val ROC': np.float64(0.9407224253116068), 'Val W_F1': 0.786259183787854, 'Val Recall_macro': 0.7101762105633074, 'Val Recall_weighted': 0.7965116279069767}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [36]  [ 0/11]  eta: 0:00:43  lr: 0.000137  min_lr: 0.000000  loss: 0.9998 (0.9998)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6820 (1.6820)  time: 3.9468  data: 3.1438  max mem: 39406
Epoch: [36]  [10/11]  eta: 0:00:01  lr: 0.000122  min_lr: 0.000000  loss: 1.1427 (1.1174)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7449 (1.7173)  time: 1.1025  data: 0.2859  max mem: 39406
Epoch: [36] Total time: 0:00:12 (1.1165 s / it)
2025-04-29 11:40:34 Averaged stats: lr: 0.000122  min_lr: 0.000000  loss: 1.1427 (1.1174)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7449 (1.7173)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0634  data: 2.6734  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6882  data: 1.3367  max mem: 39406
Test: Total time: 0:00:03 (1.7514 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6924 Acc: 0.7791 Recall_macro: 0.6924 Recall_weighted: 0.7791 AUC-ROC: 0.9424 Weighted F1-score: 0.7717
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 36, 'Val Loss': 0.6589644551277161, 'Val BAcc': np.float64(0.6923845034812777), 'Val Acc': 0.7790697674418605, 'Val ROC': np.float64(0.9424005940861364), 'Val W_F1': 0.7716515500775206, 'Val Recall_macro': 0.6923845034812777, 'Val Recall_weighted': 0.7790697674418605}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [37]  [ 0/11]  eta: 0:00:42  lr: 0.000120  min_lr: 0.000000  loss: 1.1688 (1.1688)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4788 (1.4788)  time: 3.8676  data: 3.0609  max mem: 39406
Epoch: [37]  [10/11]  eta: 0:00:01  lr: 0.000105  min_lr: 0.000000  loss: 0.9836 (1.0113)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6483 (1.7023)  time: 1.0962  data: 0.2784  max mem: 39406
Epoch: [37] Total time: 0:00:12 (1.1094 s / it)
2025-04-29 11:40:50 Averaged stats: lr: 0.000105  min_lr: 0.000000  loss: 0.9836 (1.0113)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6483 (1.7023)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1318  data: 2.7354  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7221  data: 1.3678  max mem: 39406
Test: Total time: 0:00:03 (1.7839 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6931 Acc: 0.7907 Recall_macro: 0.6931 Recall_weighted: 0.7907 AUC-ROC: 0.9426 Weighted F1-score: 0.7774
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 37, 'Val Loss': 0.6582894325256348, 'Val BAcc': np.float64(0.6930603867378061), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9425737196059844), 'Val W_F1': 0.7773722661309649, 'Val Recall_macro': 0.6930603867378061, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [38]  [ 0/11]  eta: 0:00:40  lr: 0.000104  min_lr: 0.000000  loss: 0.8574 (0.8574)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4298 (1.4298)  time: 3.6457  data: 2.8375  max mem: 39406
Epoch: [38]  [10/11]  eta: 0:00:01  lr: 0.000090  min_lr: 0.000000  loss: 0.9924 (1.0155)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0342 (1.9061)  time: 1.0743  data: 0.2581  max mem: 39406
Epoch: [38] Total time: 0:00:11 (1.0878 s / it)
2025-04-29 11:41:05 Averaged stats: lr: 0.000090  min_lr: 0.000000  loss: 0.9924 (1.0155)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0342 (1.9061)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0754  data: 2.6838  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6960  data: 1.3420  max mem: 39406
Test: Total time: 0:00:03 (1.7657 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7255 Acc: 0.8023 Recall_macro: 0.7255 Recall_weighted: 0.8023 AUC-ROC: 0.9417 Weighted F1-score: 0.7973
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 38, 'Val Loss': 0.64202481508255, 'Val BAcc': np.float64(0.7254659166917232), 'Val Acc': 0.8023255813953488, 'Val ROC': np.float64(0.9416638898987312), 'Val W_F1': 0.7973015227855039, 'Val Recall_macro': 0.7254659166917232, 'Val Recall_weighted': 0.8023255813953488}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [39]  [ 0/11]  eta: 0:00:41  lr: 0.000088  min_lr: 0.000000  loss: 1.4084 (1.4084)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9562 (1.9562)  time: 3.7373  data: 2.9304  max mem: 39406
Epoch: [39]  [10/11]  eta: 0:00:01  lr: 0.000075  min_lr: 0.000000  loss: 1.1395 (1.0914)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9562 (1.8503)  time: 1.0848  data: 0.2665  max mem: 39406
Epoch: [39] Total time: 0:00:12 (1.0990 s / it)
2025-04-29 11:41:21 Averaged stats: lr: 0.000075  min_lr: 0.000000  loss: 1.1395 (1.0914)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9562 (1.8503)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1434  data: 2.7549  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7293  data: 1.3775  max mem: 39406
Test: Total time: 0:00:03 (1.8039 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7042 Acc: 0.7994 Recall_macro: 0.7042 Recall_weighted: 0.7994 AUC-ROC: 0.9392 Weighted F1-score: 0.7856
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 39, 'Val Loss': 0.6393416523933411, 'Val BAcc': np.float64(0.7042061298835492), 'Val Acc': 0.7994186046511628, 'Val ROC': np.float64(0.9392110295578183), 'Val W_F1': 0.7855531741797268, 'Val Recall_macro': 0.7042061298835492, 'Val Recall_weighted': 0.7994186046511628}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [40]  [ 0/11]  eta: 0:00:43  lr: 0.000074  min_lr: 0.000000  loss: 0.8726 (0.8726)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5767 (1.5767)  time: 3.9771  data: 3.1695  max mem: 39406
Epoch: [40]  [10/11]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000000  loss: 1.0969 (1.0304)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7249 (1.7090)  time: 1.1071  data: 0.2882  max mem: 39406
Epoch: [40] Total time: 0:00:12 (1.1188 s / it)
2025-04-29 11:41:37 Averaged stats: lr: 0.000062  min_lr: 0.000000  loss: 1.0969 (1.0304)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7249 (1.7090)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0764  data: 2.6853  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6945  data: 1.3427  max mem: 39406
Test: Total time: 0:00:03 (1.7633 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6996 Acc: 0.7965 Recall_macro: 0.6996 Recall_weighted: 0.7965 AUC-ROC: 0.9406 Weighted F1-score: 0.7847
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 40, 'Val Loss': 0.6319226622581482, 'Val BAcc': np.float64(0.6996496320367288), 'Val Acc': 0.7965116279069767, 'Val ROC': np.float64(0.9405711290741143), 'Val W_F1': 0.7847114896327035, 'Val Recall_macro': 0.6996496320367288, 'Val Recall_weighted': 0.7965116279069767}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [41]  [ 0/11]  eta: 0:00:40  lr: 0.000061  min_lr: 0.000000  loss: 0.6079 (0.6079)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5968 (1.5968)  time: 3.7053  data: 2.9019  max mem: 39406
Epoch: [41]  [10/11]  eta: 0:00:01  lr: 0.000050  min_lr: 0.000000  loss: 1.1605 (1.0702)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8855 (1.8216)  time: 1.0827  data: 0.2639  max mem: 39406
Epoch: [41] Total time: 0:00:12 (1.0962 s / it)
2025-04-29 11:41:53 Averaged stats: lr: 0.000050  min_lr: 0.000000  loss: 1.1605 (1.0702)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8855 (1.8216)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0574  data: 2.6658  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6850  data: 1.3330  max mem: 39406
Test: Total time: 0:00:03 (1.7440 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7081 Acc: 0.7907 Recall_macro: 0.7081 Recall_weighted: 0.7907 AUC-ROC: 0.9392 Weighted F1-score: 0.7817
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 41, 'Val Loss': 0.6290315389633179, 'Val BAcc': np.float64(0.7081239832207574), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9391709701816882), 'Val W_F1': 0.7817234803112265, 'Val Recall_macro': 0.7081239832207574, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [42]  [ 0/11]  eta: 0:00:42  lr: 0.000049  min_lr: 0.000000  loss: 0.9199 (0.9199)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7097 (1.7097)  time: 3.9032  data: 3.0958  max mem: 39406
Epoch: [42]  [10/11]  eta: 0:00:01  lr: 0.000039  min_lr: 0.000000  loss: 0.9759 (1.0206)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7270 (1.7942)  time: 1.1009  data: 0.2815  max mem: 39406
Epoch: [42] Total time: 0:00:12 (1.1130 s / it)
2025-04-29 11:42:08 Averaged stats: lr: 0.000039  min_lr: 0.000000  loss: 0.9759 (1.0206)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7270 (1.7942)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1225  data: 2.7260  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7180  data: 1.3631  max mem: 39406
Test: Total time: 0:00:03 (1.7789 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7193 Acc: 0.7994 Recall_macro: 0.7193 Recall_weighted: 0.7994 AUC-ROC: 0.9394 Weighted F1-score: 0.7917
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 42, 'Val Loss': 0.6275032162666321, 'Val BAcc': np.float64(0.7192592290656807), 'Val Acc': 0.7994186046511628, 'Val ROC': np.float64(0.9393533329943528), 'Val W_F1': 0.7917245561633094, 'Val Recall_macro': 0.7192592290656807, 'Val Recall_weighted': 0.7994186046511628}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [43]  [ 0/11]  eta: 0:00:41  lr: 0.000038  min_lr: 0.000000  loss: 1.1768 (1.1768)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6457 (1.6457)  time: 3.7568  data: 2.9469  max mem: 39406
Epoch: [43]  [10/11]  eta: 0:00:01  lr: 0.000029  min_lr: 0.000000  loss: 1.0993 (1.0680)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7685 (1.7277)  time: 1.0891  data: 0.2680  max mem: 39406
Epoch: [43] Total time: 0:00:12 (1.1000 s / it)
2025-04-29 11:42:24 Averaged stats: lr: 0.000029  min_lr: 0.000000  loss: 1.0993 (1.0680)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7685 (1.7277)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.2045  data: 2.8112  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7578  data: 1.4057  max mem: 39406
Test: Total time: 0:00:03 (1.8198 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7086 Acc: 0.8023 Recall_macro: 0.7086 Recall_weighted: 0.8023 AUC-ROC: 0.9401 Weighted F1-score: 0.7896
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 43, 'Val Loss': 0.6342954039573669, 'Val BAcc': np.float64(0.7086316983091177), 'Val Acc': 0.8023255813953488, 'Val ROC': np.float64(0.9401405337830591), 'Val W_F1': 0.7896187540562378, 'Val Recall_macro': 0.7086316983091177, 'Val Recall_weighted': 0.8023255813953488}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [44]  [ 0/11]  eta: 0:00:40  lr: 0.000028  min_lr: 0.000000  loss: 1.3108 (1.3108)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9754 (1.9754)  time: 3.7080  data: 2.8991  max mem: 39406
Epoch: [44]  [10/11]  eta: 0:00:01  lr: 0.000021  min_lr: 0.000000  loss: 1.1867 (1.0934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9754 (1.8097)  time: 1.0811  data: 0.2636  max mem: 39406
Epoch: [44] Total time: 0:00:12 (1.0922 s / it)
2025-04-29 11:42:40 Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.1867 (1.0934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9754 (1.8097)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0778  data: 2.6893  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6955  data: 1.3447  max mem: 39406
Test: Total time: 0:00:03 (1.7538 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7087 Acc: 0.7936 Recall_macro: 0.7087 Recall_weighted: 0.7936 AUC-ROC: 0.9397 Weighted F1-score: 0.7837
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 44, 'Val Loss': 0.6327357888221741, 'Val BAcc': np.float64(0.708674709061806), 'Val Acc': 0.7936046511627907, 'Val ROC': np.float64(0.9396790402412711), 'Val W_F1': 0.7836556733159449, 'Val Recall_macro': 0.708674709061806, 'Val Recall_weighted': 0.7936046511627907}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [45]  [ 0/11]  eta: 0:00:43  lr: 0.000020  min_lr: 0.000000  loss: 1.1470 (1.1470)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0175 (2.0175)  time: 3.9189  data: 3.1104  max mem: 39406
Epoch: [45]  [10/11]  eta: 0:00:01  lr: 0.000014  min_lr: 0.000000  loss: 1.1470 (1.0964)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9914 (1.8428)  time: 1.1013  data: 0.2828  max mem: 39406
Epoch: [45] Total time: 0:00:12 (1.1139 s / it)
2025-04-29 11:42:56 Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.1470 (1.0964)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9914 (1.8428)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0511  data: 2.6570  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.6817  data: 1.3286  max mem: 39406
Test: Total time: 0:00:03 (1.7528 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7026 Acc: 0.7878 Recall_macro: 0.7026 Recall_weighted: 0.7878 AUC-ROC: 0.9398 Weighted F1-score: 0.7801
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 45, 'Val Loss': 0.6304678916931152, 'Val BAcc': np.float64(0.702616709713484), 'Val Acc': 0.7877906976744186, 'Val ROC': np.float64(0.9398346069918647), 'Val W_F1': 0.7800953601668545, 'Val Recall_macro': 0.702616709713484, 'Val Recall_weighted': 0.7877906976744186}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [46]  [ 0/11]  eta: 0:00:41  lr: 0.000013  min_lr: 0.000000  loss: 1.1724 (1.1724)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6740 (1.6740)  time: 3.7490  data: 2.9404  max mem: 39406
Epoch: [46]  [10/11]  eta: 0:00:01  lr: 0.000008  min_lr: 0.000000  loss: 0.9893 (1.0608)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4617 (1.6661)  time: 1.0867  data: 0.2674  max mem: 39406
Epoch: [46] Total time: 0:00:12 (1.1002 s / it)
2025-04-29 11:43:11 Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 0.9893 (1.0608)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4617 (1.6661)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3284  data: 2.9349  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8197  data: 1.4675  max mem: 39406
Test: Total time: 0:00:03 (1.8799 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7073 Acc: 0.7907 Recall_macro: 0.7073 Recall_weighted: 0.7907 AUC-ROC: 0.9389 Weighted F1-score: 0.7813
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 46, 'Val Loss': 0.6316371560096741, 'Val BAcc': np.float64(0.7073413757284727), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9388743385100375), 'Val W_F1': 0.7812940862914406, 'Val Recall_macro': 0.7073413757284727, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [47]  [ 0/11]  eta: 0:00:43  lr: 0.000008  min_lr: 0.000000  loss: 0.7636 (0.7636)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6650 (1.6650)  time: 3.9604  data: 3.1492  max mem: 39406
Epoch: [47]  [10/11]  eta: 0:00:01  lr: 0.000004  min_lr: 0.000000  loss: 0.8599 (0.9121)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6053 (1.6573)  time: 1.1051  data: 0.2864  max mem: 39406
Epoch: [47] Total time: 0:00:12 (1.1187 s / it)
2025-04-29 11:43:27 Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 0.8599 (0.9121)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6053 (1.6573)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1659  data: 2.7726  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7400  data: 1.3864  max mem: 39406
Test: Total time: 0:00:03 (1.8081 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7073 Acc: 0.7907 Recall_macro: 0.7073 Recall_weighted: 0.7907 AUC-ROC: 0.9386 Weighted F1-score: 0.7813
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 47, 'Val Loss': 0.6331915855407715, 'Val BAcc': np.float64(0.7073413757284727), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9385748915610238), 'Val W_F1': 0.7812940862914406, 'Val Recall_macro': 0.7073413757284727, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [48]  [ 0/11]  eta: 0:00:40  lr: 0.000004  min_lr: 0.000000  loss: 0.9848 (0.9848)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5119 (1.5119)  time: 3.6710  data: 2.8624  max mem: 39406
Epoch: [48]  [10/11]  eta: 0:00:01  lr: 0.000002  min_lr: 0.000000  loss: 1.1123 (1.0809)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5765 (1.6300)  time: 1.0801  data: 0.2603  max mem: 39406
Epoch: [48] Total time: 0:00:12 (1.0927 s / it)
2025-04-29 11:43:43 Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.1123 (1.0809)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5765 (1.6300)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.0872  data: 2.6909  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7014  data: 1.3455  max mem: 39406
Test: Total time: 0:00:03 (1.7646 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7073 Acc: 0.7907 Recall_macro: 0.7073 Recall_weighted: 0.7907 AUC-ROC: 0.9385 Weighted F1-score: 0.7813
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 48, 'Val Loss': 0.6338801980018616, 'Val BAcc': np.float64(0.7073413757284727), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9385153632338227), 'Val W_F1': 0.7812940862914406, 'Val Recall_macro': 0.7073413757284727, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [49]  [ 0/11]  eta: 0:00:40  lr: 0.000002  min_lr: 0.000000  loss: 1.1490 (1.1490)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7729 (1.7729)  time: 3.7041  data: 2.8928  max mem: 39406
Epoch: [49]  [10/11]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000000  loss: 1.2677 (1.2278)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9940 (1.9583)  time: 1.0839  data: 0.2631  max mem: 39406
Epoch: [49] Total time: 0:00:12 (1.0970 s / it)
2025-04-29 11:43:59 Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.2677 (1.2278)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9940 (1.9583)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.1584  data: 2.7637  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.7367  data: 1.3819  max mem: 39406
Test: Total time: 0:00:03 (1.7957 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7073 Acc: 0.7907 Recall_macro: 0.7073 Recall_weighted: 0.7907 AUC-ROC: 0.9385 Weighted F1-score: 0.7813
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 49, 'Val Loss': 0.633878231048584, 'Val BAcc': np.float64(0.7073413757284727), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9385225622576153), 'Val W_F1': 0.7812940862914406, 'Val Recall_macro': 0.7073413757284727, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:737: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_dict = torch.load(model_weight)
Starting test without tta
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/4]  eta: 0:00:10    time: 2.5140  data: 2.2617  max mem: 39406
Test:  [3/4]  eta: 0:00:00    time: 0.8640  data: 0.5655  max mem: 39406
Test: Total time: 0:00:03 (0.9067 s / it)
------------- test -------------
Sklearn Metrics - BAcc: 0.7397 Acc: 0.7874 Recall_macro: 0.7397 Recall_weighted: 0.7874 AUC-ROC: 0.9564 Weighted F1-score: 0.7816
{'balanced_accuracy': np.float64(0.7397185216183075), 'accuracy': 0.7874186550976139, 'top3 accuracy': np.float64(0.9783080260303688), 'top5 accuracy': np.float64(0.9978308026030369), 'sensitivity': np.float64(0.7397185216183075), 'specificity': np.float64(0.9488856365949797), 'auc_roc': np.float64(0.9563607993296396), 'weighted_f1': 0.7816121182937246, 'recall_macro': 0.7397185216183075, 'recall_weighted': 0.7874186550976139}
Predictions for test saved to /home/share/FM_Code/PanDerm/PAD_Res/test.csv
Training time 0:14:18
