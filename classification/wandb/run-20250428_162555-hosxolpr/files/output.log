Not using distributed mode
Namespace(mode='train', batch_size=128, epochs=50, update_freq=1, save_ckpt_freq=5, model='PanDerm_Base_FT', rel_pos_bias=True, sin_pos_emb=True, layer_scale_init_value=0.1, ood_eval=False, input_size=224, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, weights=True, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, percent_data=1.0, TTA=False, monitor='recall', opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.65, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=10, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', pretrained_checkpoint='/home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_bb_data6_checkpoint-499.pth', model_key='model|module|state_dict', model_prefix='', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, data_path='/datasets01/imagenet_full_size/061417/', eval_data_path=None, test_csv_path=None, image_key='image', nb_classes=7, imagenet_default_mean_and_std=True, data_set='IMNET', csv_path='/home/syyan/XJ/PanDerm-open_source/data/linear_probing/HAM_clean.csv', root_path='/home/share/Uni_Eval/ISIC2018_reader/images/', output_dir='/home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base', log_dir=None, device='cuda', seed=0, resume='', auto_resume=False, wandb_name='weight_sampler_max_recall_mask_B128_5e-4_0', save_ckpt=True, start_epoch=0, eval=False, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, enable_linear_eval=False, enable_multi_print=False, exp_name='ham finetune and eval', distributed=False)
Label distribution:
Label 0: 273
Label 1: 448
Label 2: 941
Label 3: 102
Label 4: 1021
Label 5: 5304
Label 6: 118
Using WeightedRandomSampler
train size: 8207 ,val size: 575 ,test size: 1232
Mixup is activated!
/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.05454545468091965)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.072727270424366)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090908616781235)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10909091681241989)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12727272510528564)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1454545557498932)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16363637149333954)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1818181872367859)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=7, bias=True)
)
Patch size = (16, 16)
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained_checkpoint, map_location='cpu')
Load ckpt from /home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_bb_data6_checkpoint-499.pth
all keys: []
##############new keys: 186 odict_keys(['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.gamma_1', 'blocks.0.gamma_2', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.q_bias', 'blocks.0.attn.v_bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.gamma_1', 'blocks.1.gamma_2', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.q_bias', 'blocks.1.attn.v_bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.gamma_1', 'blocks.2.gamma_2', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.q_bias', 'blocks.2.attn.v_bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.gamma_1', 'blocks.3.gamma_2', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.q_bias', 'blocks.3.attn.v_bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.gamma_1', 'blocks.4.gamma_2', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.q_bias', 'blocks.4.attn.v_bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.gamma_1', 'blocks.5.gamma_2', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.q_bias', 'blocks.5.attn.v_bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.gamma_1', 'blocks.6.gamma_2', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.q_bias', 'blocks.6.attn.v_bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.gamma_1', 'blocks.7.gamma_2', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.q_bias', 'blocks.7.attn.v_bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.gamma_1', 'blocks.8.gamma_2', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.q_bias', 'blocks.8.attn.v_bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.gamma_1', 'blocks.9.gamma_2', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.q_bias', 'blocks.9.attn.v_bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.gamma_1', 'blocks.10.gamma_2', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.q_bias', 'blocks.10.attn.v_bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'bl
Weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_bias_table', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.3.attn.relative_position_bias_table', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.6.attn.relative_position_bias_table', 'blocks.7.attn.relative_position_bias_table', 'blocks.8.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.10.attn.relative_position_bias_table', 'blocks.11.attn.relative_position_bias_table', 'head.weight', 'head.bias']
Ignored weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_index', 'blocks.1.attn.relative_position_index', 'blocks.2.attn.relative_position_index', 'blocks.3.attn.relative_position_index', 'blocks.4.attn.relative_position_index', 'blocks.5.attn.relative_position_index', 'blocks.6.attn.relative_position_index', 'blocks.7.attn.relative_position_index', 'blocks.8.attn.relative_position_index', 'blocks.9.attn.relative_position_index', 'blocks.10.attn.relative_position_index', 'blocks.11.attn.relative_position_index']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.05454545468091965)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.072727270424366)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090908616781235)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10909091681241989)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12727272510528564)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1454545557498932)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16363637149333954)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1818181872367859)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=7, bias=True)
)
number of params: 85767367
LR = 0.00050000
Batch size = 128
Update frequent = 1
Number of training examples = 8207
Number of training training per epoch = 64
Assigned values = [0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.gamma_1",
      "blocks.0.gamma_2",
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.relative_position_bias_table",
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.gamma_1",
      "blocks.1.gamma_2",
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.relative_position_bias_table",
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.gamma_1",
      "blocks.2.gamma_2",
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.relative_position_bias_table",
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.gamma_1",
      "blocks.3.gamma_2",
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.relative_position_bias_table",
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.gamma_1",
      "blocks.4.gamma_2",
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.relative_position_bias_table",
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.gamma_1",
      "blocks.5.gamma_2",
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.relative_position_bias_table",
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.gamma_1",
      "blocks.6.gamma_2",
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.relative_position_bias_table",
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.gamma_1",
      "blocks.7.gamma_2",
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.relative_position_bias_table",
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.gamma_1",
      "blocks.8.gamma_2",
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.relative_position_bias_table",
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.gamma_1",
      "blocks.9.gamma_2",
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.relative_position_bias_table",
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.gamma_1",
      "blocks.10.gamma_2",
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.relative_position_bias_table",
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.gamma_1",
      "blocks.11.gamma_2",
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.relative_position_bias_table",
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
/home/share/FM_Code/PanDerm/classification/furnace/utils.py:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
Use step level LR scheduler!
Set warmup steps = 640
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 50 epochs
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [0]  [ 0/64]  eta: 0:04:25  lr: 0.000000  min_lr: 0.000000  loss: 1.9459 (1.9459)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7213 (0.7213)  time: 4.1484  data: 2.5829  max mem: 14007
Epoch: [0]  [10/64]  eta: 0:00:34  lr: 0.000008  min_lr: 0.000000  loss: 1.9459 (1.9458)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8058 (0.8175)  time: 0.6320  data: 0.2351  max mem: 14662
Epoch: [0]  [20/64]  eta: 0:00:20  lr: 0.000016  min_lr: 0.000000  loss: 1.9456 (1.9456)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8538 (0.8649)  time: 0.2737  data: 0.0003  max mem: 14662
Epoch: [0]  [30/64]  eta: 0:00:13  lr: 0.000023  min_lr: 0.000000  loss: 1.9451 (1.9450)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.9692 (0.9108)  time: 0.2679  data: 0.0003  max mem: 14662
Epoch: [0]  [40/64]  eta: 0:00:08  lr: 0.000031  min_lr: 0.000000  loss: 1.9409 (1.9429)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1705 (1.0393)  time: 0.2693  data: 0.0003  max mem: 14662
Epoch: [0]  [50/64]  eta: 0:00:04  lr: 0.000039  min_lr: 0.000000  loss: 1.9296 (1.9389)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4931 (1.1430)  time: 0.2699  data: 0.0003  max mem: 14662
Epoch: [0]  [60/64]  eta: 0:00:01  lr: 0.000047  min_lr: 0.000000  loss: 1.9098 (1.9310)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6551 (1.2462)  time: 0.2702  data: 0.0002  max mem: 14662
Epoch: [0]  [63/64]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000000  loss: 1.9041 (1.9286)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6551 (1.2606)  time: 0.2703  data: 0.0001  max mem: 14662
Epoch: [0] Total time: 0:00:21 (0.3326 s / it)
2025-04-28 16:26:20 Averaged stats: lr: 0.000049  min_lr: 0.000000  loss: 1.9041 (1.9286)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6551 (1.2606)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:08    time: 2.7142  data: 2.3113  max mem: 14662
Test:  [2/3]  eta: 0:00:01    time: 1.0356  data: 0.7706  max mem: 14662
Test: Total time: 0:00:03 (1.0571 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.3736 Acc: 0.3496 Recall_macro: 0.3736 Recall_weighted: 0.3496 AUC-ROC: 0.8682 Weighted F1-score: 0.4488
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 0, 'Val Loss': 1.8409481048583984, 'Val BAcc': np.float64(0.3736162918117805), 'Val Acc': 0.34956521739130436, 'Val ROC': np.float64(0.8681767799944515), 'Val W_F1': 0.4488490754603912, 'Val Recall_macro': 0.3736162918117805, 'Val Recall_weighted': 0.34956521739130436}
Max val mean recall: 0.37%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [1]  [ 0/64]  eta: 0:03:26  lr: 0.000050  min_lr: 0.000000  loss: 1.8743 (1.8743)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.2529 (1.2529)  time: 3.2343  data: 2.9513  max mem: 14662
Epoch: [1]  [10/64]  eta: 0:00:29  lr: 0.000058  min_lr: 0.000000  loss: 1.8332 (1.8262)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9200 (1.8624)  time: 0.5439  data: 0.2687  max mem: 14662
Epoch: [1]  [20/64]  eta: 0:00:18  lr: 0.000066  min_lr: 0.000000  loss: 1.8177 (1.8106)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8676 (1.8397)  time: 0.2752  data: 0.0005  max mem: 14662
Epoch: [1]  [30/64]  eta: 0:00:12  lr: 0.000074  min_lr: 0.000000  loss: 1.7665 (1.7948)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8401 (1.8301)  time: 0.2750  data: 0.0005  max mem: 14662
Epoch: [1]  [40/64]  eta: 0:00:08  lr: 0.000081  min_lr: 0.000000  loss: 1.7567 (1.7853)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8011 (1.8577)  time: 0.2765  data: 0.0004  max mem: 14662
Epoch: [1]  [50/64]  eta: 0:00:04  lr: 0.000089  min_lr: 0.000000  loss: 1.7355 (1.7643)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9435 (1.9046)  time: 0.2779  data: 0.0004  max mem: 14662
Epoch: [1]  [60/64]  eta: 0:00:01  lr: 0.000097  min_lr: 0.000000  loss: 1.6737 (1.7478)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1391 (1.9709)  time: 0.2780  data: 0.0002  max mem: 14662
Epoch: [1]  [63/64]  eta: 0:00:00  lr: 0.000099  min_lr: 0.000000  loss: 1.6538 (1.7397)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1650 (1.9859)  time: 0.2773  data: 0.0002  max mem: 14662
Epoch: [1] Total time: 0:00:20 (0.3243 s / it)
2025-04-28 16:26:45 Averaged stats: lr: 0.000099  min_lr: 0.000000  loss: 1.6538 (1.7397)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1650 (1.9859)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.6326  data: 2.4943  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9717  data: 0.8316  max mem: 14662
Test: Total time: 0:00:02 (0.9965 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6274 Acc: 0.6922 Recall_macro: 0.6274 Recall_weighted: 0.6922 AUC-ROC: 0.9336 Weighted F1-score: 0.7381
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 1, 'Val Loss': 1.216781497001648, 'Val BAcc': np.float64(0.6273942431837168), 'Val Acc': 0.6921739130434783, 'Val ROC': np.float64(0.9336353393121241), 'Val W_F1': 0.738138858710071, 'Val Recall_macro': 0.6273942431837168, 'Val Recall_weighted': 0.6921739130434783}
Max val mean recall: 0.63%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [2]  [ 0/64]  eta: 0:03:10  lr: 0.000100  min_lr: 0.000000  loss: 1.5076 (1.5076)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3444 (2.3444)  time: 2.9731  data: 2.6911  max mem: 14662
Epoch: [2]  [10/64]  eta: 0:00:28  lr: 0.000108  min_lr: 0.000000  loss: 1.5527 (1.5947)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7467 (2.6581)  time: 0.5227  data: 0.2450  max mem: 14662
Epoch: [2]  [20/64]  eta: 0:00:17  lr: 0.000116  min_lr: 0.000000  loss: 1.5434 (1.5603)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7467 (2.8872)  time: 0.2781  data: 0.0004  max mem: 14662
Epoch: [2]  [30/64]  eta: 0:00:12  lr: 0.000124  min_lr: 0.000000  loss: 1.5387 (1.5559)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7901 (2.8259)  time: 0.2792  data: 0.0004  max mem: 14662
Epoch: [2]  [40/64]  eta: 0:00:08  lr: 0.000131  min_lr: 0.000000  loss: 1.5608 (1.5626)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6786 (2.8182)  time: 0.2800  data: 0.0004  max mem: 14662
Epoch: [2]  [50/64]  eta: 0:00:04  lr: 0.000139  min_lr: 0.000001  loss: 1.4813 (1.5344)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6973 (2.8411)  time: 0.2800  data: 0.0003  max mem: 14662
Epoch: [2]  [60/64]  eta: 0:00:01  lr: 0.000147  min_lr: 0.000001  loss: 1.4171 (1.5326)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1552 (2.9094)  time: 0.2799  data: 0.0002  max mem: 14662
Epoch: [2]  [63/64]  eta: 0:00:00  lr: 0.000149  min_lr: 0.000001  loss: 1.4171 (1.5285)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1552 (2.9136)  time: 0.2799  data: 0.0002  max mem: 14662
Epoch: [2] Total time: 0:00:20 (0.3229 s / it)
2025-04-28 16:27:11 Averaged stats: lr: 0.000149  min_lr: 0.000001  loss: 1.4171 (1.5285)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1552 (2.9136)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5047  data: 2.3641  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9307  data: 0.7881  max mem: 14662
Test: Total time: 0:00:02 (0.9530 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6929 Acc: 0.7930 Recall_macro: 0.6929 Recall_weighted: 0.7930 AUC-ROC: 0.9698 Weighted F1-score: 0.8231
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 2, 'Val Loss': 0.8157287836074829, 'Val BAcc': np.float64(0.6928533454849245), 'Val Acc': 0.7930434782608695, 'Val ROC': np.float64(0.9698295125214963), 'Val W_F1': 0.8231096109941561, 'Val Recall_macro': 0.6928533454849245, 'Val Recall_weighted': 0.7930434782608695}
Max val mean recall: 0.69%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [3]  [ 0/64]  eta: 0:03:15  lr: 0.000150  min_lr: 0.000001  loss: 1.4460 (1.4460)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4039 (3.4039)  time: 3.0551  data: 2.7732  max mem: 14662
Epoch: [3]  [10/64]  eta: 0:00:28  lr: 0.000158  min_lr: 0.000001  loss: 1.3546 (1.3664)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4039 (3.5093)  time: 0.5330  data: 0.2525  max mem: 14662
Epoch: [3]  [20/64]  eta: 0:00:18  lr: 0.000166  min_lr: 0.000001  loss: 1.3637 (1.3760)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2111 (3.4953)  time: 0.2817  data: 0.0005  max mem: 14662
Epoch: [3]  [30/64]  eta: 0:00:12  lr: 0.000174  min_lr: 0.000001  loss: 1.3831 (1.3904)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5820 (3.5883)  time: 0.2822  data: 0.0004  max mem: 14662
Epoch: [3]  [40/64]  eta: 0:00:08  lr: 0.000182  min_lr: 0.000001  loss: 1.4256 (1.4021)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5820 (3.5716)  time: 0.2825  data: 0.0004  max mem: 14662
Epoch: [3]  [50/64]  eta: 0:00:04  lr: 0.000189  min_lr: 0.000001  loss: 1.4663 (1.4201)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3572 (3.5629)  time: 0.2835  data: 0.0003  max mem: 14662
Epoch: [3]  [60/64]  eta: 0:00:01  lr: 0.000197  min_lr: 0.000001  loss: 1.3777 (1.4038)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3441 (3.5697)  time: 0.2838  data: 0.0002  max mem: 14662
Epoch: [3]  [63/64]  eta: 0:00:00  lr: 0.000200  min_lr: 0.000001  loss: 1.3528 (1.3907)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3505 (3.5694)  time: 0.2839  data: 0.0002  max mem: 14662
Epoch: [3] Total time: 0:00:20 (0.3276 s / it)
2025-04-28 16:27:36 Averaged stats: lr: 0.000200  min_lr: 0.000001  loss: 1.3528 (1.3907)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3505 (3.5694)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5627  data: 2.4198  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9513  data: 0.8067  max mem: 14662
Test: Total time: 0:00:02 (0.9716 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7326 Acc: 0.7513 Recall_macro: 0.7326 Recall_weighted: 0.7513 AUC-ROC: 0.9498 Weighted F1-score: 0.7872
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 3, 'Val Loss': 0.8564957976341248, 'Val BAcc': np.float64(0.7325784157363104), 'Val Acc': 0.7513043478260869, 'Val ROC': np.float64(0.949822847239656), 'Val W_F1': 0.7871942215803387, 'Val Recall_macro': 0.7325784157363104, 'Val Recall_weighted': 0.7513043478260869}
Max val mean recall: 0.73%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [4]  [ 0/64]  eta: 0:03:00  lr: 0.000200  min_lr: 0.000001  loss: 1.0193 (1.0193)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3702 (4.3702)  time: 2.8165  data: 2.5334  max mem: 14662
Epoch: [4]  [10/64]  eta: 0:00:27  lr: 0.000208  min_lr: 0.000001  loss: 1.3113 (1.2967)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0568 (3.8379)  time: 0.5128  data: 0.2307  max mem: 14662
Epoch: [4]  [20/64]  eta: 0:00:17  lr: 0.000216  min_lr: 0.000001  loss: 1.4664 (1.3987)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0568 (4.1299)  time: 0.2830  data: 0.0004  max mem: 14662
Epoch: [4]  [30/64]  eta: 0:00:12  lr: 0.000224  min_lr: 0.000001  loss: 1.5090 (1.3868)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9668 (4.0084)  time: 0.2840  data: 0.0004  max mem: 14662
Epoch: [4]  [40/64]  eta: 0:00:08  lr: 0.000232  min_lr: 0.000001  loss: 1.2262 (1.3416)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4331 (3.8235)  time: 0.2851  data: 0.0004  max mem: 14662
Epoch: [4]  [50/64]  eta: 0:00:04  lr: 0.000239  min_lr: 0.000001  loss: 1.2525 (1.3461)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4387 (3.7958)  time: 0.2857  data: 0.0003  max mem: 14662
Epoch: [4]  [60/64]  eta: 0:00:01  lr: 0.000247  min_lr: 0.000001  loss: 1.4970 (1.3679)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5040 (3.7315)  time: 0.2857  data: 0.0002  max mem: 14662
Epoch: [4]  [63/64]  eta: 0:00:00  lr: 0.000250  min_lr: 0.000001  loss: 1.4970 (1.3679)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3110 (3.6925)  time: 0.2858  data: 0.0001  max mem: 14662
Epoch: [4] Total time: 0:00:20 (0.3259 s / it)
2025-04-28 16:28:02 Averaged stats: lr: 0.000250  min_lr: 0.000001  loss: 1.4970 (1.3679)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3110 (3.6925)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5666  data: 2.4251  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9532  data: 0.8085  max mem: 14662
Test: Total time: 0:00:02 (0.9791 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8005 Acc: 0.8313 Recall_macro: 0.8005 Recall_weighted: 0.8313 AUC-ROC: 0.9778 Weighted F1-score: 0.8580
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 4, 'Val Loss': 0.6664655804634094, 'Val BAcc': np.float64(0.8005149236728185), 'Val Acc': 0.831304347826087, 'Val ROC': np.float64(0.9778406659613866), 'Val W_F1': 0.8580413880722793, 'Val Recall_macro': 0.8005149236728185, 'Val Recall_weighted': 0.831304347826087}
Max val mean recall: 0.80%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [5]  [ 0/64]  eta: 0:03:00  lr: 0.000250  min_lr: 0.000001  loss: 1.6062 (1.6062)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4503 (4.4503)  time: 2.8209  data: 2.5301  max mem: 14662
Epoch: [5]  [10/64]  eta: 0:00:27  lr: 0.000258  min_lr: 0.000001  loss: 1.4296 (1.3441)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3514 (3.3795)  time: 0.5145  data: 0.2304  max mem: 14662
Epoch: [5]  [20/64]  eta: 0:00:17  lr: 0.000266  min_lr: 0.000001  loss: 1.4296 (1.3859)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3063 (3.4074)  time: 0.2844  data: 0.0004  max mem: 14662
Epoch: [5]  [30/64]  eta: 0:00:12  lr: 0.000274  min_lr: 0.000001  loss: 1.3976 (1.3759)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3063 (3.3988)  time: 0.2852  data: 0.0003  max mem: 14662
Epoch: [5]  [40/64]  eta: 0:00:08  lr: 0.000282  min_lr: 0.000001  loss: 1.3663 (1.3773)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5729 (3.5776)  time: 0.2862  data: 0.0003  max mem: 14662
Epoch: [5]  [50/64]  eta: 0:00:04  lr: 0.000290  min_lr: 0.000001  loss: 1.3280 (1.3687)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8356 (3.6106)  time: 0.2875  data: 0.0003  max mem: 14662
Epoch: [5]  [60/64]  eta: 0:00:01  lr: 0.000297  min_lr: 0.000001  loss: 1.3421 (1.3590)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6340 (3.6064)  time: 0.2879  data: 0.0002  max mem: 14662
Epoch: [5]  [63/64]  eta: 0:00:00  lr: 0.000300  min_lr: 0.000001  loss: 1.3280 (1.3516)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4060 (3.5981)  time: 0.2879  data: 0.0001  max mem: 14662
Epoch: [5] Total time: 0:00:20 (0.3272 s / it)
2025-04-28 16:28:28 Averaged stats: lr: 0.000300  min_lr: 0.000001  loss: 1.3280 (1.3516)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4060 (3.5981)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4278  data: 2.2853  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9064  data: 0.7619  max mem: 14662
Test: Total time: 0:00:02 (0.9303 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8063 Acc: 0.8330 Recall_macro: 0.8063 Recall_weighted: 0.8330 AUC-ROC: 0.9810 Weighted F1-score: 0.8557
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 5, 'Val Loss': 0.5963735580444336, 'Val BAcc': np.float64(0.8063294600136706), 'Val Acc': 0.8330434782608696, 'Val ROC': np.float64(0.9810108105958664), 'Val W_F1': 0.8556560312836377, 'Val Recall_macro': 0.8063294600136706, 'Val Recall_weighted': 0.8330434782608696}
Max val mean recall: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [6]  [ 0/64]  eta: 0:03:11  lr: 0.000300  min_lr: 0.000001  loss: 1.0473 (1.0473)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8832 (3.8832)  time: 2.9863  data: 2.7052  max mem: 14662
Epoch: [6]  [10/64]  eta: 0:00:28  lr: 0.000308  min_lr: 0.000001  loss: 1.4054 (1.3332)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4686 (3.4463)  time: 0.5302  data: 0.2462  max mem: 14662
Epoch: [6]  [20/64]  eta: 0:00:18  lr: 0.000316  min_lr: 0.000001  loss: 1.4054 (1.3738)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4696 (3.5765)  time: 0.2855  data: 0.0003  max mem: 14662
Epoch: [6]  [30/64]  eta: 0:00:12  lr: 0.000324  min_lr: 0.000001  loss: 1.4011 (1.3784)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1363 (3.3915)  time: 0.2872  data: 0.0003  max mem: 14662
Epoch: [6]  [40/64]  eta: 0:00:08  lr: 0.000332  min_lr: 0.000001  loss: 1.3647 (1.3556)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1363 (3.4166)  time: 0.2878  data: 0.0003  max mem: 14662
Epoch: [6]  [50/64]  eta: 0:00:04  lr: 0.000340  min_lr: 0.000001  loss: 1.4204 (1.3755)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4963 (3.4279)  time: 0.2873  data: 0.0003  max mem: 14662
Epoch: [6]  [60/64]  eta: 0:00:01  lr: 0.000347  min_lr: 0.000001  loss: 1.4204 (1.3759)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1093 (3.3730)  time: 0.2871  data: 0.0002  max mem: 14662
Epoch: [6]  [63/64]  eta: 0:00:00  lr: 0.000350  min_lr: 0.000001  loss: 1.2926 (1.3666)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0459 (3.3505)  time: 0.2871  data: 0.0001  max mem: 14662
Epoch: [6] Total time: 0:00:21 (0.3303 s / it)
2025-04-28 16:28:54 Averaged stats: lr: 0.000350  min_lr: 0.000001  loss: 1.2926 (1.3666)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0459 (3.3505)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:08    time: 2.6887  data: 2.5454  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9942  data: 0.8486  max mem: 14662
Test: Total time: 0:00:03 (1.0164 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8114 Acc: 0.8296 Recall_macro: 0.8114 Recall_weighted: 0.8296 AUC-ROC: 0.9817 Weighted F1-score: 0.8542
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 6, 'Val Loss': 0.6189098954200745, 'Val BAcc': np.float64(0.811381484013063), 'Val Acc': 0.8295652173913044, 'Val ROC': np.float64(0.9817094806534614), 'Val W_F1': 0.8541974405836863, 'Val Recall_macro': 0.811381484013063, 'Val Recall_weighted': 0.8295652173913044}
Max val mean recall: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [7]  [ 0/64]  eta: 0:02:59  lr: 0.000351  min_lr: 0.000001  loss: 1.3571 (1.3571)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9117 (2.9117)  time: 2.8075  data: 2.5249  max mem: 14662
Epoch: [7]  [10/64]  eta: 0:00:27  lr: 0.000358  min_lr: 0.000001  loss: 1.1211 (1.1794)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4745 (3.3810)  time: 0.5152  data: 0.2298  max mem: 14662
Epoch: [7]  [20/64]  eta: 0:00:17  lr: 0.000366  min_lr: 0.000001  loss: 1.1084 (1.1614)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5562 (3.5708)  time: 0.2860  data: 0.0003  max mem: 14662
Epoch: [7]  [30/64]  eta: 0:00:12  lr: 0.000374  min_lr: 0.000001  loss: 1.1328 (1.1833)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7379 (3.6745)  time: 0.2873  data: 0.0003  max mem: 14662
Epoch: [7]  [40/64]  eta: 0:00:08  lr: 0.000382  min_lr: 0.000001  loss: 1.2730 (1.2331)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6442 (3.6535)  time: 0.2880  data: 0.0003  max mem: 14662
Epoch: [7]  [50/64]  eta: 0:00:04  lr: 0.000390  min_lr: 0.000001  loss: 1.2799 (1.2383)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1598 (3.5022)  time: 0.2880  data: 0.0002  max mem: 14662
Epoch: [7]  [60/64]  eta: 0:00:01  lr: 0.000397  min_lr: 0.000001  loss: 1.3695 (1.2629)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1992 (3.5642)  time: 0.2882  data: 0.0001  max mem: 14662
Epoch: [7]  [63/64]  eta: 0:00:00  lr: 0.000400  min_lr: 0.000001  loss: 1.3695 (1.2711)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3901 (3.5648)  time: 0.2882  data: 0.0001  max mem: 14662
Epoch: [7] Total time: 0:00:20 (0.3281 s / it)
2025-04-28 16:29:20 Averaged stats: lr: 0.000400  min_lr: 0.000001  loss: 1.3695 (1.2711)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3901 (3.5648)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4357  data: 2.2948  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9095  data: 0.7650  max mem: 14662
Test: Total time: 0:00:02 (0.9305 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7761 Acc: 0.8365 Recall_macro: 0.7761 Recall_weighted: 0.8365 AUC-ROC: 0.9635 Weighted F1-score: 0.8647
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 7, 'Val Loss': 0.5573110580444336, 'Val BAcc': np.float64(0.7761418698260805), 'Val Acc': 0.8365217391304348, 'Val ROC': np.float64(0.9634649390547868), 'Val W_F1': 0.864747797467679, 'Val Recall_macro': 0.7761418698260805, 'Val Recall_weighted': 0.8365217391304348}
Max val mean recall: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [8]  [ 0/64]  eta: 0:02:57  lr: 0.000401  min_lr: 0.000001  loss: 1.5864 (1.5864)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4040 (4.4040)  time: 2.7783  data: 2.4940  max mem: 14662
Epoch: [8]  [10/64]  eta: 0:00:27  lr: 0.000408  min_lr: 0.000002  loss: 1.3587 (1.3100)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2304 (3.3163)  time: 0.5133  data: 0.2270  max mem: 14662
Epoch: [8]  [20/64]  eta: 0:00:17  lr: 0.000416  min_lr: 0.000002  loss: 1.3298 (1.2802)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2515 (3.6348)  time: 0.2875  data: 0.0003  max mem: 14662
Epoch: [8]  [30/64]  eta: 0:00:12  lr: 0.000424  min_lr: 0.000002  loss: 1.3270 (1.2532)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6438 (3.6260)  time: 0.2885  data: 0.0003  max mem: 14662
Epoch: [8]  [40/64]  eta: 0:00:08  lr: 0.000432  min_lr: 0.000002  loss: 1.3725 (1.2713)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6597 (3.6538)  time: 0.2894  data: 0.0003  max mem: 14662
Epoch: [8]  [50/64]  eta: 0:00:04  lr: 0.000440  min_lr: 0.000002  loss: 1.3946 (1.2889)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5003 (3.6127)  time: 0.2902  data: 0.0002  max mem: 14662
Epoch: [8]  [60/64]  eta: 0:00:01  lr: 0.000448  min_lr: 0.000002  loss: 1.3665 (1.2925)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0926 (3.5400)  time: 0.2903  data: 0.0002  max mem: 14662
Epoch: [8]  [63/64]  eta: 0:00:00  lr: 0.000450  min_lr: 0.000002  loss: 1.3584 (1.2885)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1766 (3.5372)  time: 0.2904  data: 0.0001  max mem: 14662
Epoch: [8] Total time: 0:00:21 (0.3294 s / it)
2025-04-28 16:29:44 Averaged stats: lr: 0.000450  min_lr: 0.000002  loss: 1.3584 (1.2885)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1766 (3.5372)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4736  data: 2.3320  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9227  data: 0.7775  max mem: 14662
Test: Total time: 0:00:02 (0.9509 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7998 Acc: 0.8661 Recall_macro: 0.7998 Recall_weighted: 0.8661 AUC-ROC: 0.9842 Weighted F1-score: 0.8843
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 8, 'Val Loss': 0.5006148219108582, 'Val BAcc': np.float64(0.7997812713602188), 'Val Acc': 0.8660869565217392, 'Val ROC': np.float64(0.9842133073892521), 'Val W_F1': 0.8842810320477983, 'Val Recall_macro': 0.7997812713602188, 'Val Recall_weighted': 0.8660869565217392}
Max val mean recall: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [9]  [ 0/64]  eta: 0:02:55  lr: 0.000451  min_lr: 0.000002  loss: 1.6341 (1.6341)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8555 (3.8555)  time: 2.7500  data: 2.4584  max mem: 14662
Epoch: [9]  [10/64]  eta: 0:00:31  lr: 0.000459  min_lr: 0.000002  loss: 1.3977 (1.3557)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7463 (3.5681)  time: 0.5887  data: 0.3015  max mem: 14662
Epoch: [9]  [20/64]  eta: 0:00:19  lr: 0.000466  min_lr: 0.000002  loss: 1.3637 (1.3253)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5138 (3.5190)  time: 0.3303  data: 0.0430  max mem: 14662
Epoch: [9]  [30/64]  eta: 0:00:13  lr: 0.000474  min_lr: 0.000002  loss: 1.2165 (1.2922)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5702 (3.5821)  time: 0.2886  data: 0.0003  max mem: 14662
Epoch: [9]  [40/64]  eta: 0:00:08  lr: 0.000482  min_lr: 0.000002  loss: 1.2163 (1.2912)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4063 (3.5629)  time: 0.2893  data: 0.0003  max mem: 14662
Epoch: [9]  [50/64]  eta: 0:00:04  lr: 0.000490  min_lr: 0.000002  loss: 1.2888 (1.2994)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9904 (3.4549)  time: 0.2896  data: 0.0002  max mem: 14662
Epoch: [9]  [60/64]  eta: 0:00:01  lr: 0.000498  min_lr: 0.000002  loss: 1.3044 (1.2987)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1897 (3.4557)  time: 0.2895  data: 0.0001  max mem: 14662
Epoch: [9]  [63/64]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000002  loss: 1.3044 (1.2967)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1897 (3.4507)  time: 0.2897  data: 0.0001  max mem: 14662
Epoch: [9] Total time: 0:00:21 (0.3422 s / it)
2025-04-28 16:30:09 Averaged stats: lr: 0.000500  min_lr: 0.000002  loss: 1.3044 (1.2967)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1897 (3.4507)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4584  data: 2.3162  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9187  data: 0.7722  max mem: 14662
Test: Total time: 0:00:02 (0.9458 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8344 Acc: 0.7774 Recall_macro: 0.8344 Recall_weighted: 0.7774 AUC-ROC: 0.9720 Weighted F1-score: 0.8212
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 9, 'Val Loss': 0.7286869287490845, 'Val BAcc': np.float64(0.8343788041532402), 'Val Acc': 0.7773913043478261, 'Val ROC': np.float64(0.9720010759305316), 'Val W_F1': 0.82115631036601, 'Val Recall_macro': 0.8343788041532402, 'Val Recall_weighted': 0.7773913043478261}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [10]  [ 0/64]  eta: 0:03:05  lr: 0.000500  min_lr: 0.000002  loss: 0.8694 (0.8694)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3129 (3.3129)  time: 2.8973  data: 2.6145  max mem: 14662
Epoch: [10]  [10/64]  eta: 0:00:28  lr: 0.000500  min_lr: 0.000002  loss: 1.1418 (1.2020)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0922 (3.1254)  time: 0.5248  data: 0.2380  max mem: 14662
Epoch: [10]  [20/64]  eta: 0:00:18  lr: 0.000500  min_lr: 0.000002  loss: 1.1385 (1.1379)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0792 (3.1450)  time: 0.2878  data: 0.0003  max mem: 14662
Epoch: [10]  [30/64]  eta: 0:00:12  lr: 0.000500  min_lr: 0.000002  loss: 1.2457 (1.2032)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2873 (3.3442)  time: 0.2882  data: 0.0003  max mem: 14662
Epoch: [10]  [40/64]  eta: 0:00:08  lr: 0.000500  min_lr: 0.000002  loss: 1.3162 (1.2176)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3334 (3.3367)  time: 0.2887  data: 0.0003  max mem: 14662
Epoch: [10]  [50/64]  eta: 0:00:04  lr: 0.000500  min_lr: 0.000002  loss: 1.1907 (1.1982)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2938 (3.3908)  time: 0.2888  data: 0.0002  max mem: 14662
Epoch: [10]  [60/64]  eta: 0:00:01  lr: 0.000499  min_lr: 0.000002  loss: 1.1390 (1.1996)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5009 (3.4481)  time: 0.2886  data: 0.0001  max mem: 14662
Epoch: [10]  [63/64]  eta: 0:00:00  lr: 0.000499  min_lr: 0.000002  loss: 1.0521 (1.2018)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6744 (3.4707)  time: 0.2886  data: 0.0001  max mem: 14662
Epoch: [10] Total time: 0:00:21 (0.3305 s / it)
2025-04-28 16:30:35 Averaged stats: lr: 0.000499  min_lr: 0.000002  loss: 1.0521 (1.2018)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6744 (3.4707)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4230  data: 2.2797  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9045  data: 0.7600  max mem: 14662
Test: Total time: 0:00:02 (0.9282 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8196 Acc: 0.8574 Recall_macro: 0.8196 Recall_weighted: 0.8574 AUC-ROC: 0.9879 Weighted F1-score: 0.8840
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 10, 'Val Loss': 0.5134629011154175, 'Val BAcc': np.float64(0.8196161399168919), 'Val Acc': 0.8573913043478261, 'Val ROC': np.float64(0.9878668921969862), 'Val W_F1': 0.883962486070647, 'Val Recall_macro': 0.8196161399168919, 'Val Recall_weighted': 0.8573913043478261}
Max val mean recall: 0.83%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [11]  [ 0/64]  eta: 0:02:52  lr: 0.000499  min_lr: 0.000002  loss: 1.4855 (1.4855)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1163 (4.1163)  time: 2.6972  data: 2.4107  max mem: 14662
Epoch: [11]  [10/64]  eta: 0:00:27  lr: 0.000499  min_lr: 0.000002  loss: 1.0934 (1.1510)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3961 (3.3121)  time: 0.5068  data: 0.2194  max mem: 14662
Epoch: [11]  [20/64]  eta: 0:00:17  lr: 0.000499  min_lr: 0.000002  loss: 1.1201 (1.1682)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3749 (3.3180)  time: 0.2884  data: 0.0003  max mem: 14662
Epoch: [11]  [30/64]  eta: 0:00:12  lr: 0.000498  min_lr: 0.000002  loss: 1.2673 (1.2046)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9549 (3.1988)  time: 0.2891  data: 0.0003  max mem: 14662
Epoch: [11]  [40/64]  eta: 0:00:08  lr: 0.000498  min_lr: 0.000002  loss: 1.3623 (1.2321)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9503 (3.2181)  time: 0.2899  data: 0.0003  max mem: 14662
Epoch: [11]  [50/64]  eta: 0:00:04  lr: 0.000498  min_lr: 0.000002  loss: 1.3948 (1.2184)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7260 (3.4375)  time: 0.2901  data: 0.0003  max mem: 14662
Epoch: [11]  [60/64]  eta: 0:00:01  lr: 0.000497  min_lr: 0.000002  loss: 1.3163 (1.2373)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0575 (3.5039)  time: 0.2902  data: 0.0002  max mem: 14662
Epoch: [11]  [63/64]  eta: 0:00:00  lr: 0.000497  min_lr: 0.000002  loss: 1.2943 (1.2374)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7260 (3.4878)  time: 0.2903  data: 0.0001  max mem: 14662
Epoch: [11] Total time: 0:00:21 (0.3287 s / it)
2025-04-28 16:30:59 Averaged stats: lr: 0.000497  min_lr: 0.000002  loss: 1.2943 (1.2374)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7260 (3.4878)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.6111  data: 2.4681  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9678  data: 0.8229  max mem: 14662
Test: Total time: 0:00:02 (0.9942 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8398 Acc: 0.8922 Recall_macro: 0.8398 Recall_weighted: 0.8922 AUC-ROC: 0.9873 Weighted F1-score: 0.9021
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 11, 'Val Loss': 0.437978059053421, 'Val BAcc': np.float64(0.8398238019290651), 'Val Acc': 0.8921739130434783, 'Val ROC': np.float64(0.987284124123127), 'Val W_F1': 0.9020825925221757, 'Val Recall_macro': 0.8398238019290651, 'Val Recall_weighted': 0.8921739130434783}
Max val mean recall: 0.84%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [12]  [ 0/64]  eta: 0:02:58  lr: 0.000497  min_lr: 0.000002  loss: 1.0666 (1.0666)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5766 (2.5766)  time: 2.7890  data: 2.4883  max mem: 14662
Epoch: [12]  [10/64]  eta: 0:00:28  lr: 0.000496  min_lr: 0.000002  loss: 1.3137 (1.2284)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2649 (3.4573)  time: 0.5190  data: 0.2267  max mem: 14662
Epoch: [12]  [20/64]  eta: 0:00:18  lr: 0.000496  min_lr: 0.000002  loss: 1.2329 (1.2242)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9697 (3.2272)  time: 0.2916  data: 0.0005  max mem: 14662
Epoch: [12]  [30/64]  eta: 0:00:12  lr: 0.000495  min_lr: 0.000002  loss: 1.2306 (1.2214)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8986 (3.1880)  time: 0.2907  data: 0.0004  max mem: 14662
Epoch: [12]  [40/64]  eta: 0:00:08  lr: 0.000495  min_lr: 0.000002  loss: 1.0772 (1.1781)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0030 (3.2147)  time: 0.2899  data: 0.0003  max mem: 14662
Epoch: [12]  [50/64]  eta: 0:00:04  lr: 0.000494  min_lr: 0.000002  loss: 1.0772 (1.1881)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1043 (3.2661)  time: 0.2899  data: 0.0002  max mem: 14662
Epoch: [12]  [60/64]  eta: 0:00:01  lr: 0.000493  min_lr: 0.000002  loss: 1.1593 (1.1689)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3111 (3.2703)  time: 0.2905  data: 0.0002  max mem: 14662
Epoch: [12]  [63/64]  eta: 0:00:00  lr: 0.000493  min_lr: 0.000002  loss: 1.1593 (1.1663)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3111 (3.2907)  time: 0.2905  data: 0.0001  max mem: 14662
Epoch: [12] Total time: 0:00:21 (0.3311 s / it)
2025-04-28 16:31:25 Averaged stats: lr: 0.000493  min_lr: 0.000002  loss: 1.1593 (1.1663)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3111 (3.2907)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.6180  data: 2.4754  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9683  data: 0.8252  max mem: 14662
Test: Total time: 0:00:02 (0.9925 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8226 Acc: 0.8678 Recall_macro: 0.8226 Recall_weighted: 0.8678 AUC-ROC: 0.9853 Weighted F1-score: 0.8882
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 12, 'Val Loss': 0.4383867681026459, 'Val BAcc': np.float64(0.8226175829183348), 'Val Acc': 0.8678260869565217, 'Val ROC': np.float64(0.9853262555720992), 'Val W_F1': 0.8882277214047187, 'Val Recall_macro': 0.8226175829183348, 'Val Recall_weighted': 0.8678260869565217}
Max val mean recall: 0.84%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [13]  [ 0/64]  eta: 0:03:11  lr: 0.000493  min_lr: 0.000002  loss: 1.2263 (1.2263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8248 (2.8248)  time: 2.9873  data: 2.7022  max mem: 14662
Epoch: [13]  [10/64]  eta: 0:00:28  lr: 0.000492  min_lr: 0.000002  loss: 1.2061 (1.1378)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0143 (3.1365)  time: 0.5344  data: 0.2460  max mem: 14662
Epoch: [13]  [20/64]  eta: 0:00:18  lr: 0.000492  min_lr: 0.000002  loss: 1.0829 (1.1243)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3132 (3.3907)  time: 0.2897  data: 0.0004  max mem: 14662
Epoch: [13]  [30/64]  eta: 0:00:12  lr: 0.000491  min_lr: 0.000002  loss: 1.1523 (1.1401)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4564 (3.3827)  time: 0.2904  data: 0.0004  max mem: 14662
Epoch: [13]  [40/64]  eta: 0:00:08  lr: 0.000490  min_lr: 0.000002  loss: 1.2708 (1.1588)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4564 (3.4041)  time: 0.2905  data: 0.0004  max mem: 14662
Epoch: [13]  [50/64]  eta: 0:00:04  lr: 0.000489  min_lr: 0.000002  loss: 1.2139 (1.1471)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3937 (3.3848)  time: 0.2906  data: 0.0003  max mem: 14662
Epoch: [13]  [60/64]  eta: 0:00:01  lr: 0.000488  min_lr: 0.000002  loss: 1.1375 (1.1456)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1364 (3.3716)  time: 0.2901  data: 0.0002  max mem: 14662
Epoch: [13]  [63/64]  eta: 0:00:00  lr: 0.000488  min_lr: 0.000002  loss: 1.2003 (1.1489)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2074 (3.3609)  time: 0.2900  data: 0.0001  max mem: 14662
Epoch: [13] Total time: 0:00:21 (0.3338 s / it)
2025-04-28 16:31:49 Averaged stats: lr: 0.000488  min_lr: 0.000002  loss: 1.2003 (1.1489)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2074 (3.3609)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4316  data: 2.2886  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9085  data: 0.7630  max mem: 14662
Test: Total time: 0:00:02 (0.9282 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8056 Acc: 0.8904 Recall_macro: 0.8056 Recall_weighted: 0.8904 AUC-ROC: 0.9878 Weighted F1-score: 0.9021
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 13, 'Val Loss': 0.3975008726119995, 'Val BAcc': np.float64(0.8056205448686652), 'Val Acc': 0.8904347826086957, 'Val ROC': np.float64(0.9877969259751636), 'Val W_F1': 0.9020956870318846, 'Val Recall_macro': 0.8056205448686652, 'Val Recall_weighted': 0.8904347826086957}
Max val mean recall: 0.84%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [14]  [ 0/64]  eta: 0:03:05  lr: 0.000488  min_lr: 0.000002  loss: 1.0546 (1.0546)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0727 (3.0727)  time: 2.8939  data: 2.6057  max mem: 14662
Epoch: [14]  [10/64]  eta: 0:00:28  lr: 0.000487  min_lr: 0.000002  loss: 1.3143 (1.1536)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9478 (2.9658)  time: 0.5272  data: 0.2372  max mem: 14662
Epoch: [14]  [20/64]  eta: 0:00:18  lr: 0.000486  min_lr: 0.000002  loss: 1.3143 (1.1677)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8989 (3.0009)  time: 0.2909  data: 0.0004  max mem: 14662
Epoch: [14]  [30/64]  eta: 0:00:12  lr: 0.000485  min_lr: 0.000002  loss: 1.2403 (1.2029)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9665 (3.0085)  time: 0.2910  data: 0.0004  max mem: 14662
Epoch: [14]  [40/64]  eta: 0:00:08  lr: 0.000484  min_lr: 0.000002  loss: 1.2340 (1.1899)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9665 (2.9973)  time: 0.2910  data: 0.0004  max mem: 14662
Epoch: [14]  [50/64]  eta: 0:00:04  lr: 0.000483  min_lr: 0.000002  loss: 1.1065 (1.1686)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2186 (3.0924)  time: 0.2909  data: 0.0003  max mem: 14662
Epoch: [14]  [60/64]  eta: 0:00:01  lr: 0.000481  min_lr: 0.000002  loss: 1.0590 (1.1522)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3006 (3.1045)  time: 0.2904  data: 0.0002  max mem: 14662
Epoch: [14]  [63/64]  eta: 0:00:00  lr: 0.000481  min_lr: 0.000002  loss: 1.0590 (1.1530)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2327 (3.1186)  time: 0.2907  data: 0.0001  max mem: 14662
Epoch: [14] Total time: 0:00:21 (0.3329 s / it)
2025-04-28 16:32:13 Averaged stats: lr: 0.000481  min_lr: 0.000002  loss: 1.0590 (1.1530)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2327 (3.1186)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5751  data: 2.4267  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9582  data: 0.8091  max mem: 14662
Test: Total time: 0:00:02 (0.9825 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8159 Acc: 0.9217 Recall_macro: 0.8159 Recall_weighted: 0.9217 AUC-ROC: 0.9874 Weighted F1-score: 0.9239
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 14, 'Val Loss': 0.35605737566947937, 'Val BAcc': np.float64(0.8158547884863674), 'Val Acc': 0.9217391304347826, 'Val ROC': np.float64(0.9873733207506813), 'Val W_F1': 0.9239153178069411, 'Val Recall_macro': 0.8158547884863674, 'Val Recall_weighted': 0.9217391304347826}
Max val mean recall: 0.84%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [15]  [ 0/64]  eta: 0:02:51  lr: 0.000481  min_lr: 0.000002  loss: 1.2847 (1.2847)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2250 (3.2250)  time: 2.6728  data: 2.3866  max mem: 14662
Epoch: [15]  [10/64]  eta: 0:00:27  lr: 0.000480  min_lr: 0.000002  loss: 1.2387 (1.2370)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0366 (3.1315)  time: 0.5072  data: 0.2173  max mem: 14662
Epoch: [15]  [20/64]  eta: 0:00:17  lr: 0.000479  min_lr: 0.000002  loss: 1.2288 (1.2073)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0366 (3.2215)  time: 0.2913  data: 0.0004  max mem: 14662
Epoch: [15]  [30/64]  eta: 0:00:12  lr: 0.000477  min_lr: 0.000002  loss: 1.2288 (1.1921)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3841 (3.2909)  time: 0.2915  data: 0.0004  max mem: 14662
Epoch: [15]  [40/64]  eta: 0:00:08  lr: 0.000476  min_lr: 0.000002  loss: 1.2119 (1.1789)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3850 (3.2625)  time: 0.2912  data: 0.0003  max mem: 14662
Epoch: [15]  [50/64]  eta: 0:00:04  lr: 0.000475  min_lr: 0.000002  loss: 1.2035 (1.1855)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2250 (3.2443)  time: 0.2918  data: 0.0003  max mem: 14662
Epoch: [15]  [60/64]  eta: 0:00:01  lr: 0.000473  min_lr: 0.000002  loss: 1.2626 (1.1768)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8170 (3.1787)  time: 0.2913  data: 0.0002  max mem: 14662
Epoch: [15]  [63/64]  eta: 0:00:00  lr: 0.000473  min_lr: 0.000002  loss: 1.2468 (1.1644)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8114 (3.1699)  time: 0.2913  data: 0.0001  max mem: 14662
Epoch: [15] Total time: 0:00:21 (0.3299 s / it)
2025-04-28 16:32:37 Averaged stats: lr: 0.000473  min_lr: 0.000002  loss: 1.2468 (1.1644)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8114 (3.1699)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4389  data: 2.2982  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9104  data: 0.7662  max mem: 14662
Test: Total time: 0:00:02 (0.9296 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8478 Acc: 0.8800 Recall_macro: 0.8478 Recall_weighted: 0.8800 AUC-ROC: 0.9830 Weighted F1-score: 0.8947
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 15, 'Val Loss': 0.4287090301513672, 'Val BAcc': np.float64(0.8478165109744057), 'Val Acc': 0.88, 'Val ROC': np.float64(0.9829882768575354), 'Val W_F1': 0.8947002410353568, 'Val Recall_macro': 0.8478165109744057, 'Val Recall_weighted': 0.88}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [16]  [ 0/64]  eta: 0:02:51  lr: 0.000473  min_lr: 0.000002  loss: 0.6539 (0.6539)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0739 (3.0739)  time: 2.6726  data: 2.3876  max mem: 14662
Epoch: [16]  [10/64]  eta: 0:00:27  lr: 0.000471  min_lr: 0.000002  loss: 1.1766 (1.0980)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1929 (3.5608)  time: 0.5054  data: 0.2174  max mem: 14662
Epoch: [16]  [20/64]  eta: 0:00:17  lr: 0.000470  min_lr: 0.000002  loss: 1.2665 (1.1782)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1700 (3.4243)  time: 0.2897  data: 0.0004  max mem: 14662
Epoch: [16]  [30/64]  eta: 0:00:12  lr: 0.000468  min_lr: 0.000002  loss: 1.3250 (1.1962)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0779 (3.3458)  time: 0.2895  data: 0.0003  max mem: 14662
Epoch: [16]  [40/64]  eta: 0:00:08  lr: 0.000467  min_lr: 0.000002  loss: 1.2534 (1.1983)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0584 (3.2894)  time: 0.2892  data: 0.0003  max mem: 14662
Epoch: [16]  [50/64]  eta: 0:00:04  lr: 0.000465  min_lr: 0.000002  loss: 1.1700 (1.1605)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1374 (3.2846)  time: 0.2899  data: 0.0003  max mem: 14662
Epoch: [16]  [60/64]  eta: 0:00:01  lr: 0.000464  min_lr: 0.000002  loss: 1.0472 (1.1610)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2217 (3.3461)  time: 0.2901  data: 0.0002  max mem: 14662
Epoch: [16]  [63/64]  eta: 0:00:00  lr: 0.000463  min_lr: 0.000002  loss: 1.0523 (1.1534)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5276 (3.3540)  time: 0.2901  data: 0.0001  max mem: 14662
Epoch: [16] Total time: 0:00:21 (0.3282 s / it)
2025-04-28 16:33:03 Averaged stats: lr: 0.000463  min_lr: 0.000002  loss: 1.0523 (1.1534)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5276 (3.3540)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4220  data: 2.2775  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9058  data: 0.7593  max mem: 14662
Test: Total time: 0:00:02 (0.9293 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8265 Acc: 0.9287 Recall_macro: 0.8265 Recall_weighted: 0.9287 AUC-ROC: 0.9902 Weighted F1-score: 0.9309
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 16, 'Val Loss': 0.3284752368927002, 'Val BAcc': np.float64(0.8264559667567185), 'Val Acc': 0.928695652173913, 'Val ROC': np.float64(0.9902392813363801), 'Val W_F1': 0.9308601163659116, 'Val Recall_macro': 0.8264559667567185, 'Val Recall_weighted': 0.928695652173913}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [17]  [ 0/64]  eta: 0:03:12  lr: 0.000463  min_lr: 0.000002  loss: 1.1806 (1.1806)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0757 (3.0757)  time: 3.0045  data: 2.7216  max mem: 14662
Epoch: [17]  [10/64]  eta: 0:00:28  lr: 0.000462  min_lr: 0.000002  loss: 1.1806 (1.1321)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3317 (3.4845)  time: 0.5362  data: 0.2478  max mem: 14662
Epoch: [17]  [20/64]  eta: 0:00:18  lr: 0.000460  min_lr: 0.000002  loss: 1.2056 (1.1574)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1335 (3.3766)  time: 0.2893  data: 0.0003  max mem: 14662
Epoch: [17]  [30/64]  eta: 0:00:12  lr: 0.000458  min_lr: 0.000002  loss: 1.2755 (1.2118)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1523 (3.4226)  time: 0.2894  data: 0.0003  max mem: 14662
Epoch: [17]  [40/64]  eta: 0:00:08  lr: 0.000457  min_lr: 0.000002  loss: 1.2755 (1.1932)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2145 (3.3380)  time: 0.2901  data: 0.0003  max mem: 14662
Epoch: [17]  [50/64]  eta: 0:00:04  lr: 0.000455  min_lr: 0.000002  loss: 1.1668 (1.1792)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0041 (3.2407)  time: 0.2910  data: 0.0003  max mem: 14662
Epoch: [17]  [60/64]  eta: 0:00:01  lr: 0.000453  min_lr: 0.000002  loss: 1.1970 (1.1770)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0649 (3.2544)  time: 0.2910  data: 0.0002  max mem: 14662
Epoch: [17]  [63/64]  eta: 0:00:00  lr: 0.000453  min_lr: 0.000002  loss: 1.1970 (1.1790)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0649 (3.2579)  time: 0.2909  data: 0.0001  max mem: 14662
Epoch: [17] Total time: 0:00:21 (0.3338 s / it)
2025-04-28 16:33:27 Averaged stats: lr: 0.000453  min_lr: 0.000002  loss: 1.1970 (1.1790)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0649 (3.2579)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3377  data: 2.1958  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8793  data: 0.7320  max mem: 14662
Test: Total time: 0:00:02 (0.8990 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8448 Acc: 0.9026 Recall_macro: 0.8448 Recall_weighted: 0.9026 AUC-ROC: 0.9888 Weighted F1-score: 0.9153
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 17, 'Val Loss': 0.40038806200027466, 'Val BAcc': np.float64(0.8448124640605844), 'Val Acc': 0.9026086956521739, 'Val ROC': np.float64(0.988837237456204), 'Val W_F1': 0.9153098048718323, 'Val Recall_macro': 0.8448124640605844, 'Val Recall_weighted': 0.9026086956521739}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [18]  [ 0/64]  eta: 0:02:54  lr: 0.000452  min_lr: 0.000002  loss: 1.3450 (1.3450)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3266 (2.3266)  time: 2.7289  data: 2.4431  max mem: 14662
Epoch: [18]  [10/64]  eta: 0:00:27  lr: 0.000451  min_lr: 0.000002  loss: 1.0948 (1.0881)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7459 (3.0189)  time: 0.5108  data: 0.2224  max mem: 14662
Epoch: [18]  [20/64]  eta: 0:00:17  lr: 0.000449  min_lr: 0.000002  loss: 1.1263 (1.1415)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0306 (3.1469)  time: 0.2898  data: 0.0004  max mem: 14662
Epoch: [18]  [30/64]  eta: 0:00:12  lr: 0.000447  min_lr: 0.000002  loss: 1.2919 (1.1981)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8718 (3.0480)  time: 0.2904  data: 0.0004  max mem: 14662
Epoch: [18]  [40/64]  eta: 0:00:08  lr: 0.000445  min_lr: 0.000002  loss: 1.2571 (1.1841)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7722 (3.0326)  time: 0.2907  data: 0.0004  max mem: 14662
Epoch: [18]  [50/64]  eta: 0:00:04  lr: 0.000443  min_lr: 0.000002  loss: 1.1795 (1.1880)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9383 (3.0660)  time: 0.2912  data: 0.0003  max mem: 14662
Epoch: [18]  [60/64]  eta: 0:00:01  lr: 0.000441  min_lr: 0.000002  loss: 1.2297 (1.1865)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6552 (3.0234)  time: 0.2916  data: 0.0002  max mem: 14662
Epoch: [18]  [63/64]  eta: 0:00:00  lr: 0.000440  min_lr: 0.000002  loss: 1.2297 (1.1810)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6552 (3.0237)  time: 0.2915  data: 0.0001  max mem: 14662
Epoch: [18] Total time: 0:00:21 (0.3302 s / it)
2025-04-28 16:33:51 Averaged stats: lr: 0.000440  min_lr: 0.000002  loss: 1.2297 (1.1810)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6552 (3.0237)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3808  data: 2.2377  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8945  data: 0.7461  max mem: 14662
Test: Total time: 0:00:02 (0.9173 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8487 Acc: 0.8870 Recall_macro: 0.8487 Recall_weighted: 0.8870 AUC-ROC: 0.9824 Weighted F1-score: 0.9038
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 18, 'Val Loss': 0.4507606029510498, 'Val BAcc': np.float64(0.8487400318979266), 'Val Acc': 0.8869565217391304, 'Val ROC': np.float64(0.9823698597466678), 'Val W_F1': 0.903799523042689, 'Val Recall_macro': 0.8487400318979266, 'Val Recall_weighted': 0.8869565217391304}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [19]  [ 0/64]  eta: 0:03:08  lr: 0.000440  min_lr: 0.000002  loss: 1.4435 (1.4435)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8009 (3.8009)  time: 2.9481  data: 2.6628  max mem: 14662
Epoch: [19]  [10/64]  eta: 0:00:29  lr: 0.000438  min_lr: 0.000002  loss: 1.0574 (1.0881)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2059 (3.1572)  time: 0.5400  data: 0.2513  max mem: 14662
Epoch: [19]  [20/64]  eta: 0:00:18  lr: 0.000436  min_lr: 0.000002  loss: 1.0574 (1.0757)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1696 (3.1308)  time: 0.2946  data: 0.0053  max mem: 14662
Epoch: [19]  [30/64]  eta: 0:00:12  lr: 0.000434  min_lr: 0.000002  loss: 1.1423 (1.1002)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9424 (3.0707)  time: 0.2904  data: 0.0004  max mem: 14662
Epoch: [19]  [40/64]  eta: 0:00:08  lr: 0.000432  min_lr: 0.000002  loss: 1.1157 (1.0777)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9999 (3.1543)  time: 0.2909  data: 0.0003  max mem: 14662
Epoch: [19]  [50/64]  eta: 0:00:04  lr: 0.000430  min_lr: 0.000002  loss: 1.1517 (1.1200)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4089 (3.1877)  time: 0.2910  data: 0.0003  max mem: 14662
Epoch: [19]  [60/64]  eta: 0:00:01  lr: 0.000428  min_lr: 0.000002  loss: 1.2810 (1.1289)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2196 (3.1910)  time: 0.2909  data: 0.0002  max mem: 14662
Epoch: [19]  [63/64]  eta: 0:00:00  lr: 0.000427  min_lr: 0.000002  loss: 1.2499 (1.1362)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2663 (3.2157)  time: 0.2908  data: 0.0001  max mem: 14662
Epoch: [19] Total time: 0:00:21 (0.3350 s / it)
2025-04-28 16:34:17 Averaged stats: lr: 0.000427  min_lr: 0.000002  loss: 1.2499 (1.1362)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2663 (3.2157)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.6270  data: 2.4834  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9740  data: 0.8279  max mem: 14662
Test: Total time: 0:00:02 (0.9985 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8344 Acc: 0.8904 Recall_macro: 0.8344 Recall_weighted: 0.8904 AUC-ROC: 0.9811 Weighted F1-score: 0.9049
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 19, 'Val Loss': 0.4579007625579834, 'Val BAcc': np.float64(0.8343636146643664), 'Val Acc': 0.8904347826086957, 'Val ROC': np.float64(0.981050578232697), 'Val W_F1': 0.9048563885366068, 'Val Recall_macro': 0.8343636146643664, 'Val Recall_weighted': 0.8904347826086957}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [20]  [ 0/64]  eta: 0:03:19  lr: 0.000427  min_lr: 0.000002  loss: 1.3251 (1.3251)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7855 (2.7855)  time: 3.1244  data: 2.8395  max mem: 14662
Epoch: [20]  [10/64]  eta: 0:00:29  lr: 0.000425  min_lr: 0.000002  loss: 1.2657 (1.2207)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2869 (3.3078)  time: 0.5465  data: 0.2585  max mem: 14662
Epoch: [20]  [20/64]  eta: 0:00:18  lr: 0.000423  min_lr: 0.000002  loss: 1.1972 (1.1877)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2134 (3.1637)  time: 0.2893  data: 0.0004  max mem: 14662
Epoch: [20]  [30/64]  eta: 0:00:12  lr: 0.000420  min_lr: 0.000002  loss: 1.1195 (1.1354)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8190 (3.0286)  time: 0.2898  data: 0.0003  max mem: 14662
Epoch: [20]  [40/64]  eta: 0:00:08  lr: 0.000418  min_lr: 0.000002  loss: 0.9772 (1.0975)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7464 (3.0396)  time: 0.2900  data: 0.0003  max mem: 14662
Epoch: [20]  [50/64]  eta: 0:00:04  lr: 0.000416  min_lr: 0.000002  loss: 1.0341 (1.1090)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1365 (3.1177)  time: 0.2901  data: 0.0003  max mem: 14662
Epoch: [20]  [60/64]  eta: 0:00:01  lr: 0.000413  min_lr: 0.000002  loss: 1.2022 (1.1127)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0657 (3.0814)  time: 0.2909  data: 0.0002  max mem: 14662
Epoch: [20]  [63/64]  eta: 0:00:00  lr: 0.000413  min_lr: 0.000002  loss: 1.1904 (1.1044)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0657 (3.0694)  time: 0.2911  data: 0.0002  max mem: 14662
Epoch: [20] Total time: 0:00:21 (0.3359 s / it)
2025-04-28 16:34:42 Averaged stats: lr: 0.000413  min_lr: 0.000002  loss: 1.1904 (1.1044)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0657 (3.0694)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:08    time: 2.6906  data: 2.5472  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9962  data: 0.8492  max mem: 14662
Test: Total time: 0:00:03 (1.0207 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8094 Acc: 0.9026 Recall_macro: 0.8094 Recall_weighted: 0.9026 AUC-ROC: 0.9875 Weighted F1-score: 0.9110
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 20, 'Val Loss': 0.3766310214996338, 'Val BAcc': np.float64(0.8094285497293016), 'Val Acc': 0.9026086956521739, 'Val ROC': np.float64(0.98754328045551), 'Val W_F1': 0.9110127118255568, 'Val Recall_macro': 0.8094285497293016, 'Val Recall_weighted': 0.9026086956521739}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [21]  [ 0/64]  eta: 0:03:02  lr: 0.000413  min_lr: 0.000002  loss: 1.0503 (1.0503)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2915 (2.2915)  time: 2.8563  data: 2.5713  max mem: 14662
Epoch: [21]  [10/64]  eta: 0:00:28  lr: 0.000410  min_lr: 0.000002  loss: 1.1983 (1.1792)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9306 (2.8373)  time: 0.5218  data: 0.2340  max mem: 14662
Epoch: [21]  [20/64]  eta: 0:00:18  lr: 0.000408  min_lr: 0.000002  loss: 1.2480 (1.2043)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9306 (2.9222)  time: 0.2888  data: 0.0003  max mem: 14662
Epoch: [21]  [30/64]  eta: 0:00:12  lr: 0.000405  min_lr: 0.000001  loss: 1.2480 (1.2046)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0959 (2.9994)  time: 0.2890  data: 0.0003  max mem: 14662
Epoch: [21]  [40/64]  eta: 0:00:08  lr: 0.000403  min_lr: 0.000001  loss: 1.1445 (1.1771)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1848 (3.0715)  time: 0.2894  data: 0.0003  max mem: 14662
Epoch: [21]  [50/64]  eta: 0:00:04  lr: 0.000401  min_lr: 0.000001  loss: 1.1445 (1.1678)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1622 (3.0726)  time: 0.2912  data: 0.0003  max mem: 14662
Epoch: [21]  [60/64]  eta: 0:00:01  lr: 0.000398  min_lr: 0.000001  loss: 1.1775 (1.1739)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8960 (3.0713)  time: 0.2916  data: 0.0002  max mem: 14662
Epoch: [21]  [63/64]  eta: 0:00:00  lr: 0.000397  min_lr: 0.000001  loss: 1.1871 (1.1713)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8776 (3.0472)  time: 0.2908  data: 0.0002  max mem: 14662
Epoch: [21] Total time: 0:00:21 (0.3315 s / it)
2025-04-28 16:35:06 Averaged stats: lr: 0.000397  min_lr: 0.000001  loss: 1.1871 (1.1713)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8776 (3.0472)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:08    time: 2.7173  data: 2.5736  max mem: 14662
Test:  [2/3]  eta: 0:00:01    time: 1.0060  data: 0.8580  max mem: 14662
Test: Total time: 0:00:03 (1.0337 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8121 Acc: 0.9096 Recall_macro: 0.8121 Recall_weighted: 0.9096 AUC-ROC: 0.9825 Weighted F1-score: 0.9180
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 21, 'Val Loss': 0.36929112672805786, 'Val BAcc': np.float64(0.8121413924421443), 'Val Acc': 0.9095652173913044, 'Val ROC': np.float64(0.9824939359901715), 'Val W_F1': 0.9180432259473873, 'Val Recall_macro': 0.8121413924421443, 'Val Recall_weighted': 0.9095652173913044}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [22]  [ 0/64]  eta: 0:03:01  lr: 0.000397  min_lr: 0.000001  loss: 1.1079 (1.1079)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4551 (2.4551)  time: 2.8416  data: 2.5561  max mem: 14662
Epoch: [22]  [10/64]  eta: 0:00:28  lr: 0.000395  min_lr: 0.000001  loss: 1.1614 (1.0645)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7078 (2.7717)  time: 0.5216  data: 0.2327  max mem: 14662
Epoch: [22]  [20/64]  eta: 0:00:18  lr: 0.000392  min_lr: 0.000001  loss: 1.1206 (1.0630)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8716 (2.9848)  time: 0.2898  data: 0.0004  max mem: 14662
Epoch: [22]  [30/64]  eta: 0:00:12  lr: 0.000390  min_lr: 0.000001  loss: 1.1206 (1.0779)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9795 (2.9784)  time: 0.2902  data: 0.0003  max mem: 14662
Epoch: [22]  [40/64]  eta: 0:00:08  lr: 0.000387  min_lr: 0.000001  loss: 1.1962 (1.0942)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8976 (2.9765)  time: 0.2907  data: 0.0003  max mem: 14662
Epoch: [22]  [50/64]  eta: 0:00:04  lr: 0.000384  min_lr: 0.000001  loss: 1.2154 (1.1022)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8383 (2.9257)  time: 0.2907  data: 0.0003  max mem: 14662
Epoch: [22]  [60/64]  eta: 0:00:01  lr: 0.000382  min_lr: 0.000001  loss: 1.1960 (1.1103)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7017 (2.9144)  time: 0.2905  data: 0.0001  max mem: 14662
Epoch: [22]  [63/64]  eta: 0:00:00  lr: 0.000381  min_lr: 0.000001  loss: 1.2178 (1.1097)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8176 (2.9150)  time: 0.2908  data: 0.0001  max mem: 14662
Epoch: [22] Total time: 0:00:21 (0.3318 s / it)
2025-04-28 16:35:31 Averaged stats: lr: 0.000381  min_lr: 0.000001  loss: 1.2178 (1.1097)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8176 (2.9150)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5797  data: 2.4365  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9592  data: 0.8123  max mem: 14662
Test: Total time: 0:00:02 (0.9819 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8301 Acc: 0.9235 Recall_macro: 0.8301 Recall_weighted: 0.9235 AUC-ROC: 0.9785 Weighted F1-score: 0.9285
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 22, 'Val Loss': 0.35759666562080383, 'Val BAcc': np.float64(0.8301101237943344), 'Val Acc': 0.9234782608695652, 'Val ROC': np.float64(0.9785368517156229), 'Val W_F1': 0.9284816887374431, 'Val Recall_macro': 0.8301101237943344, 'Val Recall_weighted': 0.9234782608695652}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [23]  [ 0/64]  eta: 0:03:03  lr: 0.000381  min_lr: 0.000001  loss: 1.1351 (1.1351)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5094 (2.5094)  time: 2.8672  data: 2.5794  max mem: 14662
Epoch: [23]  [10/64]  eta: 0:00:28  lr: 0.000378  min_lr: 0.000001  loss: 0.8804 (0.9138)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9613 (2.8696)  time: 0.5233  data: 0.2348  max mem: 14662
Epoch: [23]  [20/64]  eta: 0:00:18  lr: 0.000376  min_lr: 0.000001  loss: 0.8804 (0.9569)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9841 (3.0293)  time: 0.2893  data: 0.0003  max mem: 14662
Epoch: [23]  [30/64]  eta: 0:00:12  lr: 0.000373  min_lr: 0.000001  loss: 1.0430 (0.9917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8090 (2.9576)  time: 0.2902  data: 0.0003  max mem: 14662
Epoch: [23]  [40/64]  eta: 0:00:08  lr: 0.000370  min_lr: 0.000001  loss: 1.1379 (1.0371)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6604 (2.9941)  time: 0.2907  data: 0.0003  max mem: 14662
Epoch: [23]  [50/64]  eta: 0:00:04  lr: 0.000368  min_lr: 0.000001  loss: 1.1480 (1.0568)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7339 (2.9726)  time: 0.2910  data: 0.0003  max mem: 14662
Epoch: [23]  [60/64]  eta: 0:00:01  lr: 0.000365  min_lr: 0.000001  loss: 1.0940 (1.0512)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7149 (2.9210)  time: 0.2908  data: 0.0001  max mem: 14662
Epoch: [23]  [63/64]  eta: 0:00:00  lr: 0.000364  min_lr: 0.000001  loss: 1.0940 (1.0585)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7149 (2.9361)  time: 0.2906  data: 0.0001  max mem: 14662
Epoch: [23] Total time: 0:00:21 (0.3321 s / it)
2025-04-28 16:35:55 Averaged stats: lr: 0.000364  min_lr: 0.000001  loss: 1.0940 (1.0585)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7149 (2.9361)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5118  data: 2.3668  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9500  data: 0.8073  max mem: 14662
Test: Total time: 0:00:02 (0.9738 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8328 Acc: 0.8817 Recall_macro: 0.8328 Recall_weighted: 0.8817 AUC-ROC: 0.9796 Weighted F1-score: 0.8968
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 23, 'Val Loss': 0.3866586685180664, 'Val BAcc': np.float64(0.8327763130770648), 'Val Acc': 0.8817391304347826, 'Val ROC': np.float64(0.9795687327642002), 'Val W_F1': 0.8967736707683329, 'Val Recall_macro': 0.8327763130770648, 'Val Recall_weighted': 0.8817391304347826}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [24]  [ 0/64]  eta: 0:03:01  lr: 0.000364  min_lr: 0.000001  loss: 1.0286 (1.0286)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4479 (2.4479)  time: 2.8335  data: 2.5435  max mem: 14662
Epoch: [24]  [10/64]  eta: 0:00:28  lr: 0.000361  min_lr: 0.000001  loss: 1.0286 (1.0116)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7146 (2.7271)  time: 0.5218  data: 0.2316  max mem: 14662
Epoch: [24]  [20/64]  eta: 0:00:18  lr: 0.000358  min_lr: 0.000001  loss: 1.0844 (1.0557)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7613 (2.8467)  time: 0.2908  data: 0.0004  max mem: 14662
Epoch: [24]  [30/64]  eta: 0:00:12  lr: 0.000356  min_lr: 0.000001  loss: 1.0470 (1.0089)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7613 (2.8171)  time: 0.2908  data: 0.0004  max mem: 14662
Epoch: [24]  [40/64]  eta: 0:00:08  lr: 0.000353  min_lr: 0.000001  loss: 0.9640 (1.0107)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6340 (2.8322)  time: 0.2909  data: 0.0003  max mem: 14662
Epoch: [24]  [50/64]  eta: 0:00:04  lr: 0.000350  min_lr: 0.000001  loss: 1.0977 (1.0248)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6769 (2.8390)  time: 0.2914  data: 0.0002  max mem: 14662
Epoch: [24]  [60/64]  eta: 0:00:01  lr: 0.000347  min_lr: 0.000001  loss: 1.1602 (1.0477)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9469 (2.8988)  time: 0.2914  data: 0.0001  max mem: 14662
Epoch: [24]  [63/64]  eta: 0:00:00  lr: 0.000346  min_lr: 0.000001  loss: 1.1691 (1.0576)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0027 (2.9324)  time: 0.2914  data: 0.0001  max mem: 14662
Epoch: [24] Total time: 0:00:21 (0.3323 s / it)
2025-04-28 16:36:19 Averaged stats: lr: 0.000346  min_lr: 0.000001  loss: 1.1691 (1.0576)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0027 (2.9324)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5197  data: 2.3761  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9382  data: 0.7922  max mem: 14662
Test: Total time: 0:00:02 (0.9658 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8309 Acc: 0.8887 Recall_macro: 0.8309 Recall_weighted: 0.8887 AUC-ROC: 0.9775 Weighted F1-score: 0.9058
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 24, 'Val Loss': 0.408131867647171, 'Val BAcc': np.float64(0.8308988922522758), 'Val Acc': 0.888695652173913, 'Val ROC': np.float64(0.9775339653450619), 'Val W_F1': 0.9057838128522107, 'Val Recall_macro': 0.8308988922522758, 'Val Recall_weighted': 0.888695652173913}
Max val mean recall: 0.85%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [25]  [ 0/64]  eta: 0:02:55  lr: 0.000346  min_lr: 0.000001  loss: 1.3283 (1.3283)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2294 (3.2294)  time: 2.7436  data: 2.4577  max mem: 14662
Epoch: [25]  [10/64]  eta: 0:00:27  lr: 0.000343  min_lr: 0.000001  loss: 1.1633 (1.1491)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8346 (3.0237)  time: 0.5120  data: 0.2237  max mem: 14662
Epoch: [25]  [20/64]  eta: 0:00:17  lr: 0.000340  min_lr: 0.000001  loss: 1.0974 (1.1109)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8346 (2.9533)  time: 0.2895  data: 0.0003  max mem: 14662
Epoch: [25]  [30/64]  eta: 0:00:12  lr: 0.000337  min_lr: 0.000001  loss: 1.1212 (1.1215)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0020 (3.0056)  time: 0.2903  data: 0.0003  max mem: 14662
Epoch: [25]  [40/64]  eta: 0:00:08  lr: 0.000335  min_lr: 0.000001  loss: 1.1397 (1.1141)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9576 (3.0364)  time: 0.2906  data: 0.0003  max mem: 14662
Epoch: [25]  [50/64]  eta: 0:00:04  lr: 0.000332  min_lr: 0.000001  loss: 1.0397 (1.0663)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8662 (3.0363)  time: 0.2910  data: 0.0002  max mem: 14662
Epoch: [25]  [60/64]  eta: 0:00:01  lr: 0.000329  min_lr: 0.000001  loss: 0.8612 (1.0425)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0719 (3.0864)  time: 0.2910  data: 0.0001  max mem: 14662
Epoch: [25]  [63/64]  eta: 0:00:00  lr: 0.000328  min_lr: 0.000001  loss: 0.9046 (1.0376)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0719 (3.0487)  time: 0.2908  data: 0.0001  max mem: 14662
Epoch: [25] Total time: 0:00:21 (0.3301 s / it)
2025-04-28 16:36:43 Averaged stats: lr: 0.000328  min_lr: 0.000001  loss: 0.9046 (1.0376)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0719 (3.0487)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3922  data: 2.2477  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8957  data: 0.7497  max mem: 14662
Test: Total time: 0:00:02 (0.9189 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8637 Acc: 0.9078 Recall_macro: 0.8637 Recall_weighted: 0.9078 AUC-ROC: 0.9735 Weighted F1-score: 0.9179
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 25, 'Val Loss': 0.362491637468338, 'Val BAcc': np.float64(0.8637335763651554), 'Val Acc': 0.9078260869565218, 'Val ROC': np.float64(0.9735300630727979), 'Val W_F1': 0.9178517874626371, 'Val Recall_macro': 0.8637335763651554, 'Val Recall_weighted': 0.9078260869565218}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [26]  [ 0/64]  eta: 0:03:01  lr: 0.000328  min_lr: 0.000001  loss: 1.1013 (1.1013)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3929 (2.3929)  time: 2.8356  data: 2.5509  max mem: 14662
Epoch: [26]  [10/64]  eta: 0:00:28  lr: 0.000325  min_lr: 0.000001  loss: 1.1277 (1.0788)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7971 (2.7467)  time: 0.5220  data: 0.2323  max mem: 14662
Epoch: [26]  [20/64]  eta: 0:00:18  lr: 0.000322  min_lr: 0.000001  loss: 1.1277 (1.0667)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7971 (2.6991)  time: 0.2901  data: 0.0004  max mem: 14662
Epoch: [26]  [30/64]  eta: 0:00:12  lr: 0.000319  min_lr: 0.000001  loss: 1.1612 (1.0957)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8714 (2.7321)  time: 0.2903  data: 0.0004  max mem: 14662
Epoch: [26]  [40/64]  eta: 0:00:08  lr: 0.000316  min_lr: 0.000001  loss: 1.2264 (1.1220)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9534 (2.8205)  time: 0.2909  data: 0.0004  max mem: 14662
Epoch: [26]  [50/64]  eta: 0:00:04  lr: 0.000313  min_lr: 0.000001  loss: 1.0710 (1.0936)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9534 (2.8398)  time: 0.2901  data: 0.0003  max mem: 14662
Epoch: [26]  [60/64]  eta: 0:00:01  lr: 0.000310  min_lr: 0.000001  loss: 1.0271 (1.0910)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8704 (2.8799)  time: 0.2890  data: 0.0002  max mem: 14662
Epoch: [26]  [63/64]  eta: 0:00:00  lr: 0.000309  min_lr: 0.000001  loss: 1.0271 (1.0901)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8704 (2.8795)  time: 0.2890  data: 0.0001  max mem: 14662
Epoch: [26] Total time: 0:00:21 (0.3312 s / it)
2025-04-28 16:37:09 Averaged stats: lr: 0.000309  min_lr: 0.000001  loss: 1.0271 (1.0901)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8704 (2.8795)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.6143  data: 2.4714  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9699  data: 0.8239  max mem: 14662
Test: Total time: 0:00:02 (0.9929 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8219 Acc: 0.8817 Recall_macro: 0.8219 Recall_weighted: 0.8817 AUC-ROC: 0.9629 Weighted F1-score: 0.8979
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 26, 'Val Loss': 0.4501601755619049, 'Val BAcc': np.float64(0.8218824116568477), 'Val Acc': 0.8817391304347826, 'Val ROC': np.float64(0.9628731275677199), 'Val W_F1': 0.8979432496366461, 'Val Recall_macro': 0.8218824116568477, 'Val Recall_weighted': 0.8817391304347826}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [27]  [ 0/64]  eta: 0:03:12  lr: 0.000309  min_lr: 0.000001  loss: 1.3069 (1.3069)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3284 (3.3284)  time: 3.0044  data: 2.7213  max mem: 14662
Epoch: [27]  [10/64]  eta: 0:00:28  lr: 0.000306  min_lr: 0.000001  loss: 1.1748 (1.0745)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7296 (2.7995)  time: 0.5366  data: 0.2477  max mem: 14662
Epoch: [27]  [20/64]  eta: 0:00:18  lr: 0.000303  min_lr: 0.000001  loss: 1.0370 (1.0610)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8029 (3.1022)  time: 0.2899  data: 0.0004  max mem: 14662
Epoch: [27]  [30/64]  eta: 0:00:12  lr: 0.000300  min_lr: 0.000001  loss: 0.9140 (1.0255)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0769 (3.1283)  time: 0.2899  data: 0.0004  max mem: 14662
Epoch: [27]  [40/64]  eta: 0:00:08  lr: 0.000297  min_lr: 0.000001  loss: 1.1173 (1.0637)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9243 (3.1439)  time: 0.2903  data: 0.0003  max mem: 14662
Epoch: [27]  [50/64]  eta: 0:00:04  lr: 0.000294  min_lr: 0.000001  loss: 1.1833 (1.0622)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7432 (3.0422)  time: 0.2905  data: 0.0002  max mem: 14662
Epoch: [27]  [60/64]  eta: 0:00:01  lr: 0.000291  min_lr: 0.000001  loss: 1.0772 (1.0552)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7429 (3.0072)  time: 0.2908  data: 0.0002  max mem: 14662
Epoch: [27]  [63/64]  eta: 0:00:00  lr: 0.000290  min_lr: 0.000001  loss: 1.1167 (1.0585)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7429 (3.0056)  time: 0.2909  data: 0.0001  max mem: 14662
Epoch: [27] Total time: 0:00:21 (0.3343 s / it)
2025-04-28 16:37:34 Averaged stats: lr: 0.000290  min_lr: 0.000001  loss: 1.1167 (1.0585)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7429 (3.0056)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.6105  data: 2.4667  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9676  data: 0.8224  max mem: 14662
Test: Total time: 0:00:02 (0.9962 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8346 Acc: 0.8887 Recall_macro: 0.8346 Recall_weighted: 0.8887 AUC-ROC: 0.9759 Weighted F1-score: 0.9038
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 27, 'Val Loss': 0.3934757709503174, 'Val BAcc': np.float64(0.8345986177565126), 'Val Acc': 0.888695652173913, 'Val ROC': np.float64(0.9759361343070478), 'Val W_F1': 0.9038161290073161, 'Val Recall_macro': 0.8345986177565126, 'Val Recall_weighted': 0.888695652173913}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [28]  [ 0/64]  eta: 0:02:50  lr: 0.000290  min_lr: 0.000001  loss: 1.2806 (1.2806)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7670 (2.7670)  time: 2.6675  data: 2.3812  max mem: 14662
Epoch: [28]  [10/64]  eta: 0:00:27  lr: 0.000287  min_lr: 0.000001  loss: 1.1785 (1.1414)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5841 (2.6102)  time: 0.5054  data: 0.2168  max mem: 14662
Epoch: [28]  [20/64]  eta: 0:00:17  lr: 0.000283  min_lr: 0.000001  loss: 1.0811 (1.0849)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5841 (2.6873)  time: 0.2897  data: 0.0004  max mem: 14662
Epoch: [28]  [30/64]  eta: 0:00:12  lr: 0.000280  min_lr: 0.000001  loss: 1.0510 (1.0725)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8622 (2.7329)  time: 0.2904  data: 0.0004  max mem: 14662
Epoch: [28]  [40/64]  eta: 0:00:08  lr: 0.000277  min_lr: 0.000001  loss: 1.1564 (1.0982)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9886 (2.8386)  time: 0.2903  data: 0.0003  max mem: 14662
Epoch: [28]  [50/64]  eta: 0:00:04  lr: 0.000274  min_lr: 0.000001  loss: 1.0822 (1.0745)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9391 (2.9092)  time: 0.2902  data: 0.0002  max mem: 14662
Epoch: [28]  [60/64]  eta: 0:00:01  lr: 0.000271  min_lr: 0.000001  loss: 0.9857 (1.0627)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9129 (2.9527)  time: 0.2900  data: 0.0002  max mem: 14662
Epoch: [28]  [63/64]  eta: 0:00:00  lr: 0.000270  min_lr: 0.000001  loss: 0.9720 (1.0573)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9129 (2.9723)  time: 0.2899  data: 0.0001  max mem: 14662
Epoch: [28] Total time: 0:00:21 (0.3286 s / it)
2025-04-28 16:37:58 Averaged stats: lr: 0.000270  min_lr: 0.000001  loss: 0.9720 (1.0573)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9129 (2.9723)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:06    time: 2.2876  data: 2.1437  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8622  data: 0.7147  max mem: 14662
Test: Total time: 0:00:02 (0.8851 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8365 Acc: 0.8765 Recall_macro: 0.8365 Recall_weighted: 0.8765 AUC-ROC: 0.9764 Weighted F1-score: 0.8995
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 28, 'Val Loss': 0.42455416917800903, 'Val BAcc': np.float64(0.8364855862600223), 'Val Acc': 0.8765217391304347, 'Val ROC': np.float64(0.9764000473328176), 'Val W_F1': 0.8995127179019905, 'Val Recall_macro': 0.8364855862600223, 'Val Recall_weighted': 0.8765217391304347}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [29]  [ 0/64]  eta: 0:03:04  lr: 0.000270  min_lr: 0.000001  loss: 1.0311 (1.0311)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2597 (2.2597)  time: 2.8800  data: 2.5931  max mem: 14662
Epoch: [29]  [10/64]  eta: 0:00:28  lr: 0.000267  min_lr: 0.000001  loss: 1.0472 (1.0039)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7408 (2.7261)  time: 0.5247  data: 0.2360  max mem: 14662
Epoch: [29]  [20/64]  eta: 0:00:18  lr: 0.000264  min_lr: 0.000001  loss: 1.0702 (1.0749)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8209 (2.8763)  time: 0.2901  data: 0.0003  max mem: 14662
Epoch: [29]  [30/64]  eta: 0:00:12  lr: 0.000261  min_lr: 0.000001  loss: 1.1364 (1.0805)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9826 (2.8666)  time: 0.2906  data: 0.0004  max mem: 14662
Epoch: [29]  [40/64]  eta: 0:00:08  lr: 0.000258  min_lr: 0.000001  loss: 1.0499 (1.0471)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9568 (2.9005)  time: 0.2913  data: 0.0004  max mem: 14662
Epoch: [29]  [50/64]  eta: 0:00:04  lr: 0.000255  min_lr: 0.000001  loss: 1.1167 (1.0601)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8963 (2.8918)  time: 0.2921  data: 0.0003  max mem: 14662
Epoch: [29]  [60/64]  eta: 0:00:01  lr: 0.000252  min_lr: 0.000001  loss: 1.0034 (1.0344)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4607 (2.8129)  time: 0.2911  data: 0.0002  max mem: 14662
Epoch: [29]  [63/64]  eta: 0:00:00  lr: 0.000251  min_lr: 0.000001  loss: 1.0033 (1.0288)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4859 (2.8124)  time: 0.2907  data: 0.0001  max mem: 14662
Epoch: [29] Total time: 0:00:21 (0.3329 s / it)
2025-04-28 16:38:22 Averaged stats: lr: 0.000251  min_lr: 0.000001  loss: 1.0033 (1.0288)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4859 (2.8124)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5146  data: 2.3735  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9369  data: 0.7913  max mem: 14662
Test: Total time: 0:00:02 (0.9595 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8467 Acc: 0.9078 Recall_macro: 0.8467 Recall_weighted: 0.9078 AUC-ROC: 0.9751 Weighted F1-score: 0.9177
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 29, 'Val Loss': 0.3583623170852661, 'Val BAcc': np.float64(0.8467350193665982), 'Val Acc': 0.9078260869565218, 'Val ROC': np.float64(0.9750647101048642), 'Val W_F1': 0.917715847110523, 'Val Recall_macro': 0.8467350193665982, 'Val Recall_weighted': 0.9078260869565218}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [30]  [ 0/64]  eta: 0:02:58  lr: 0.000251  min_lr: 0.000001  loss: 0.8906 (0.8906)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9336 (1.9336)  time: 2.7960  data: 2.5119  max mem: 14662
Epoch: [30]  [10/64]  eta: 0:00:27  lr: 0.000247  min_lr: 0.000001  loss: 0.9461 (1.0001)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6217 (2.6377)  time: 0.5178  data: 0.2287  max mem: 14662
Epoch: [30]  [20/64]  eta: 0:00:18  lr: 0.000244  min_lr: 0.000001  loss: 1.0362 (0.9969)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6366 (2.6990)  time: 0.2904  data: 0.0004  max mem: 14662
Epoch: [30]  [30/64]  eta: 0:00:12  lr: 0.000241  min_lr: 0.000001  loss: 1.0496 (0.9958)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0681 (2.8902)  time: 0.2910  data: 0.0004  max mem: 14662
Epoch: [30]  [40/64]  eta: 0:00:08  lr: 0.000238  min_lr: 0.000001  loss: 1.0999 (1.0001)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3053 (2.9749)  time: 0.2918  data: 0.0003  max mem: 14662
Epoch: [30]  [50/64]  eta: 0:00:04  lr: 0.000235  min_lr: 0.000001  loss: 1.0633 (0.9848)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0086 (2.9423)  time: 0.2921  data: 0.0003  max mem: 14662
Epoch: [30]  [60/64]  eta: 0:00:01  lr: 0.000232  min_lr: 0.000001  loss: 0.9633 (0.9877)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9189 (2.9667)  time: 0.2913  data: 0.0002  max mem: 14662
Epoch: [30]  [63/64]  eta: 0:00:00  lr: 0.000231  min_lr: 0.000001  loss: 0.9633 (0.9805)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8913 (2.9648)  time: 0.2911  data: 0.0001  max mem: 14662
Epoch: [30] Total time: 0:00:21 (0.3318 s / it)
2025-04-28 16:38:46 Averaged stats: lr: 0.000231  min_lr: 0.000001  loss: 0.9633 (0.9805)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8913 (2.9648)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4719  data: 2.3290  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9238  data: 0.7764  max mem: 14662
Test: Total time: 0:00:02 (0.9472 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8398 Acc: 0.9061 Recall_macro: 0.8398 Recall_weighted: 0.9061 AUC-ROC: 0.9728 Weighted F1-score: 0.9176
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 30, 'Val Loss': 0.39819398522377014, 'Val BAcc': np.float64(0.8397604400611919), 'Val Acc': 0.9060869565217391, 'Val ROC': np.float64(0.9728487053572513), 'Val W_F1': 0.9176085674361386, 'Val Recall_macro': 0.8397604400611919, 'Val Recall_weighted': 0.9060869565217391}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [31]  [ 0/64]  eta: 0:02:56  lr: 0.000231  min_lr: 0.000001  loss: 1.3241 (1.3241)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3531 (4.3531)  time: 2.7581  data: 2.4718  max mem: 14662
Epoch: [31]  [10/64]  eta: 0:00:27  lr: 0.000228  min_lr: 0.000001  loss: 1.0218 (1.0208)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0921 (3.2201)  time: 0.5142  data: 0.2252  max mem: 14662
Epoch: [31]  [20/64]  eta: 0:00:17  lr: 0.000225  min_lr: 0.000001  loss: 1.0952 (1.0718)  loss_scale: 65536.0000 (84260.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0571 (3.0817)  time: 0.2905  data: 0.0005  max mem: 14662
Epoch: [31]  [30/64]  eta: 0:00:12  lr: 0.000222  min_lr: 0.000001  loss: 1.1335 (1.0743)  loss_scale: 131072.0000 (99361.0323)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7709 (2.9102)  time: 0.2912  data: 0.0004  max mem: 14662
Epoch: [31]  [40/64]  eta: 0:00:08  lr: 0.000219  min_lr: 0.000001  loss: 1.1326 (1.0675)  loss_scale: 131072.0000 (107095.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7709 (2.8578)  time: 0.2911  data: 0.0003  max mem: 14662
Epoch: [31]  [50/64]  eta: 0:00:04  lr: 0.000216  min_lr: 0.000001  loss: 1.1326 (1.0689)  loss_scale: 131072.0000 (111796.7059)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8426 (2.8415)  time: 0.2915  data: 0.0002  max mem: 14662
Epoch: [31]  [60/64]  eta: 0:00:01  lr: 0.000213  min_lr: 0.000001  loss: 1.1105 (1.0775)  loss_scale: 131072.0000 (114956.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8460 (2.8618)  time: 0.2913  data: 0.0001  max mem: 14662
Epoch: [31]  [63/64]  eta: 0:00:00  lr: 0.000212  min_lr: 0.000001  loss: 1.1105 (1.0755)  loss_scale: 131072.0000 (115712.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8460 (2.8441)  time: 0.2916  data: 0.0001  max mem: 14662
Epoch: [31] Total time: 0:00:21 (0.3309 s / it)
2025-04-28 16:39:10 Averaged stats: lr: 0.000212  min_lr: 0.000001  loss: 1.1105 (1.0755)  loss_scale: 131072.0000 (115712.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8460 (2.8441)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3777  data: 2.2350  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8903  data: 0.7451  max mem: 14662
Test: Total time: 0:00:02 (0.9120 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8253 Acc: 0.8696 Recall_macro: 0.8253 Recall_weighted: 0.8696 AUC-ROC: 0.9708 Weighted F1-score: 0.8906
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 31, 'Val Loss': 0.4409758150577545, 'Val BAcc': np.float64(0.8253482190324296), 'Val Acc': 0.8695652173913043, 'Val ROC': np.float64(0.9708287131412641), 'Val W_F1': 0.8905950484631395, 'Val Recall_macro': 0.8253482190324296, 'Val Recall_weighted': 0.8695652173913043}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [32]  [ 0/64]  eta: 0:02:55  lr: 0.000211  min_lr: 0.000001  loss: 0.8947 (0.8947)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6822 (2.6822)  time: 2.7397  data: 2.4487  max mem: 14662
Epoch: [32]  [10/64]  eta: 0:00:27  lr: 0.000208  min_lr: 0.000001  loss: 0.9775 (1.0001)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5446 (2.5295)  time: 0.5132  data: 0.2230  max mem: 14662
Epoch: [32]  [20/64]  eta: 0:00:17  lr: 0.000205  min_lr: 0.000001  loss: 0.9991 (1.0139)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5030 (2.5728)  time: 0.2908  data: 0.0004  max mem: 14662
Epoch: [32]  [30/64]  eta: 0:00:12  lr: 0.000202  min_lr: 0.000001  loss: 1.0454 (1.0418)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4696 (2.5686)  time: 0.2910  data: 0.0003  max mem: 14662
Epoch: [32]  [40/64]  eta: 0:00:08  lr: 0.000199  min_lr: 0.000001  loss: 1.1418 (1.0475)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6383 (2.6365)  time: 0.2909  data: 0.0003  max mem: 14662
Epoch: [32]  [50/64]  eta: 0:00:04  lr: 0.000196  min_lr: 0.000001  loss: 1.0659 (1.0485)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1773 (2.7321)  time: 0.2911  data: 0.0002  max mem: 14662
Epoch: [32]  [60/64]  eta: 0:00:01  lr: 0.000193  min_lr: 0.000001  loss: 1.0456 (1.0444)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7771 (2.6976)  time: 0.2911  data: 0.0001  max mem: 14662
Epoch: [32]  [63/64]  eta: 0:00:00  lr: 0.000193  min_lr: 0.000001  loss: 1.0410 (1.0394)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7227 (2.6905)  time: 0.2909  data: 0.0001  max mem: 14662
Epoch: [32] Total time: 0:00:21 (0.3305 s / it)
2025-04-28 16:39:34 Averaged stats: lr: 0.000193  min_lr: 0.000001  loss: 1.0410 (1.0394)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7227 (2.6905)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3670  data: 2.2224  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8886  data: 0.7409  max mem: 14662
Test: Total time: 0:00:02 (0.9137 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8037 Acc: 0.8748 Recall_macro: 0.8037 Recall_weighted: 0.8748 AUC-ROC: 0.9737 Weighted F1-score: 0.8952
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 32, 'Val Loss': 0.4060780704021454, 'Val BAcc': np.float64(0.8036702144972822), 'Val Acc': 0.8747826086956522, 'Val ROC': np.float64(0.9736775872296345), 'Val W_F1': 0.8951631659819766, 'Val Recall_macro': 0.8036702144972822, 'Val Recall_weighted': 0.8747826086956522}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [33]  [ 0/64]  eta: 0:03:04  lr: 0.000192  min_lr: 0.000001  loss: 0.7092 (0.7092)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0363 (3.0363)  time: 2.8808  data: 2.5980  max mem: 14662
Epoch: [33]  [10/64]  eta: 0:00:28  lr: 0.000189  min_lr: 0.000001  loss: 1.0492 (1.0079)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7036 (2.7032)  time: 0.5251  data: 0.2365  max mem: 14662
Epoch: [33]  [20/64]  eta: 0:00:18  lr: 0.000186  min_lr: 0.000001  loss: 1.1100 (1.0608)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7276 (2.6654)  time: 0.2905  data: 0.0004  max mem: 14662
Epoch: [33]  [30/64]  eta: 0:00:12  lr: 0.000183  min_lr: 0.000001  loss: 1.1712 (1.0731)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8197 (2.7965)  time: 0.2912  data: 0.0004  max mem: 14662
Epoch: [33]  [40/64]  eta: 0:00:08  lr: 0.000180  min_lr: 0.000001  loss: 1.1282 (1.0613)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9443 (2.8065)  time: 0.2916  data: 0.0003  max mem: 14662
Epoch: [33]  [50/64]  eta: 0:00:04  lr: 0.000177  min_lr: 0.000001  loss: 1.0540 (1.0639)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0509 (2.8578)  time: 0.2918  data: 0.0002  max mem: 14662
Epoch: [33]  [60/64]  eta: 0:00:01  lr: 0.000175  min_lr: 0.000001  loss: 1.0577 (1.0614)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9663 (2.8896)  time: 0.2913  data: 0.0001  max mem: 14662
Epoch: [33]  [63/64]  eta: 0:00:00  lr: 0.000174  min_lr: 0.000001  loss: 1.0577 (1.0621)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9663 (2.9029)  time: 0.2914  data: 0.0001  max mem: 14662
Epoch: [33] Total time: 0:00:21 (0.3331 s / it)
2025-04-28 16:39:58 Averaged stats: lr: 0.000174  min_lr: 0.000001  loss: 1.0577 (1.0621)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9663 (2.9029)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3864  data: 2.2431  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8939  data: 0.7478  max mem: 14662
Test: Total time: 0:00:02 (0.9177 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8038 Acc: 0.8957 Recall_macro: 0.8038 Recall_weighted: 0.8957 AUC-ROC: 0.9734 Weighted F1-score: 0.9091
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 33, 'Val Loss': 0.3803691864013672, 'Val BAcc': np.float64(0.8038433746704424), 'Val Acc': 0.8956521739130435, 'Val ROC': np.float64(0.973359410703557), 'Val W_F1': 0.9090517274630808, 'Val Recall_macro': 0.8038433746704424, 'Val Recall_weighted': 0.8956521739130435}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [34]  [ 0/64]  eta: 0:02:59  lr: 0.000173  min_lr: 0.000001  loss: 1.0475 (1.0475)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3664 (2.3664)  time: 2.7989  data: 2.5150  max mem: 14662
Epoch: [34]  [10/64]  eta: 0:00:27  lr: 0.000170  min_lr: 0.000001  loss: 1.0752 (1.0545)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8878 (2.9866)  time: 0.5183  data: 0.2289  max mem: 14662
Epoch: [34]  [20/64]  eta: 0:00:18  lr: 0.000168  min_lr: 0.000001  loss: 1.0752 (1.0428)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8878 (2.8663)  time: 0.2899  data: 0.0003  max mem: 14662
Epoch: [34]  [30/64]  eta: 0:00:12  lr: 0.000165  min_lr: 0.000001  loss: 1.0515 (1.0272)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7786 (2.9345)  time: 0.2899  data: 0.0003  max mem: 14662
Epoch: [34]  [40/64]  eta: 0:00:08  lr: 0.000162  min_lr: 0.000001  loss: 1.0632 (1.0314)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8104 (2.9669)  time: 0.2907  data: 0.0004  max mem: 14662
Epoch: [34]  [50/64]  eta: 0:00:04  lr: 0.000159  min_lr: 0.000001  loss: 0.9714 (1.0044)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8104 (2.9194)  time: 0.2914  data: 0.0003  max mem: 14662
Epoch: [34]  [60/64]  eta: 0:00:01  lr: 0.000156  min_lr: 0.000001  loss: 0.9667 (0.9943)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4477 (2.8422)  time: 0.2915  data: 0.0002  max mem: 14662
Epoch: [34]  [63/64]  eta: 0:00:00  lr: 0.000155  min_lr: 0.000001  loss: 0.9667 (0.9946)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4894 (2.8190)  time: 0.2914  data: 0.0001  max mem: 14662
Epoch: [34] Total time: 0:00:21 (0.3314 s / it)
2025-04-28 16:40:22 Averaged stats: lr: 0.000155  min_lr: 0.000001  loss: 0.9667 (0.9946)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4894 (2.8190)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5158  data: 2.3715  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9360  data: 0.7906  max mem: 14662
Test: Total time: 0:00:02 (0.9605 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7902 Acc: 0.9026 Recall_macro: 0.7902 Recall_weighted: 0.9026 AUC-ROC: 0.9765 Weighted F1-score: 0.9131
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 34, 'Val Loss': 0.3443344533443451, 'Val BAcc': np.float64(0.7902366305373824), 'Val Acc': 0.9026086956521739, 'Val ROC': np.float64(0.9765333124460719), 'Val W_F1': 0.9130663527282609, 'Val Recall_macro': 0.7902366305373824, 'Val Recall_weighted': 0.9026086956521739}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [35]  [ 0/64]  eta: 0:02:58  lr: 0.000155  min_lr: 0.000001  loss: 1.0215 (1.0215)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9484 (2.9484)  time: 2.7832  data: 2.4979  max mem: 14662
Epoch: [35]  [10/64]  eta: 0:00:27  lr: 0.000152  min_lr: 0.000001  loss: 0.8821 (0.9537)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8911 (2.8264)  time: 0.5159  data: 0.2274  max mem: 14662
Epoch: [35]  [20/64]  eta: 0:00:17  lr: 0.000149  min_lr: 0.000001  loss: 0.8564 (0.9800)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8551 (2.8208)  time: 0.2903  data: 0.0003  max mem: 14662
Epoch: [35]  [30/64]  eta: 0:00:12  lr: 0.000147  min_lr: 0.000001  loss: 0.9814 (0.9994)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6291 (2.7463)  time: 0.2911  data: 0.0003  max mem: 14662
Epoch: [35]  [40/64]  eta: 0:00:08  lr: 0.000144  min_lr: 0.000001  loss: 1.0760 (1.0274)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6962 (2.7698)  time: 0.2912  data: 0.0003  max mem: 14662
Epoch: [35]  [50/64]  eta: 0:00:04  lr: 0.000141  min_lr: 0.000001  loss: 1.0807 (1.0270)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8241 (2.8215)  time: 0.2913  data: 0.0003  max mem: 14662
Epoch: [35]  [60/64]  eta: 0:00:01  lr: 0.000138  min_lr: 0.000001  loss: 0.9826 (1.0142)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8241 (2.8491)  time: 0.2907  data: 0.0001  max mem: 14662
Epoch: [35]  [63/64]  eta: 0:00:00  lr: 0.000138  min_lr: 0.000001  loss: 1.0456 (1.0132)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8035 (2.8301)  time: 0.2909  data: 0.0001  max mem: 14662
Epoch: [35] Total time: 0:00:21 (0.3311 s / it)
2025-04-28 16:40:46 Averaged stats: lr: 0.000138  min_lr: 0.000001  loss: 1.0456 (1.0132)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8035 (2.8301)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3780  data: 2.2333  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8929  data: 0.7484  max mem: 14662
Test: Total time: 0:00:02 (0.9171 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7976 Acc: 0.9009 Recall_macro: 0.7976 Recall_weighted: 0.9009 AUC-ROC: 0.9731 Weighted F1-score: 0.9109
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 35, 'Val Loss': 0.3475360870361328, 'Val BAcc': np.float64(0.7975944189478025), 'Val Acc': 0.9008695652173913, 'Val ROC': np.float64(0.9731036306973031), 'Val W_F1': 0.9109381922479112, 'Val Recall_macro': 0.7975944189478025, 'Val Recall_weighted': 0.9008695652173913}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [36]  [ 0/64]  eta: 0:02:50  lr: 0.000137  min_lr: 0.000001  loss: 0.6741 (0.6741)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2982 (2.2982)  time: 2.6717  data: 2.3846  max mem: 14662
Epoch: [36]  [10/64]  eta: 0:00:27  lr: 0.000135  min_lr: 0.000000  loss: 1.0240 (0.9868)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6487 (2.6735)  time: 0.5063  data: 0.2171  max mem: 14662
Epoch: [36]  [20/64]  eta: 0:00:17  lr: 0.000132  min_lr: 0.000000  loss: 1.0938 (1.0488)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6487 (2.7971)  time: 0.2909  data: 0.0004  max mem: 14662
Epoch: [36]  [30/64]  eta: 0:00:12  lr: 0.000129  min_lr: 0.000000  loss: 1.1116 (1.0551)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5718 (2.7911)  time: 0.2916  data: 0.0004  max mem: 14662
Epoch: [36]  [40/64]  eta: 0:00:08  lr: 0.000126  min_lr: 0.000000  loss: 1.0357 (1.0403)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5718 (2.7549)  time: 0.2918  data: 0.0004  max mem: 14662
Epoch: [36]  [50/64]  eta: 0:00:04  lr: 0.000124  min_lr: 0.000000  loss: 0.9699 (1.0354)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5070 (2.7048)  time: 0.2916  data: 0.0003  max mem: 14662
Epoch: [36]  [60/64]  eta: 0:00:01  lr: 0.000121  min_lr: 0.000000  loss: 0.9633 (1.0270)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3978 (2.6836)  time: 0.2906  data: 0.0001  max mem: 14662
Epoch: [36]  [63/64]  eta: 0:00:00  lr: 0.000120  min_lr: 0.000000  loss: 0.9329 (1.0216)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3662 (2.6715)  time: 0.2906  data: 0.0001  max mem: 14662
Epoch: [36] Total time: 0:00:21 (0.3297 s / it)
2025-04-28 16:41:10 Averaged stats: lr: 0.000120  min_lr: 0.000000  loss: 0.9329 (1.0216)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3662 (2.6715)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5651  data: 2.4228  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9526  data: 0.8077  max mem: 14662
Test: Total time: 0:00:02 (0.9768 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8572 Acc: 0.8922 Recall_macro: 0.8572 Recall_weighted: 0.8922 AUC-ROC: 0.9777 Weighted F1-score: 0.9077
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 36, 'Val Loss': 0.37343642115592957, 'Val BAcc': np.float64(0.8571918974926493), 'Val Acc': 0.8921739130434783, 'Val ROC': np.float64(0.9777344964069633), 'Val W_F1': 0.9076567537437101, 'Val Recall_macro': 0.8571918974926493, 'Val Recall_weighted': 0.8921739130434783}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [37]  [ 0/64]  eta: 0:02:48  lr: 0.000120  min_lr: 0.000000  loss: 0.6912 (0.6912)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1379 (2.1379)  time: 2.6329  data: 2.3478  max mem: 14662
Epoch: [37]  [10/64]  eta: 0:00:27  lr: 0.000118  min_lr: 0.000000  loss: 1.1379 (1.0092)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6599 (2.8072)  time: 0.5044  data: 0.2152  max mem: 14662
Epoch: [37]  [20/64]  eta: 0:00:17  lr: 0.000115  min_lr: 0.000000  loss: 1.1063 (1.0088)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7461 (2.9000)  time: 0.2909  data: 0.0012  max mem: 14662
Epoch: [37]  [30/64]  eta: 0:00:12  lr: 0.000112  min_lr: 0.000000  loss: 1.0031 (0.9930)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7783 (2.8884)  time: 0.2908  data: 0.0004  max mem: 14662
Epoch: [37]  [40/64]  eta: 0:00:08  lr: 0.000110  min_lr: 0.000000  loss: 1.0436 (1.0370)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7893 (2.9370)  time: 0.2916  data: 0.0004  max mem: 14662
Epoch: [37]  [50/64]  eta: 0:00:04  lr: 0.000107  min_lr: 0.000000  loss: 0.9829 (0.9987)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8008 (2.9053)  time: 0.2917  data: 0.0004  max mem: 14662
Epoch: [37]  [60/64]  eta: 0:00:01  lr: 0.000105  min_lr: 0.000000  loss: 0.9424 (1.0092)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8008 (2.9269)  time: 0.2907  data: 0.0002  max mem: 14662
Epoch: [37]  [63/64]  eta: 0:00:00  lr: 0.000104  min_lr: 0.000000  loss: 0.9438 (1.0088)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8008 (2.9182)  time: 0.2905  data: 0.0001  max mem: 14662
Epoch: [37] Total time: 0:00:21 (0.3290 s / it)
2025-04-28 16:41:34 Averaged stats: lr: 0.000104  min_lr: 0.000000  loss: 0.9438 (1.0088)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8008 (2.9182)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4533  data: 2.3112  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9168  data: 0.7705  max mem: 14662
Test: Total time: 0:00:02 (0.9361 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8204 Acc: 0.9130 Recall_macro: 0.8204 Recall_weighted: 0.9130 AUC-ROC: 0.9739 Weighted F1-score: 0.9222
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 37, 'Val Loss': 0.34491318464279175, 'Val BAcc': np.float64(0.8203953606961125), 'Val Acc': 0.9130434782608695, 'Val ROC': np.float64(0.9738663491135725), 'Val W_F1': 0.92216067906049, 'Val Recall_macro': 0.8203953606961125, 'Val Recall_weighted': 0.9130434782608695}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [38]  [ 0/64]  eta: 0:02:50  lr: 0.000104  min_lr: 0.000000  loss: 1.1172 (1.1172)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3959 (2.3959)  time: 2.6692  data: 2.3855  max mem: 14662
Epoch: [38]  [10/64]  eta: 0:00:27  lr: 0.000101  min_lr: 0.000000  loss: 1.0767 (1.0432)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7733 (2.6297)  time: 0.5069  data: 0.2172  max mem: 14662
Epoch: [38]  [20/64]  eta: 0:00:17  lr: 0.000099  min_lr: 0.000000  loss: 1.0878 (1.0949)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7733 (2.7820)  time: 0.2915  data: 0.0004  max mem: 14662
Epoch: [38]  [30/64]  eta: 0:00:12  lr: 0.000097  min_lr: 0.000000  loss: 1.0941 (1.0481)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6896 (2.7601)  time: 0.2924  data: 0.0004  max mem: 14662
Epoch: [38]  [40/64]  eta: 0:00:08  lr: 0.000094  min_lr: 0.000000  loss: 0.9020 (1.0371)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7107 (2.7919)  time: 0.2928  data: 0.0003  max mem: 14662
Epoch: [38]  [50/64]  eta: 0:00:04  lr: 0.000092  min_lr: 0.000000  loss: 1.0631 (1.0293)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6308 (2.7500)  time: 0.2923  data: 0.0003  max mem: 14662
Epoch: [38]  [60/64]  eta: 0:00:01  lr: 0.000089  min_lr: 0.000000  loss: 1.0135 (1.0399)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6890 (2.7876)  time: 0.2911  data: 0.0002  max mem: 14662
Epoch: [38]  [63/64]  eta: 0:00:00  lr: 0.000089  min_lr: 0.000000  loss: 0.9994 (1.0331)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8110 (2.7792)  time: 0.2909  data: 0.0001  max mem: 14662
Epoch: [38] Total time: 0:00:21 (0.3305 s / it)
2025-04-28 16:41:58 Averaged stats: lr: 0.000089  min_lr: 0.000000  loss: 0.9994 (1.0331)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8110 (2.7792)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3756  data: 2.2316  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8918  data: 0.7440  max mem: 14662
Test: Total time: 0:00:02 (0.9165 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8448 Acc: 0.9165 Recall_macro: 0.8448 Recall_weighted: 0.9165 AUC-ROC: 0.9667 Weighted F1-score: 0.9247
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 38, 'Val Loss': 0.36825257539749146, 'Val BAcc': np.float64(0.8447573479152427), 'Val Acc': 0.9165217391304348, 'Val ROC': np.float64(0.9667159407395095), 'Val W_F1': 0.9246997956304155, 'Val Recall_macro': 0.8447573479152427, 'Val Recall_weighted': 0.9165217391304348}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [39]  [ 0/64]  eta: 0:03:03  lr: 0.000088  min_lr: 0.000000  loss: 0.8509 (0.8509)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5433 (2.5433)  time: 2.8661  data: 2.5699  max mem: 14662
Epoch: [39]  [10/64]  eta: 0:00:28  lr: 0.000086  min_lr: 0.000000  loss: 0.8511 (0.9379)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2759 (2.5095)  time: 0.5246  data: 0.2340  max mem: 14662
Epoch: [39]  [20/64]  eta: 0:00:18  lr: 0.000084  min_lr: 0.000000  loss: 0.9862 (1.0031)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5087 (2.6759)  time: 0.2907  data: 0.0004  max mem: 14662
Epoch: [39]  [30/64]  eta: 0:00:12  lr: 0.000082  min_lr: 0.000000  loss: 1.1019 (1.0314)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9579 (2.7656)  time: 0.2907  data: 0.0004  max mem: 14662
Epoch: [39]  [40/64]  eta: 0:00:08  lr: 0.000079  min_lr: 0.000000  loss: 1.1019 (1.0322)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7340 (2.7078)  time: 0.2903  data: 0.0003  max mem: 14662
Epoch: [39]  [50/64]  eta: 0:00:04  lr: 0.000077  min_lr: 0.000000  loss: 1.0032 (1.0210)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3610 (2.6594)  time: 0.2913  data: 0.0003  max mem: 14662
Epoch: [39]  [60/64]  eta: 0:00:01  lr: 0.000075  min_lr: 0.000000  loss: 1.0728 (1.0348)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6595 (2.6954)  time: 0.2917  data: 0.0002  max mem: 14662
Epoch: [39]  [63/64]  eta: 0:00:00  lr: 0.000074  min_lr: 0.000000  loss: 1.1245 (1.0451)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7309 (2.7091)  time: 0.2916  data: 0.0001  max mem: 14662
Epoch: [39] Total time: 0:00:21 (0.3327 s / it)
2025-04-28 16:42:22 Averaged stats: lr: 0.000074  min_lr: 0.000000  loss: 1.1245 (1.0451)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7309 (2.7091)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3963  data: 2.2524  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8971  data: 0.7509  max mem: 14662
Test: Total time: 0:00:02 (0.9200 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8166 Acc: 0.8922 Recall_macro: 0.8166 Recall_weighted: 0.8922 AUC-ROC: 0.9736 Weighted F1-score: 0.9075
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 39, 'Val Loss': 0.3814997673034668, 'Val BAcc': np.float64(0.8165858368865887), 'Val Acc': 0.8921739130434783, 'Val ROC': np.float64(0.9736368361989161), 'Val W_F1': 0.907543554862221, 'Val Recall_macro': 0.8165858368865887, 'Val Recall_weighted': 0.8921739130434783}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [40]  [ 0/64]  eta: 0:02:49  lr: 0.000074  min_lr: 0.000000  loss: 1.2501 (1.2501)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5055 (2.5055)  time: 2.6528  data: 2.3641  max mem: 14662
Epoch: [40]  [10/64]  eta: 0:00:27  lr: 0.000072  min_lr: 0.000000  loss: 1.0682 (1.1176)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6358 (2.8184)  time: 0.5047  data: 0.2153  max mem: 14662
Epoch: [40]  [20/64]  eta: 0:00:17  lr: 0.000070  min_lr: 0.000000  loss: 1.0132 (1.0647)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8601 (2.7555)  time: 0.2899  data: 0.0004  max mem: 14662
Epoch: [40]  [30/64]  eta: 0:00:12  lr: 0.000068  min_lr: 0.000000  loss: 1.0132 (1.0397)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5740 (2.6947)  time: 0.2901  data: 0.0003  max mem: 14662
Epoch: [40]  [40/64]  eta: 0:00:08  lr: 0.000066  min_lr: 0.000000  loss: 1.0601 (1.0469)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5740 (2.6993)  time: 0.2906  data: 0.0003  max mem: 14662
Epoch: [40]  [50/64]  eta: 0:00:04  lr: 0.000064  min_lr: 0.000000  loss: 1.0601 (1.0194)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5194 (2.6207)  time: 0.2923  data: 0.0003  max mem: 14662
Epoch: [40]  [60/64]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000000  loss: 0.9867 (1.0251)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4627 (2.6594)  time: 0.2925  data: 0.0002  max mem: 14662
Epoch: [40]  [63/64]  eta: 0:00:00  lr: 0.000061  min_lr: 0.000000  loss: 0.9867 (1.0180)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6019 (2.6459)  time: 0.2922  data: 0.0002  max mem: 14662
Epoch: [40] Total time: 0:00:21 (0.3294 s / it)
2025-04-28 16:42:46 Averaged stats: lr: 0.000061  min_lr: 0.000000  loss: 0.9867 (1.0180)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6019 (2.6459)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4347  data: 2.2890  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9099  data: 0.7631  max mem: 14662
Test: Total time: 0:00:02 (0.9299 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.8474 Acc: 0.9113 Recall_macro: 0.8474 Recall_weighted: 0.9113 AUC-ROC: 0.9734 Weighted F1-score: 0.9212
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 40, 'Val Loss': 0.35771074891090393, 'Val BAcc': np.float64(0.847369940001519), 'Val Acc': 0.9113043478260869, 'Val ROC': np.float64(0.9734090848078633), 'Val W_F1': 0.9212163885298581, 'Val Recall_macro': 0.847369940001519, 'Val Recall_weighted': 0.9113043478260869}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [41]  [ 0/64]  eta: 0:03:00  lr: 0.000061  min_lr: 0.000000  loss: 1.0479 (1.0479)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7751 (2.7751)  time: 2.8133  data: 2.5272  max mem: 14662
Epoch: [41]  [10/64]  eta: 0:00:28  lr: 0.000059  min_lr: 0.000000  loss: 1.0947 (1.0777)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9570 (3.0924)  time: 0.5204  data: 0.2301  max mem: 14662
Epoch: [41]  [20/64]  eta: 0:00:18  lr: 0.000057  min_lr: 0.000000  loss: 1.1095 (1.1043)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9707 (3.1949)  time: 0.2908  data: 0.0004  max mem: 14662
Epoch: [41]  [30/64]  eta: 0:00:12  lr: 0.000055  min_lr: 0.000000  loss: 1.1095 (1.0431)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8398 (3.1032)  time: 0.2909  data: 0.0003  max mem: 14662
Epoch: [41]  [40/64]  eta: 0:00:08  lr: 0.000053  min_lr: 0.000000  loss: 1.0804 (1.0452)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7179 (2.9734)  time: 0.2914  data: 0.0003  max mem: 14662
Epoch: [41]  [50/64]  eta: 0:00:04  lr: 0.000051  min_lr: 0.000000  loss: 1.0804 (1.0413)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7152 (2.9458)  time: 0.2912  data: 0.0003  max mem: 14662
Epoch: [41]  [60/64]  eta: 0:00:01  lr: 0.000049  min_lr: 0.000000  loss: 1.1196 (1.0385)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8434 (2.8939)  time: 0.2908  data: 0.0001  max mem: 14662
Epoch: [41]  [63/64]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000000  loss: 1.1355 (1.0504)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8878 (2.9216)  time: 0.2912  data: 0.0001  max mem: 14662
Epoch: [41] Total time: 0:00:21 (0.3319 s / it)
2025-04-28 16:43:10 Averaged stats: lr: 0.000049  min_lr: 0.000000  loss: 1.1355 (1.0504)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8878 (2.9216)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4870  data: 2.3447  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9274  data: 0.7817  max mem: 14662
Test: Total time: 0:00:02 (0.9478 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7932 Acc: 0.9130 Recall_macro: 0.7932 Recall_weighted: 0.9130 AUC-ROC: 0.9725 Weighted F1-score: 0.9217
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 41, 'Val Loss': 0.3490670919418335, 'Val BAcc': np.float64(0.7932380735388254), 'Val Acc': 0.9130434782608695, 'Val ROC': np.float64(0.9724631033531154), 'Val W_F1': 0.9216668620365855, 'Val Recall_macro': 0.7932380735388254, 'Val Recall_weighted': 0.9130434782608695}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [42]  [ 0/64]  eta: 0:03:04  lr: 0.000049  min_lr: 0.000000  loss: 1.2126 (1.2126)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1355 (3.1355)  time: 2.8799  data: 2.5948  max mem: 14662
Epoch: [42]  [10/64]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 1.1159 (1.0775)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9667 (2.7600)  time: 0.5256  data: 0.2362  max mem: 14662
Epoch: [42]  [20/64]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000000  loss: 1.0642 (1.0976)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9667 (2.8965)  time: 0.2903  data: 0.0003  max mem: 14662
Epoch: [42]  [30/64]  eta: 0:00:12  lr: 0.000043  min_lr: 0.000000  loss: 1.0820 (1.0818)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5734 (2.7794)  time: 0.2913  data: 0.0003  max mem: 14662
Epoch: [42]  [40/64]  eta: 0:00:08  lr: 0.000042  min_lr: 0.000000  loss: 1.0998 (1.0817)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5582 (2.7861)  time: 0.2918  data: 0.0004  max mem: 14662
Epoch: [42]  [50/64]  eta: 0:00:04  lr: 0.000040  min_lr: 0.000000  loss: 1.0175 (1.0486)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7287 (2.7824)  time: 0.2912  data: 0.0003  max mem: 14662
Epoch: [42]  [60/64]  eta: 0:00:01  lr: 0.000038  min_lr: 0.000000  loss: 0.9088 (1.0341)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7287 (2.7386)  time: 0.2908  data: 0.0002  max mem: 14662
Epoch: [42]  [63/64]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000000  loss: 0.9820 (1.0342)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6835 (2.7189)  time: 0.2906  data: 0.0001  max mem: 14662
Epoch: [42] Total time: 0:00:21 (0.3329 s / it)
2025-04-28 16:43:35 Averaged stats: lr: 0.000038  min_lr: 0.000000  loss: 0.9820 (1.0342)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6835 (2.7189)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4777  data: 2.3356  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9250  data: 0.7786  max mem: 14662
Test: Total time: 0:00:02 (0.9492 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7856 Acc: 0.9113 Recall_macro: 0.7856 Recall_weighted: 0.9113 AUC-ROC: 0.9747 Weighted F1-score: 0.9202
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 42, 'Val Loss': 0.3374752104282379, 'Val BAcc': np.float64(0.7856478859486378), 'Val Acc': 0.9113043478260869, 'Val ROC': np.float64(0.9746646872880057), 'Val W_F1': 0.9201588383531536, 'Val Recall_macro': 0.7856478859486378, 'Val Recall_weighted': 0.9113043478260869}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [43]  [ 0/64]  eta: 0:02:59  lr: 0.000038  min_lr: 0.000000  loss: 0.6818 (0.6818)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8516 (3.8516)  time: 2.8106  data: 2.5242  max mem: 14662
Epoch: [43]  [10/64]  eta: 0:00:28  lr: 0.000036  min_lr: 0.000000  loss: 1.0489 (0.9800)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9622 (2.9017)  time: 0.5190  data: 0.2298  max mem: 14662
Epoch: [43]  [20/64]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000000  loss: 1.1294 (1.0353)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8546 (2.8559)  time: 0.2900  data: 0.0004  max mem: 14662
Epoch: [43]  [30/64]  eta: 0:00:12  lr: 0.000033  min_lr: 0.000000  loss: 1.0931 (1.0213)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8546 (2.8582)  time: 0.2902  data: 0.0003  max mem: 14662
Epoch: [43]  [40/64]  eta: 0:00:08  lr: 0.000032  min_lr: 0.000000  loss: 0.8998 (0.9803)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4173 (2.7197)  time: 0.2905  data: 0.0003  max mem: 14662
Epoch: [43]  [50/64]  eta: 0:00:04  lr: 0.000030  min_lr: 0.000000  loss: 0.9069 (0.9840)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3307 (2.6815)  time: 0.2904  data: 0.0002  max mem: 14662
Epoch: [43]  [60/64]  eta: 0:00:01  lr: 0.000029  min_lr: 0.000000  loss: 1.0751 (0.9909)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5640 (2.6915)  time: 0.2904  data: 0.0001  max mem: 14662
Epoch: [43]  [63/64]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000000  loss: 1.1029 (0.9964)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7305 (2.6988)  time: 0.2904  data: 0.0001  max mem: 14662
Epoch: [43] Total time: 0:00:21 (0.3312 s / it)
2025-04-28 16:43:59 Averaged stats: lr: 0.000028  min_lr: 0.000000  loss: 1.1029 (0.9964)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7305 (2.6988)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:06    time: 2.3292  data: 2.1866  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8756  data: 0.7290  max mem: 14662
Test: Total time: 0:00:02 (0.8988 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7847 Acc: 0.9200 Recall_macro: 0.7847 Recall_weighted: 0.9200 AUC-ROC: 0.9753 Weighted F1-score: 0.9249
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 43, 'Val Loss': 0.3370070457458496, 'Val BAcc': np.float64(0.7846955049962568), 'Val Acc': 0.92, 'Val ROC': np.float64(0.975345980177967), 'Val W_F1': 0.9249416927276874, 'Val Recall_macro': 0.7846955049962568, 'Val Recall_weighted': 0.92}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [44]  [ 0/64]  eta: 0:02:48  lr: 0.000028  min_lr: 0.000000  loss: 1.3230 (1.3230)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9119 (3.9119)  time: 2.6266  data: 2.3399  max mem: 14662
Epoch: [44]  [10/64]  eta: 0:00:27  lr: 0.000027  min_lr: 0.000000  loss: 0.9529 (0.9604)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4594 (2.4726)  time: 0.5022  data: 0.2130  max mem: 14662
Epoch: [44]  [20/64]  eta: 0:00:17  lr: 0.000025  min_lr: 0.000000  loss: 0.9550 (0.9933)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5549 (2.6207)  time: 0.2900  data: 0.0003  max mem: 14662
Epoch: [44]  [30/64]  eta: 0:00:12  lr: 0.000024  min_lr: 0.000000  loss: 0.9661 (0.9722)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6825 (2.6410)  time: 0.2907  data: 0.0004  max mem: 14662
Epoch: [44]  [40/64]  eta: 0:00:08  lr: 0.000023  min_lr: 0.000000  loss: 0.8974 (0.9551)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5483 (2.5885)  time: 0.2911  data: 0.0004  max mem: 14662
Epoch: [44]  [50/64]  eta: 0:00:04  lr: 0.000022  min_lr: 0.000000  loss: 0.8247 (0.9335)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4597 (2.5886)  time: 0.2910  data: 0.0003  max mem: 14662
Epoch: [44]  [60/64]  eta: 0:00:01  lr: 0.000020  min_lr: 0.000000  loss: 0.9331 (0.9435)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6954 (2.6080)  time: 0.2910  data: 0.0002  max mem: 14662
Epoch: [44]  [63/64]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 0.9532 (0.9350)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3757 (2.5765)  time: 0.2911  data: 0.0001  max mem: 14662
Epoch: [44] Total time: 0:00:21 (0.3286 s / it)
2025-04-28 16:44:22 Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 0.9532 (0.9350)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3757 (2.5765)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.4321  data: 2.2901  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9090  data: 0.7635  max mem: 14662
Test: Total time: 0:00:02 (0.9288 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7942 Acc: 0.9043 Recall_macro: 0.7942 Recall_weighted: 0.9043 AUC-ROC: 0.9748 Weighted F1-score: 0.9154
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 44, 'Val Loss': 0.35694125294685364, 'Val BAcc': np.float64(0.7941904544912063), 'Val Acc': 0.9043478260869565, 'Val ROC': np.float64(0.9747542545896561), 'Val W_F1': 0.9153674447109866, 'Val Recall_macro': 0.7941904544912063, 'Val Recall_weighted': 0.9043478260869565}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [45]  [ 0/64]  eta: 0:02:56  lr: 0.000020  min_lr: 0.000000  loss: 1.1265 (1.1265)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1694 (3.1694)  time: 2.7649  data: 2.4789  max mem: 14662
Epoch: [45]  [10/64]  eta: 0:00:27  lr: 0.000019  min_lr: 0.000000  loss: 0.8819 (0.9170)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2887 (2.4907)  time: 0.5152  data: 0.2258  max mem: 14662
Epoch: [45]  [20/64]  eta: 0:00:17  lr: 0.000018  min_lr: 0.000000  loss: 1.0459 (0.9946)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2887 (2.5469)  time: 0.2904  data: 0.0004  max mem: 14662
Epoch: [45]  [30/64]  eta: 0:00:12  lr: 0.000017  min_lr: 0.000000  loss: 1.0692 (0.9614)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7236 (2.6415)  time: 0.2908  data: 0.0003  max mem: 14662
Epoch: [45]  [40/64]  eta: 0:00:08  lr: 0.000016  min_lr: 0.000000  loss: 0.9426 (0.9731)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8153 (2.7110)  time: 0.2911  data: 0.0003  max mem: 14662
Epoch: [45]  [50/64]  eta: 0:00:04  lr: 0.000015  min_lr: 0.000000  loss: 1.0928 (0.9983)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5208 (2.6862)  time: 0.2912  data: 0.0003  max mem: 14662
Epoch: [45]  [60/64]  eta: 0:00:01  lr: 0.000014  min_lr: 0.000000  loss: 1.1027 (1.0131)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4905 (2.6615)  time: 0.2914  data: 0.0001  max mem: 14662
Epoch: [45]  [63/64]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.1027 (1.0076)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4905 (2.6638)  time: 0.2912  data: 0.0001  max mem: 14662
Epoch: [45] Total time: 0:00:21 (0.3310 s / it)
2025-04-28 16:44:46 Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.1027 (1.0076)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4905 (2.6638)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3978  data: 2.2542  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8964  data: 0.7515  max mem: 14662
Test: Total time: 0:00:02 (0.9228 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7915 Acc: 0.9096 Recall_macro: 0.7915 Recall_weighted: 0.9096 AUC-ROC: 0.9752 Weighted F1-score: 0.9185
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 45, 'Val Loss': 0.34934061765670776, 'Val BAcc': np.float64(0.7915064718072236), 'Val Acc': 0.9095652173913044, 'Val ROC': np.float64(0.9752187919160062), 'Val W_F1': 0.9184624247194375, 'Val Recall_macro': 0.7915064718072236, 'Val Recall_weighted': 0.9095652173913044}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [46]  [ 0/64]  eta: 0:03:00  lr: 0.000013  min_lr: 0.000000  loss: 1.0336 (1.0336)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0341 (3.0341)  time: 2.8154  data: 2.5316  max mem: 14662
Epoch: [46]  [10/64]  eta: 0:00:28  lr: 0.000012  min_lr: 0.000000  loss: 1.0045 (0.9144)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7086 (2.7227)  time: 0.5186  data: 0.2304  max mem: 14662
Epoch: [46]  [20/64]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 0.9464 (0.9404)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2718 (2.5198)  time: 0.2897  data: 0.0003  max mem: 14662
Epoch: [46]  [30/64]  eta: 0:00:12  lr: 0.000011  min_lr: 0.000000  loss: 0.9858 (0.9806)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6141 (2.6252)  time: 0.2909  data: 0.0003  max mem: 14662
Epoch: [46]  [40/64]  eta: 0:00:08  lr: 0.000010  min_lr: 0.000000  loss: 0.9849 (0.9541)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8456 (2.6484)  time: 0.2914  data: 0.0004  max mem: 14662
Epoch: [46]  [50/64]  eta: 0:00:04  lr: 0.000009  min_lr: 0.000000  loss: 0.9849 (0.9761)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5989 (2.6836)  time: 0.2912  data: 0.0004  max mem: 14662
Epoch: [46]  [60/64]  eta: 0:00:01  lr: 0.000008  min_lr: 0.000000  loss: 1.0387 (0.9807)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5989 (2.6483)  time: 0.2905  data: 0.0001  max mem: 14662
Epoch: [46]  [63/64]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.0099 (0.9738)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5130 (2.6276)  time: 0.2905  data: 0.0001  max mem: 14662
Epoch: [46] Total time: 0:00:21 (0.3314 s / it)
2025-04-28 16:45:11 Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.0099 (0.9738)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5130 (2.6276)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3658  data: 2.2250  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8874  data: 0.7418  max mem: 14662
Test: Total time: 0:00:02 (0.9138 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7898 Acc: 0.9061 Recall_macro: 0.7898 Recall_weighted: 0.9061 AUC-ROC: 0.9744 Weighted F1-score: 0.9162
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 46, 'Val Loss': 0.3557029962539673, 'Val BAcc': np.float64(0.7898462606733283), 'Val Acc': 0.9060869565217391, 'Val ROC': np.float64(0.9743675931948786), 'Val W_F1': 0.9162007860639947, 'Val Recall_macro': 0.7898462606733283, 'Val Recall_weighted': 0.9060869565217391}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [47]  [ 0/64]  eta: 0:02:51  lr: 0.000008  min_lr: 0.000000  loss: 0.9314 (0.9314)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7517 (1.7517)  time: 2.6782  data: 2.3937  max mem: 14662
Epoch: [47]  [10/64]  eta: 0:00:27  lr: 0.000007  min_lr: 0.000000  loss: 1.0549 (1.0559)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6446 (2.6344)  time: 0.5060  data: 0.2179  max mem: 14662
Epoch: [47]  [20/64]  eta: 0:00:17  lr: 0.000007  min_lr: 0.000000  loss: 1.0092 (0.9787)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6843 (2.6006)  time: 0.2911  data: 0.0004  max mem: 14662
Epoch: [47]  [30/64]  eta: 0:00:12  lr: 0.000006  min_lr: 0.000000  loss: 1.0827 (1.0227)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6968 (2.6434)  time: 0.2931  data: 0.0004  max mem: 14662
Epoch: [47]  [40/64]  eta: 0:00:08  lr: 0.000005  min_lr: 0.000000  loss: 1.0912 (0.9989)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7100 (2.6606)  time: 0.2923  data: 0.0004  max mem: 14662
Epoch: [47]  [50/64]  eta: 0:00:04  lr: 0.000005  min_lr: 0.000000  loss: 1.0192 (1.0043)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9601 (2.7455)  time: 0.2921  data: 0.0003  max mem: 14662
Epoch: [47]  [60/64]  eta: 0:00:01  lr: 0.000004  min_lr: 0.000000  loss: 1.0192 (0.9909)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9601 (2.7074)  time: 0.2926  data: 0.0002  max mem: 14662
Epoch: [47]  [63/64]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 0.9443 (0.9950)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6539 (2.7051)  time: 0.2926  data: 0.0001  max mem: 14662
Epoch: [47] Total time: 0:00:21 (0.3306 s / it)
2025-04-28 16:45:34 Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 0.9443 (0.9950)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6539 (2.7051)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.5432  data: 2.3982  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9471  data: 0.7995  max mem: 14662
Test: Total time: 0:00:02 (0.9719 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7893 Acc: 0.9113 Recall_macro: 0.7893 Recall_weighted: 0.9113 AUC-ROC: 0.9740 Weighted F1-score: 0.9198
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 47, 'Val Loss': 0.3515513241291046, 'Val BAcc': np.float64(0.7892842495850015), 'Val Acc': 0.9113043478260869, 'Val ROC': np.float64(0.974039450471179), 'Val W_F1': 0.9197555263603736, 'Val Recall_macro': 0.7892842495850015, 'Val Recall_weighted': 0.9113043478260869}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [48]  [ 0/64]  eta: 0:02:55  lr: 0.000004  min_lr: 0.000000  loss: 0.9620 (0.9620)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0582 (2.0582)  time: 2.7487  data: 2.4630  max mem: 14662
Epoch: [48]  [10/64]  eta: 0:00:27  lr: 0.000004  min_lr: 0.000000  loss: 0.9620 (0.8975)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4851 (2.8976)  time: 0.5138  data: 0.2244  max mem: 14662
Epoch: [48]  [20/64]  eta: 0:00:17  lr: 0.000003  min_lr: 0.000000  loss: 1.0179 (0.9426)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6833 (2.7669)  time: 0.2904  data: 0.0004  max mem: 14662
Epoch: [48]  [30/64]  eta: 0:00:12  lr: 0.000003  min_lr: 0.000000  loss: 1.0894 (0.9620)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6900 (2.7513)  time: 0.2909  data: 0.0004  max mem: 14662
Epoch: [48]  [40/64]  eta: 0:00:08  lr: 0.000002  min_lr: 0.000000  loss: 1.0529 (0.9699)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6900 (2.7594)  time: 0.2913  data: 0.0004  max mem: 14662
Epoch: [48]  [50/64]  eta: 0:00:04  lr: 0.000002  min_lr: 0.000000  loss: 1.0529 (0.9868)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6357 (2.7877)  time: 0.2914  data: 0.0003  max mem: 14662
Epoch: [48]  [60/64]  eta: 0:00:01  lr: 0.000002  min_lr: 0.000000  loss: 1.0531 (0.9932)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6428 (2.7766)  time: 0.2917  data: 0.0002  max mem: 14662
Epoch: [48]  [63/64]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.0107 (0.9932)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4547 (2.7550)  time: 0.2917  data: 0.0001  max mem: 14662
Epoch: [48] Total time: 0:00:21 (0.3310 s / it)
2025-04-28 16:45:59 Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.0107 (0.9932)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4547 (2.7550)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.3500  data: 2.2091  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.8805  data: 0.7365  max mem: 14662
Test: Total time: 0:00:02 (0.9034 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7918 Acc: 0.9113 Recall_macro: 0.7918 Recall_weighted: 0.9113 AUC-ROC: 0.9745 Weighted F1-score: 0.9201
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 48, 'Val Loss': 0.3488948345184326, 'Val BAcc': np.float64(0.7918239321246839), 'Val Acc': 0.9113043478260869, 'Val ROC': np.float64(0.9744955610380349), 'Val W_F1': 0.9201065265170321, 'Val Recall_macro': 0.7918239321246839, 'Val Recall_weighted': 0.9113043478260869}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [49]  [ 0/64]  eta: 0:02:50  lr: 0.000002  min_lr: 0.000000  loss: 0.6245 (0.6245)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4669 (1.4669)  time: 2.6666  data: 2.3792  max mem: 14662
Epoch: [49]  [10/64]  eta: 0:00:27  lr: 0.000002  min_lr: 0.000000  loss: 0.8169 (0.8596)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3011 (2.2405)  time: 0.5060  data: 0.2167  max mem: 14662
Epoch: [49]  [20/64]  eta: 0:00:17  lr: 0.000001  min_lr: 0.000000  loss: 0.9728 (0.9188)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3361 (2.3701)  time: 0.2902  data: 0.0004  max mem: 14662
Epoch: [49]  [30/64]  eta: 0:00:12  lr: 0.000001  min_lr: 0.000000  loss: 0.9728 (0.9321)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5652 (2.5186)  time: 0.2911  data: 0.0003  max mem: 14662
Epoch: [49]  [40/64]  eta: 0:00:08  lr: 0.000001  min_lr: 0.000000  loss: 1.0033 (0.9485)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6225 (2.5564)  time: 0.2919  data: 0.0003  max mem: 14662
Epoch: [49]  [50/64]  eta: 0:00:04  lr: 0.000001  min_lr: 0.000000  loss: 1.0051 (0.9442)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7530 (2.6168)  time: 0.2915  data: 0.0003  max mem: 14662
Epoch: [49]  [60/64]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000000  loss: 1.0446 (0.9575)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7423 (2.5791)  time: 0.2914  data: 0.0002  max mem: 14662
Epoch: [49]  [63/64]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.0446 (0.9618)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3855 (2.5847)  time: 0.2913  data: 0.0001  max mem: 14662
Epoch: [49] Total time: 0:00:21 (0.3298 s / it)
2025-04-28 16:46:22 Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.0446 (0.9618)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3855 (2.5847)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/3]  eta: 0:00:07    time: 2.6125  data: 2.4686  max mem: 14662
Test:  [2/3]  eta: 0:00:00    time: 0.9688  data: 0.8230  max mem: 14662
Test: Total time: 0:00:02 (0.9939 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7843 Acc: 0.9096 Recall_macro: 0.7843 Recall_weighted: 0.9096 AUC-ROC: 0.9747 Weighted F1-score: 0.9178
Predictions for val saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/val.csv
-------------------------- {'Epoch': 49, 'Val Loss': 0.3459063768386841, 'Val BAcc': np.float64(0.7843051351322028), 'Val Acc': 0.9095652173913044, 'Val ROC': np.float64(0.9747349437922368), 'Val W_F1': 0.9177886100652692, 'Val Recall_macro': 0.7843051351322028, 'Val Recall_weighted': 0.9095652173913044}
Max val mean recall: 0.86%
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:723: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_dict = torch.load(model_weight)
Starting test without tta
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [ 0/10]  eta: 0:00:19    time: 1.9536  data: 1.8625  max mem: 14662
Test:  [ 9/10]  eta: 0:00:00    time: 0.2952  data: 0.1877  max mem: 14662
Test: Total time: 0:00:03 (0.3051 s / it)
------------- test -------------
Sklearn Metrics - BAcc: 0.8589 Acc: 0.8847 Recall_macro: 0.8589 Recall_weighted: 0.8847 AUC-ROC: 0.9781 Weighted F1-score: 0.8956
{'balanced_accuracy': np.float64(0.8589142905178067), 'accuracy': 0.8847402597402597, 'top3 accuracy': np.float64(0.9918831168831169), 'top5 accuracy': np.float64(0.9983766233766234), 'sensitivity': np.float64(0.8589142905178067), 'specificity': np.float64(0.9798390556202232), 'auc_roc': np.float64(0.9781076455171686), 'weighted_f1': 0.8955727025972919, 'recall_macro': 0.8589142905178067, 'recall_weighted': 0.8847402597402597}
Predictions for test saved to /home/syyan/XJ/PanDerm-open_source/finetune/work_dir/HAM10000_cleaned_using_lp_setting1_base/test.csv
Training time 0:20:31
