Not using distributed mode
Namespace(mode='train', batch_size=128, epochs=50, update_freq=1, save_ckpt_freq=5, model='PanDerm_Large_FT', rel_pos_bias=True, sin_pos_emb=True, layer_scale_init_value=0.1, ood_eval=False, input_size=224, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, weights=True, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, percent_data=1.0, TTA=True, monitor='acc', opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.65, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=10, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', pretrained_checkpoint='/home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth', model_key='model|module|state_dict', model_prefix='', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, data_path='/datasets01/imagenet_full_size/061417/', eval_data_path=None, test_csv_path=None, image_key='image', nb_classes=7, imagenet_default_mean_and_std=True, data_set='IMNET', csv_path='/home/syyan/XJ/PanDerm-open_source/data/linear_probing/HAM-official-7-lp.csv', root_path='/home/share/Uni_Eval/ISIC2018_reader/images/', output_dir='/home/share/FM_Code/PanDerm/HAM_Res_monitor_acc_TTA/', log_dir=None, device='cuda', seed=122, resume='', auto_resume=False, wandb_name='Reproduce_HAM_FT_122', save_ckpt=True, start_epoch=0, eval=False, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, enable_linear_eval=False, enable_multi_print=False, exp_name='ham finetune and eval_acc', distributed=False)
Label distribution:
Label 0: 262
Label 1: 411
Label 2: 879
Label 3: 92
Label 4: 890
Label 5: 5364
Label 6: 114
Using WeightedRandomSampler
train size: 8012 ,val size: 500 ,test size: 1503
Mixup is activated!
/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=7, bias=True)
)
Patch size = (16, 16)
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained_checkpoint, map_location='cpu')
Load ckpt from /home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth
Load state_dict by model_key = model
all keys: ['encoder.cls_token', 'encoder.pos_embed', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.blocks.0.gamma_1', 'encoder.blocks.0.gamma_2', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.q_bias', 'encoder.blocks.0.attn.v_bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.gamma_1', 'encoder.blocks.1.gamma_2', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.q_bias', 'encoder.blocks.1.attn.v_bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.gamma_1', 'encoder.blocks.2.gamma_2', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.q_bias', 'encoder.blocks.2.attn.v_bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.gamma_1', 'encoder.blocks.3.gamma_2', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.q_bias', 'encoder.blocks.3.attn.v_bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.gamma_1', 'encoder.blocks.4.gamma_2', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.q_bias', 'encoder.blocks.4.attn.v_bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.gamma_1', 'encoder.blocks.5.gamma_2', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.q_bias', 'encoder.blocks.5.attn.v_bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.gamma_1', 'encoder.blocks.6.gamma_2', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.q_bias', 'encoder.blocks.6.attn.v_bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.gamma_1', 'encoder.blocks.7.gamma_2', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.q_bias', 'encoder.blocks.7.attn.v_bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.blocks.8.gamma_1', 'encoder.bl
##############new keys: 454 odict_keys(['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight', 'regresser.regressor_
Weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_bias_table', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.3.attn.relative_position_bias_table', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.6.attn.relative_position_bias_table', 'blocks.7.attn.relative_position_bias_table', 'blocks.8.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.10.attn.relative_position_bias_table', 'blocks.11.attn.relative_position_bias_table', 'blocks.12.attn.relative_position_bias_table', 'blocks.13.attn.relative_position_bias_table', 'blocks.14.attn.relative_position_bias_table', 'blocks.15.attn.relative_position_bias_table', 'blocks.16.attn.relative_position_bias_table', 'blocks.17.attn.relative_position_bias_table', 'blocks.18.attn.relative_position_bias_table', 'blocks.19.attn.relative_position_bias_table', 'blocks.20.attn.relative_position_bias_table', 'blocks.21.attn.relative_position_bias_table', 'blocks.22.attn.relative_position_bias_table', 'blocks.23.attn.relative_position_bias_table', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight',
Ignored weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_index', 'blocks.1.attn.relative_position_index', 'blocks.2.attn.relative_position_index', 'blocks.3.attn.relative_position_index', 'blocks.4.attn.relative_position_index', 'blocks.5.attn.relative_position_index', 'blocks.6.attn.relative_position_index', 'blocks.7.attn.relative_position_index', 'blocks.8.attn.relative_position_index', 'blocks.9.attn.relative_position_index', 'blocks.10.attn.relative_position_index', 'blocks.11.attn.relative_position_index', 'blocks.12.attn.relative_position_index', 'blocks.13.attn.relative_position_index', 'blocks.14.attn.relative_position_index', 'blocks.15.attn.relative_position_index', 'blocks.16.attn.relative_position_index', 'blocks.17.attn.relative_position_index', 'blocks.18.attn.relative_position_index', 'blocks.19.attn.relative_position_index', 'blocks.20.attn.relative_position_index', 'blocks.21.attn.relative_position_index', 'blocks.22.attn.relative_position_index', 'blocks.23.attn.relative_position_index']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=7, bias=True)
)
number of params: 303412743
LR = 0.00050000
Batch size = 128
Update frequent = 1
Number of training examples = 8012
Number of training training per epoch = 62
Assigned values = [2.1029740616282293e-05, 3.2353447101972754e-05, 4.977453400303501e-05, 7.65762061585154e-05, 0.00011780954793617752, 0.00018124545836335003, 0.0002788391667128462, 0.0004289833334043787, 0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "patch_embed.proj.bias"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.gamma_1",
      "blocks.0.gamma_2",
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.relative_position_bias_table",
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.gamma_1",
      "blocks.1.gamma_2",
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.relative_position_bias_table",
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.gamma_1",
      "blocks.2.gamma_2",
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.relative_position_bias_table",
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.gamma_1",
      "blocks.3.gamma_2",
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.relative_position_bias_table",
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.gamma_1",
      "blocks.4.gamma_2",
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.relative_position_bias_table",
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.gamma_1",
      "blocks.5.gamma_2",
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.relative_position_bias_table",
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.gamma_1",
      "blocks.6.gamma_2",
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.relative_position_bias_table",
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.gamma_1",
      "blocks.7.gamma_2",
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.relative_position_bias_table",
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.gamma_1",
      "blocks.8.gamma_2",
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.relative_position_bias_table",
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.gamma_1",
      "blocks.9.gamma_2",
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.relative_position_bias_table",
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.gamma_1",
      "blocks.10.gamma_2",
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.relative_position_bias_table",
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.gamma_1",
      "blocks.11.gamma_2",
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.relative_position_bias_table",
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.12.gamma_1",
      "blocks.12.gamma_2",
      "blocks.12.norm1.weight",
      "blocks.12.norm1.bias",
      "blocks.12.attn.q_bias",
      "blocks.12.attn.v_bias",
      "blocks.12.attn.proj.bias",
      "blocks.12.norm2.weight",
      "blocks.12.norm2.bias",
      "blocks.12.mlp.fc1.bias",
      "blocks.12.mlp.fc2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.12.attn.relative_position_bias_table",
      "blocks.12.attn.qkv.weight",
      "blocks.12.attn.proj.weight",
      "blocks.12.mlp.fc1.weight",
      "blocks.12.mlp.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_14_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.13.gamma_1",
      "blocks.13.gamma_2",
      "blocks.13.norm1.weight",
      "blocks.13.norm1.bias",
      "blocks.13.attn.q_bias",
      "blocks.13.attn.v_bias",
      "blocks.13.attn.proj.bias",
      "blocks.13.norm2.weight",
      "blocks.13.norm2.bias",
      "blocks.13.mlp.fc1.bias",
      "blocks.13.mlp.fc2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_14_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.13.attn.relative_position_bias_table",
      "blocks.13.attn.qkv.weight",
      "blocks.13.attn.proj.weight",
      "blocks.13.mlp.fc1.weight",
      "blocks.13.mlp.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_15_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.14.gamma_1",
      "blocks.14.gamma_2",
      "blocks.14.norm1.weight",
      "blocks.14.norm1.bias",
      "blocks.14.attn.q_bias",
      "blocks.14.attn.v_bias",
      "blocks.14.attn.proj.bias",
      "blocks.14.norm2.weight",
      "blocks.14.norm2.bias",
      "blocks.14.mlp.fc1.bias",
      "blocks.14.mlp.fc2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_15_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.14.attn.relative_position_bias_table",
      "blocks.14.attn.qkv.weight",
      "blocks.14.attn.proj.weight",
      "blocks.14.mlp.fc1.weight",
      "blocks.14.mlp.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_16_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.15.gamma_1",
      "blocks.15.gamma_2",
      "blocks.15.norm1.weight",
      "blocks.15.norm1.bias",
      "blocks.15.attn.q_bias",
      "blocks.15.attn.v_bias",
      "blocks.15.attn.proj.bias",
      "blocks.15.norm2.weight",
      "blocks.15.norm2.bias",
      "blocks.15.mlp.fc1.bias",
      "blocks.15.mlp.fc2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_16_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.15.attn.relative_position_bias_table",
      "blocks.15.attn.qkv.weight",
      "blocks.15.attn.proj.weight",
      "blocks.15.mlp.fc1.weight",
      "blocks.15.mlp.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.16.gamma_1",
      "blocks.16.gamma_2",
      "blocks.16.norm1.weight",
      "blocks.16.norm1.bias",
      "blocks.16.attn.q_bias",
      "blocks.16.attn.v_bias",
      "blocks.16.attn.proj.bias",
      "blocks.16.norm2.weight",
      "blocks.16.norm2.bias",
      "blocks.16.mlp.fc1.bias",
      "blocks.16.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.16.attn.relative_position_bias_table",
      "blocks.16.attn.qkv.weight",
      "blocks.16.attn.proj.weight",
      "blocks.16.mlp.fc1.weight",
      "blocks.16.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_18_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.17.gamma_1",
      "blocks.17.gamma_2",
      "blocks.17.norm1.weight",
      "blocks.17.norm1.bias",
      "blocks.17.attn.q_bias",
      "blocks.17.attn.v_bias",
      "blocks.17.attn.proj.bias",
      "blocks.17.norm2.weight",
      "blocks.17.norm2.bias",
      "blocks.17.mlp.fc1.bias",
      "blocks.17.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_18_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.17.attn.relative_position_bias_table",
      "blocks.17.attn.qkv.weight",
      "blocks.17.attn.proj.weight",
      "blocks.17.mlp.fc1.weight",
      "blocks.17.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_19_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.18.gamma_1",
      "blocks.18.gamma_2",
      "blocks.18.norm1.weight",
      "blocks.18.norm1.bias",
      "blocks.18.attn.q_bias",
      "blocks.18.attn.v_bias",
      "blocks.18.attn.proj.bias",
      "blocks.18.norm2.weight",
      "blocks.18.norm2.bias",
      "blocks.18.mlp.fc1.bias",
      "blocks.18.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_19_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.18.attn.relative_position_bias_table",
      "blocks.18.attn.qkv.weight",
      "blocks.18.attn.proj.weight",
      "blocks.18.mlp.fc1.weight",
      "blocks.18.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_20_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.19.gamma_1",
      "blocks.19.gamma_2",
      "blocks.19.norm1.weight",
      "blocks.19.norm1.bias",
      "blocks.19.attn.q_bias",
      "blocks.19.attn.v_bias",
      "blocks.19.attn.proj.bias",
      "blocks.19.norm2.weight",
      "blocks.19.norm2.bias",
      "blocks.19.mlp.fc1.bias",
      "blocks.19.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_20_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.19.attn.relative_position_bias_table",
      "blocks.19.attn.qkv.weight",
      "blocks.19.attn.proj.weight",
      "blocks.19.mlp.fc1.weight",
      "blocks.19.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_21_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.20.gamma_1",
      "blocks.20.gamma_2",
      "blocks.20.norm1.weight",
      "blocks.20.norm1.bias",
      "blocks.20.attn.q_bias",
      "blocks.20.attn.v_bias",
      "blocks.20.attn.proj.bias",
      "blocks.20.norm2.weight",
      "blocks.20.norm2.bias",
      "blocks.20.mlp.fc1.bias",
      "blocks.20.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_21_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.20.attn.relative_position_bias_table",
      "blocks.20.attn.qkv.weight",
      "blocks.20.attn.proj.weight",
      "blocks.20.mlp.fc1.weight",
      "blocks.20.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_22_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.21.gamma_1",
      "blocks.21.gamma_2",
      "blocks.21.norm1.weight",
      "blocks.21.norm1.bias",
      "blocks.21.attn.q_bias",
      "blocks.21.attn.v_bias",
      "blocks.21.attn.proj.bias",
      "blocks.21.norm2.weight",
      "blocks.21.norm2.bias",
      "blocks.21.mlp.fc1.bias",
      "blocks.21.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_22_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.21.attn.relative_position_bias_table",
      "blocks.21.attn.qkv.weight",
      "blocks.21.attn.proj.weight",
      "blocks.21.mlp.fc1.weight",
      "blocks.21.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_23_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.22.gamma_1",
      "blocks.22.gamma_2",
      "blocks.22.norm1.weight",
      "blocks.22.norm1.bias",
      "blocks.22.attn.q_bias",
      "blocks.22.attn.v_bias",
      "blocks.22.attn.proj.bias",
      "blocks.22.norm2.weight",
      "blocks.22.norm2.bias",
      "blocks.22.mlp.fc1.bias",
      "blocks.22.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_23_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.22.attn.relative_position_bias_table",
      "blocks.22.attn.qkv.weight",
      "blocks.22.attn.proj.weight",
      "blocks.22.mlp.fc1.weight",
      "blocks.22.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_24_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.23.gamma_1",
      "blocks.23.gamma_2",
      "blocks.23.norm1.weight",
      "blocks.23.norm1.bias",
      "blocks.23.attn.q_bias",
      "blocks.23.attn.v_bias",
      "blocks.23.attn.proj.bias",
      "blocks.23.norm2.weight",
      "blocks.23.norm2.bias",
      "blocks.23.mlp.fc1.bias",
      "blocks.23.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_24_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.23.attn.relative_position_bias_table",
      "blocks.23.attn.qkv.weight",
      "blocks.23.attn.proj.weight",
      "blocks.23.mlp.fc1.weight",
      "blocks.23.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_25_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_25_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
/home/share/FM_Code/PanDerm/classification/furnace/utils.py:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
Use step level LR scheduler!
Set warmup steps = 620
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 50 epochs
Traceback (most recent call last):
  File "/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py", line 751, in <module>
    main(opts, ds_init)
  File "/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py", line 681, in main
    train_stats = train_one_epoch(
  File "/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py", line 91, in train_one_epoch
    for data_iter_step, (samples, _, targets) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):
  File "/home/share/FM_Code/PanDerm/classification/furnace/utils.py", line 217, in log_every
    for obj in iterable:
  File "/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1283, in _get_data
    success, data = self._try_get_data()
  File "/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/queue.py", line 180, in get
    self.not_empty.wait(remaining)
  File "/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/threading.py", line 324, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt
