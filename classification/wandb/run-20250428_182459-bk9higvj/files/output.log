Not using distributed mode
Namespace(mode='train', batch_size=128, epochs=50, update_freq=1, save_ckpt_freq=5, model='PanDerm_Large_FT', rel_pos_bias=True, sin_pos_emb=True, layer_scale_init_value=0.1, ood_eval=False, input_size=224, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, weights=True, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, percent_data=1.0, TTA=False, monitor='acc', opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.65, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=10, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', pretrained_checkpoint='/home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth', model_key='model|module|state_dict', model_prefix='', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, data_path='/datasets01/imagenet_full_size/061417/', eval_data_path=None, test_csv_path=None, image_key='image', nb_classes=6, imagenet_default_mean_and_std=True, data_set='IMNET', csv_path='/home/share/Uni_Eval/pad-ufes/2000.csv', root_path='/home/share/Uni_Eval/pad-ufes/images/', output_dir='/home/share/FM_Code/PanDerm/PAD_Res/', log_dir=None, device='cuda', seed=122, resume='', auto_resume=False, wandb_name='Reproduce_PAD_FT128_5e-4_122', save_ckpt=True, start_epoch=0, eval=False, dist_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, enable_linear_eval=False, enable_multi_print=False, exp_name='pad finetune and eval', distributed=False)
Label distribution:
Label 0: 174
Label 1: 543
Label 2: 464
Label 3: 154
Label 4: 123
Label 5: 35
Using WeightedRandomSampler
train size: 1493 ,val size: 344 ,test size: 461
Mixup is activated!
/home/syyan/anaconda3/envs/PanDerm/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=6, bias=True)
)
Patch size = (16, 16)
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained_checkpoint, map_location='cpu')
Load ckpt from /home/syyan/XJ/PanDerm-open_source/pretrain_weight/panderm_ll_data6_checkpoint-499.pth
Load state_dict by model_key = model
all keys: ['encoder.cls_token', 'encoder.pos_embed', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.blocks.0.gamma_1', 'encoder.blocks.0.gamma_2', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.q_bias', 'encoder.blocks.0.attn.v_bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.gamma_1', 'encoder.blocks.1.gamma_2', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.q_bias', 'encoder.blocks.1.attn.v_bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.gamma_1', 'encoder.blocks.2.gamma_2', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.q_bias', 'encoder.blocks.2.attn.v_bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.gamma_1', 'encoder.blocks.3.gamma_2', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.q_bias', 'encoder.blocks.3.attn.v_bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.gamma_1', 'encoder.blocks.4.gamma_2', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.q_bias', 'encoder.blocks.4.attn.v_bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.gamma_1', 'encoder.blocks.5.gamma_2', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.q_bias', 'encoder.blocks.5.attn.v_bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.gamma_1', 'encoder.blocks.6.gamma_2', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.q_bias', 'encoder.blocks.6.attn.v_bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.gamma_1', 'encoder.blocks.7.gamma_2', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.q_bias', 'encoder.blocks.7.attn.v_bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.blocks.8.gamma_1', 'encoder.bl
##############new keys: 454 odict_keys(['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight', 'regresser.regressor_
Weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_bias_table', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.3.attn.relative_position_bias_table', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.6.attn.relative_position_bias_table', 'blocks.7.attn.relative_position_bias_table', 'blocks.8.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.10.attn.relative_position_bias_table', 'blocks.11.attn.relative_position_bias_table', 'blocks.12.attn.relative_position_bias_table', 'blocks.13.attn.relative_position_bias_table', 'blocks.14.attn.relative_position_bias_table', 'blocks.15.attn.relative_position_bias_table', 'blocks.16.attn.relative_position_bias_table', 'blocks.17.attn.relative_position_bias_table', 'blocks.18.attn.relative_position_bias_table', 'blocks.19.attn.relative_position_bias_table', 'blocks.20.attn.relative_position_bias_table', 'blocks.21.attn.relative_position_bias_table', 'blocks.22.attn.relative_position_bias_table', 'blocks.23.attn.relative_position_bias_table', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['rd_pos_embed', 'mask_token', 'regresser.regressor_blocks.0.gamma_1_cross', 'regresser.regressor_blocks.0.gamma_2_cross', 'regresser.regressor_blocks.0.norm1_q.weight', 'regresser.regressor_blocks.0.norm1_q.bias', 'regresser.regressor_blocks.0.norm1_k.weight', 'regresser.regressor_blocks.0.norm1_k.bias', 'regresser.regressor_blocks.0.norm1_v.weight', 'regresser.regressor_blocks.0.norm1_v.bias', 'regresser.regressor_blocks.0.norm2_cross.weight', 'regresser.regressor_blocks.0.norm2_cross.bias', 'regresser.regressor_blocks.0.cross_attn.q_bias', 'regresser.regressor_blocks.0.cross_attn.v_bias', 'regresser.regressor_blocks.0.cross_attn.q.weight', 'regresser.regressor_blocks.0.cross_attn.k.weight', 'regresser.regressor_blocks.0.cross_attn.v.weight', 'regresser.regressor_blocks.0.cross_attn.proj.weight', 'regresser.regressor_blocks.0.cross_attn.proj.bias', 'regresser.regressor_blocks.0.mlp_cross.fc1.weight', 'regresser.regressor_blocks.0.mlp_cross.fc1.bias', 'regresser.regressor_blocks.0.mlp_cross.fc2.weight', 'regresser.regressor_blocks.0.mlp_cross.fc2.bias', 'regresser.regressor_blocks.1.gamma_1_cross', 'regresser.regressor_blocks.1.gamma_2_cross', 'regresser.regressor_blocks.1.norm1_q.weight', 'regresser.regressor_blocks.1.norm1_q.bias', 'regresser.regressor_blocks.1.norm1_k.weight', 'regresser.regressor_blocks.1.norm1_k.bias', 'regresser.regressor_blocks.1.norm1_v.weight', 'regresser.regressor_blocks.1.norm1_v.bias', 'regresser.regressor_blocks.1.norm2_cross.weight', 'regresser.regressor_blocks.1.norm2_cross.bias', 'regresser.regressor_blocks.1.cross_attn.q_bias', 'regresser.regressor_blocks.1.cross_attn.v_bias', 'regresser.regressor_blocks.1.cross_attn.q.weight', 'regresser.regressor_blocks.1.cross_attn.k.weight', 'regresser.regressor_blocks.1.cross_attn.v.weight', 'regresser.regressor_blocks.1.cross_attn.proj.weight', 'regresser.regressor_blocks.1.cross_attn.proj.bias', 'regresser.regressor_blocks.1.mlp_cross.fc1.weight', 'regresser.regressor_blocks.1.mlp_cross.fc1.bias', 'regresser.regressor_blocks.1.mlp_cross.fc2.weight', 'regresser.regressor_blocks.1.mlp_cross.fc2.bias', 'regresser.regressor_blocks.2.gamma_1_cross', 'regresser.regressor_blocks.2.gamma_2_cross', 'regresser.regressor_blocks.2.norm1_q.weight', 'regresser.regressor_blocks.2.norm1_q.bias', 'regresser.regressor_blocks.2.norm1_k.weight', 'regresser.regressor_blocks.2.norm1_k.bias', 'regresser.regressor_blocks.2.norm1_v.weight', 'regresser.regressor_blocks.2.norm1_v.bias', 'regresser.regressor_blocks.2.norm2_cross.weight', 'regresser.regressor_blocks.2.norm2_cross.bias', 'regresser.regressor_blocks.2.cross_attn.q_bias', 'regresser.regressor_blocks.2.cross_attn.v_bias', 'regresser.regressor_blocks.2.cross_attn.q.weight', 'regresser.regressor_blocks.2.cross_attn.k.weight', 'regresser.regressor_blocks.2.cross_attn.v.weight', 'regresser.regressor_blocks.2.cross_attn.proj.weight', 'regresser.regressor_blocks.2.cross_attn.proj.bias', 'regresser.regressor_blocks.2.mlp_cross.fc1.weight', 'regresser.regressor_blocks.2.mlp_cross.fc1.bias', 'regresser.regressor_blocks.2.mlp_cross.fc2.weight', 'regresser.regressor_blocks.2.mlp_cross.fc2.bias', 'regresser.regressor_blocks.3.gamma_1_cross', 'regresser.regressor_blocks.3.gamma_2_cross', 'regresser.regressor_blocks.3.norm1_q.weight', 'regresser.regressor_blocks.3.norm1_q.bias', 'regresser.regressor_blocks.3.norm1_k.weight', 'regresser.regressor_blocks.3.norm1_k.bias', 'regresser.regressor_blocks.3.norm1_v.weight', 'regresser.regressor_blocks.3.norm1_v.bias', 'regresser.regressor_blocks.3.norm2_cross.weight', 'regresser.regressor_blocks.3.norm2_cross.bias', 'regresser.regressor_blocks.3.cross_attn.q_bias', 'regresser.regressor_blocks.3.cross_attn.v_bias', 'regresser.regressor_blocks.3.cross_attn.q.weight', 'regresser.regressor_blocks.3.cross_attn.k.weight', 'regresser.regressor_blocks.3.cross_attn.v.weight', 'regresser.regressor_blocks.3.cross_attn.proj.weight', 'regresser.regressor_blocks.3.cross_attn.proj.bias', 'regresser.regressor_blocks.3.mlp_cross.fc1.weight',
Ignored weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.relative_position_index', 'blocks.1.attn.relative_position_index', 'blocks.2.attn.relative_position_index', 'blocks.3.attn.relative_position_index', 'blocks.4.attn.relative_position_index', 'blocks.5.attn.relative_position_index', 'blocks.6.attn.relative_position_index', 'blocks.7.attn.relative_position_index', 'blocks.8.attn.relative_position_index', 'blocks.9.attn.relative_position_index', 'blocks.10.attn.relative_position_index', 'blocks.11.attn.relative_position_index', 'blocks.12.attn.relative_position_index', 'blocks.13.attn.relative_position_index', 'blocks.14.attn.relative_position_index', 'blocks.15.attn.relative_position_index', 'blocks.16.attn.relative_position_index', 'blocks.17.attn.relative_position_index', 'blocks.18.attn.relative_position_index', 'blocks.19.attn.relative_position_index', 'blocks.20.attn.relative_position_index', 'blocks.21.attn.relative_position_index', 'blocks.22.attn.relative_position_index', 'blocks.23.attn.relative_position_index']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.008695652708411217)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.017391305416822433)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.02608695812523365)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.03478261083364487)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.04347826540470123)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0521739162504673)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06086956709623337)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06956522166728973)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0782608762383461)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08695653080940247)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09565217792987823)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.104347825050354)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.11304347217082977)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12173912674188614)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1304347813129425)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13913042843341827)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.14782609045505524)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.156521737575531)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16521739959716797)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.17391304671764374)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1826086938381195)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19130435585975647)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=6, bias=True)
)
number of params: 303411718
LR = 0.00050000
Batch size = 128
Update frequent = 1
Number of training examples = 1493
Number of training training per epoch = 11
Assigned values = [2.1029740616282293e-05, 3.2353447101972754e-05, 4.977453400303501e-05, 7.65762061585154e-05, 0.00011780954793617752, 0.00018124545836335003, 0.0002788391667128462, 0.0004289833334043787, 0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "patch_embed.proj.bias"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.gamma_1",
      "blocks.0.gamma_2",
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.relative_position_bias_table",
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.gamma_1",
      "blocks.1.gamma_2",
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.relative_position_bias_table",
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.gamma_1",
      "blocks.2.gamma_2",
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.relative_position_bias_table",
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.gamma_1",
      "blocks.3.gamma_2",
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.relative_position_bias_table",
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.gamma_1",
      "blocks.4.gamma_2",
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.relative_position_bias_table",
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.gamma_1",
      "blocks.5.gamma_2",
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.relative_position_bias_table",
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.gamma_1",
      "blocks.6.gamma_2",
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.relative_position_bias_table",
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.gamma_1",
      "blocks.7.gamma_2",
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.relative_position_bias_table",
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.gamma_1",
      "blocks.8.gamma_2",
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.relative_position_bias_table",
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.gamma_1",
      "blocks.9.gamma_2",
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.relative_position_bias_table",
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.gamma_1",
      "blocks.10.gamma_2",
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.relative_position_bias_table",
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.gamma_1",
      "blocks.11.gamma_2",
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.relative_position_bias_table",
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.12.gamma_1",
      "blocks.12.gamma_2",
      "blocks.12.norm1.weight",
      "blocks.12.norm1.bias",
      "blocks.12.attn.q_bias",
      "blocks.12.attn.v_bias",
      "blocks.12.attn.proj.bias",
      "blocks.12.norm2.weight",
      "blocks.12.norm2.bias",
      "blocks.12.mlp.fc1.bias",
      "blocks.12.mlp.fc2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.12.attn.relative_position_bias_table",
      "blocks.12.attn.qkv.weight",
      "blocks.12.attn.proj.weight",
      "blocks.12.mlp.fc1.weight",
      "blocks.12.mlp.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_14_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.13.gamma_1",
      "blocks.13.gamma_2",
      "blocks.13.norm1.weight",
      "blocks.13.norm1.bias",
      "blocks.13.attn.q_bias",
      "blocks.13.attn.v_bias",
      "blocks.13.attn.proj.bias",
      "blocks.13.norm2.weight",
      "blocks.13.norm2.bias",
      "blocks.13.mlp.fc1.bias",
      "blocks.13.mlp.fc2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_14_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.13.attn.relative_position_bias_table",
      "blocks.13.attn.qkv.weight",
      "blocks.13.attn.proj.weight",
      "blocks.13.mlp.fc1.weight",
      "blocks.13.mlp.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_15_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.14.gamma_1",
      "blocks.14.gamma_2",
      "blocks.14.norm1.weight",
      "blocks.14.norm1.bias",
      "blocks.14.attn.q_bias",
      "blocks.14.attn.v_bias",
      "blocks.14.attn.proj.bias",
      "blocks.14.norm2.weight",
      "blocks.14.norm2.bias",
      "blocks.14.mlp.fc1.bias",
      "blocks.14.mlp.fc2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_15_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.14.attn.relative_position_bias_table",
      "blocks.14.attn.qkv.weight",
      "blocks.14.attn.proj.weight",
      "blocks.14.mlp.fc1.weight",
      "blocks.14.mlp.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_16_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.15.gamma_1",
      "blocks.15.gamma_2",
      "blocks.15.norm1.weight",
      "blocks.15.norm1.bias",
      "blocks.15.attn.q_bias",
      "blocks.15.attn.v_bias",
      "blocks.15.attn.proj.bias",
      "blocks.15.norm2.weight",
      "blocks.15.norm2.bias",
      "blocks.15.mlp.fc1.bias",
      "blocks.15.mlp.fc2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_16_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.15.attn.relative_position_bias_table",
      "blocks.15.attn.qkv.weight",
      "blocks.15.attn.proj.weight",
      "blocks.15.mlp.fc1.weight",
      "blocks.15.mlp.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.16.gamma_1",
      "blocks.16.gamma_2",
      "blocks.16.norm1.weight",
      "blocks.16.norm1.bias",
      "blocks.16.attn.q_bias",
      "blocks.16.attn.v_bias",
      "blocks.16.attn.proj.bias",
      "blocks.16.norm2.weight",
      "blocks.16.norm2.bias",
      "blocks.16.mlp.fc1.bias",
      "blocks.16.mlp.fc2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.16.attn.relative_position_bias_table",
      "blocks.16.attn.qkv.weight",
      "blocks.16.attn.proj.weight",
      "blocks.16.mlp.fc1.weight",
      "blocks.16.mlp.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_18_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.17.gamma_1",
      "blocks.17.gamma_2",
      "blocks.17.norm1.weight",
      "blocks.17.norm1.bias",
      "blocks.17.attn.q_bias",
      "blocks.17.attn.v_bias",
      "blocks.17.attn.proj.bias",
      "blocks.17.norm2.weight",
      "blocks.17.norm2.bias",
      "blocks.17.mlp.fc1.bias",
      "blocks.17.mlp.fc2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_18_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.17.attn.relative_position_bias_table",
      "blocks.17.attn.qkv.weight",
      "blocks.17.attn.proj.weight",
      "blocks.17.mlp.fc1.weight",
      "blocks.17.mlp.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_19_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.18.gamma_1",
      "blocks.18.gamma_2",
      "blocks.18.norm1.weight",
      "blocks.18.norm1.bias",
      "blocks.18.attn.q_bias",
      "blocks.18.attn.v_bias",
      "blocks.18.attn.proj.bias",
      "blocks.18.norm2.weight",
      "blocks.18.norm2.bias",
      "blocks.18.mlp.fc1.bias",
      "blocks.18.mlp.fc2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_19_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.18.attn.relative_position_bias_table",
      "blocks.18.attn.qkv.weight",
      "blocks.18.attn.proj.weight",
      "blocks.18.mlp.fc1.weight",
      "blocks.18.mlp.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_20_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.19.gamma_1",
      "blocks.19.gamma_2",
      "blocks.19.norm1.weight",
      "blocks.19.norm1.bias",
      "blocks.19.attn.q_bias",
      "blocks.19.attn.v_bias",
      "blocks.19.attn.proj.bias",
      "blocks.19.norm2.weight",
      "blocks.19.norm2.bias",
      "blocks.19.mlp.fc1.bias",
      "blocks.19.mlp.fc2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_20_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.19.attn.relative_position_bias_table",
      "blocks.19.attn.qkv.weight",
      "blocks.19.attn.proj.weight",
      "blocks.19.mlp.fc1.weight",
      "blocks.19.mlp.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_21_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.20.gamma_1",
      "blocks.20.gamma_2",
      "blocks.20.norm1.weight",
      "blocks.20.norm1.bias",
      "blocks.20.attn.q_bias",
      "blocks.20.attn.v_bias",
      "blocks.20.attn.proj.bias",
      "blocks.20.norm2.weight",
      "blocks.20.norm2.bias",
      "blocks.20.mlp.fc1.bias",
      "blocks.20.mlp.fc2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_21_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.20.attn.relative_position_bias_table",
      "blocks.20.attn.qkv.weight",
      "blocks.20.attn.proj.weight",
      "blocks.20.mlp.fc1.weight",
      "blocks.20.mlp.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_22_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.21.gamma_1",
      "blocks.21.gamma_2",
      "blocks.21.norm1.weight",
      "blocks.21.norm1.bias",
      "blocks.21.attn.q_bias",
      "blocks.21.attn.v_bias",
      "blocks.21.attn.proj.bias",
      "blocks.21.norm2.weight",
      "blocks.21.norm2.bias",
      "blocks.21.mlp.fc1.bias",
      "blocks.21.mlp.fc2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_22_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.21.attn.relative_position_bias_table",
      "blocks.21.attn.qkv.weight",
      "blocks.21.attn.proj.weight",
      "blocks.21.mlp.fc1.weight",
      "blocks.21.mlp.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_23_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.22.gamma_1",
      "blocks.22.gamma_2",
      "blocks.22.norm1.weight",
      "blocks.22.norm1.bias",
      "blocks.22.attn.q_bias",
      "blocks.22.attn.v_bias",
      "blocks.22.attn.proj.bias",
      "blocks.22.norm2.weight",
      "blocks.22.norm2.bias",
      "blocks.22.mlp.fc1.bias",
      "blocks.22.mlp.fc2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_23_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.22.attn.relative_position_bias_table",
      "blocks.22.attn.qkv.weight",
      "blocks.22.attn.proj.weight",
      "blocks.22.mlp.fc1.weight",
      "blocks.22.mlp.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_24_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.23.gamma_1",
      "blocks.23.gamma_2",
      "blocks.23.norm1.weight",
      "blocks.23.norm1.bias",
      "blocks.23.attn.q_bias",
      "blocks.23.attn.v_bias",
      "blocks.23.attn.proj.bias",
      "blocks.23.norm2.weight",
      "blocks.23.norm2.bias",
      "blocks.23.mlp.fc1.bias",
      "blocks.23.mlp.fc2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_24_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.23.attn.relative_position_bias_table",
      "blocks.23.attn.qkv.weight",
      "blocks.23.attn.proj.weight",
      "blocks.23.mlp.fc1.weight",
      "blocks.23.mlp.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_25_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_25_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
/home/share/FM_Code/PanDerm/classification/furnace/utils.py:424: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
Use step level LR scheduler!
Set warmup steps = 110
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 50 epochs
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [0]  [ 0/11]  eta: 0:01:03  lr: 0.000000  min_lr: 0.000000  loss: 1.7918 (1.7918)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.9695 (0.9695)  time: 5.7906  data: 3.5323  max mem: 37088
Epoch: [0]  [10/11]  eta: 0:00:01  lr: 0.000046  min_lr: 0.000000  loss: 1.7915 (1.7912)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8382 (0.7836)  time: 1.2080  data: 0.3212  max mem: 39406
Epoch: [0] Total time: 0:00:13 (1.2244 s / it)
2025-04-28 18:25:21 Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 1.7915 (1.7912)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8382 (0.7836)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.9310  data: 3.1066  max mem: 39406
Test:  [1/2]  eta: 0:00:02    time: 2.1676  data: 1.5534  max mem: 39406
Test: Total time: 0:00:04 (2.2572 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.3729 Acc: 0.4302 Recall_macro: 0.3729 Recall_weighted: 0.4302 AUC-ROC: 0.8581 Weighted F1-score: 0.3526
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 0, 'Val Loss': 1.7878738641738892, 'Val BAcc': np.float64(0.3728675168675169), 'Val Acc': 0.43023255813953487, 'Val ROC': np.float64(0.8581267178292508), 'Val W_F1': 0.3525949651496472, 'Val Recall_macro': 0.3728675168675169, 'Val Recall_weighted': 0.43023255813953487}
Max val mean accuracy: 0.43%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [1]  [ 0/11]  eta: 0:00:43  lr: 0.000050  min_lr: 0.000000  loss: 1.7882 (1.7882)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1725 (1.1725)  time: 3.9741  data: 3.2066  max mem: 39406
Epoch: [1]  [10/11]  eta: 0:00:01  lr: 0.000096  min_lr: 0.000000  loss: 1.7833 (1.7823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1743 (1.2213)  time: 1.0522  data: 0.2916  max mem: 39406
Epoch: [1] Total time: 0:00:11 (1.0736 s / it)
2025-04-28 18:25:42 Averaged stats: lr: 0.000096  min_lr: 0.000000  loss: 1.7833 (1.7823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1743 (1.2213)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3439  data: 2.9836  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8150  data: 1.4919  max mem: 39406
Test: Total time: 0:00:03 (1.9300 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5025 Acc: 0.2878 Recall_macro: 0.5025 Recall_weighted: 0.2878 AUC-ROC: 0.8827 Weighted F1-score: 0.2858
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 1, 'Val Loss': 1.7634598016738892, 'Val BAcc': np.float64(0.5025387398290624), 'Val Acc': 0.2877906976744186, 'Val ROC': np.float64(0.8826518135622546), 'Val W_F1': 0.2857953195506727, 'Val Recall_macro': 0.5025387398290624, 'Val Recall_weighted': 0.2877906976744186}
Max val mean accuracy: 0.43%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [2]  [ 0/11]  eta: 0:00:45  lr: 0.000101  min_lr: 0.000000  loss: 1.7592 (1.7592)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3563 (1.3563)  time: 4.1264  data: 3.3659  max mem: 39406
Epoch: [2]  [10/11]  eta: 0:00:01  lr: 0.000147  min_lr: 0.000000  loss: 1.7333 (1.7135)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8357 (1.7744)  time: 1.0732  data: 0.3061  max mem: 39406
Epoch: [2] Total time: 0:00:12 (1.0932 s / it)
2025-04-28 18:25:58 Averaged stats: lr: 0.000147  min_lr: 0.000000  loss: 1.7333 (1.7135)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8357 (1.7744)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4102  data: 3.0457  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8496  data: 1.5229  max mem: 39406
Test: Total time: 0:00:03 (1.9420 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5252 Acc: 0.4709 Recall_macro: 0.5252 Recall_weighted: 0.4709 AUC-ROC: 0.8708 Weighted F1-score: 0.5020
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 2, 'Val Loss': 1.6007529497146606, 'Val BAcc': np.float64(0.5251813422136002), 'Val Acc': 0.47093023255813954, 'Val ROC': np.float64(0.8707628133600909), 'Val W_F1': 0.5019549227809496, 'Val Recall_macro': 0.5251813422136002, 'Val Recall_weighted': 0.47093023255813954}
Max val mean accuracy: 0.47%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [3]  [ 0/11]  eta: 0:00:46  lr: 0.000151  min_lr: 0.000000  loss: 1.6823 (1.6823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0131 (2.0131)  time: 4.2197  data: 3.4512  max mem: 39406
Epoch: [3]  [10/11]  eta: 0:00:01  lr: 0.000197  min_lr: 0.000000  loss: 1.4879 (1.5289)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9687 (1.9766)  time: 1.0869  data: 0.3138  max mem: 39406
Epoch: [3] Total time: 0:00:12 (1.1074 s / it)
2025-04-28 18:26:20 Averaged stats: lr: 0.000197  min_lr: 0.000000  loss: 1.4879 (1.5289)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9687 (1.9766)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4237  data: 3.0517  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8580  data: 1.5259  max mem: 39406
Test: Total time: 0:00:03 (1.9497 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5271 Acc: 0.3866 Recall_macro: 0.5271 Recall_weighted: 0.3866 AUC-ROC: 0.8943 Weighted F1-score: 0.3583
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 3, 'Val Loss': 1.322593331336975, 'Val BAcc': np.float64(0.5271137362750266), 'Val Acc': 0.3866279069767442, 'Val ROC': np.float64(0.8943233586196984), 'Val W_F1': 0.3583329386633155, 'Val Recall_macro': 0.5271137362750266, 'Val Recall_weighted': 0.3866279069767442}
Max val mean accuracy: 0.47%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [4]  [ 0/11]  eta: 0:00:43  lr: 0.000202  min_lr: 0.000000  loss: 1.4084 (1.4084)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5380 (1.5380)  time: 3.9857  data: 3.2133  max mem: 39406
Epoch: [4]  [10/11]  eta: 0:00:01  lr: 0.000248  min_lr: 0.000000  loss: 1.4401 (1.4568)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6765 (1.7491)  time: 1.0725  data: 0.2922  max mem: 39406
Epoch: [4] Total time: 0:00:12 (1.0927 s / it)
2025-04-28 18:26:36 Averaged stats: lr: 0.000248  min_lr: 0.000000  loss: 1.4401 (1.4568)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6765 (1.7491)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4286  data: 3.0504  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8626  data: 1.5253  max mem: 39406
Test: Total time: 0:00:03 (1.9548 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.5882 Acc: 0.6570 Recall_macro: 0.5882 Recall_weighted: 0.6570 AUC-ROC: 0.8960 Weighted F1-score: 0.6450
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 4, 'Val Loss': 1.1074732542037964, 'Val BAcc': np.float64(0.5882122591800011), 'Val Acc': 0.6569767441860465, 'Val ROC': np.float64(0.8959738323252814), 'Val W_F1': 0.6449929296901872, 'Val Recall_macro': 0.5882122591800011, 'Val Recall_weighted': 0.6569767441860465}
Max val mean accuracy: 0.66%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [5]  [ 0/11]  eta: 0:00:45  lr: 0.000252  min_lr: 0.000000  loss: 1.2636 (1.2636)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8995 (1.8995)  time: 4.1728  data: 3.3978  max mem: 39406
Epoch: [5]  [10/11]  eta: 0:00:01  lr: 0.000298  min_lr: 0.000000  loss: 1.3655 (1.4171)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7757 (1.8367)  time: 1.0912  data: 0.3090  max mem: 39406
Epoch: [5] Total time: 0:00:12 (1.1144 s / it)
2025-04-28 18:26:58 Averaged stats: lr: 0.000298  min_lr: 0.000000  loss: 1.3655 (1.4171)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7757 (1.8367)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4914  data: 3.1149  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8941  data: 1.5575  max mem: 39406
Test: Total time: 0:00:04 (2.0008 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6685 Acc: 0.5959 Recall_macro: 0.6685 Recall_weighted: 0.5959 AUC-ROC: 0.9085 Weighted F1-score: 0.6223
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 5, 'Val Loss': 1.0368362665176392, 'Val BAcc': np.float64(0.6684628507854313), 'Val Acc': 0.5959302325581395, 'Val ROC': np.float64(0.908511645955956), 'Val W_F1': 0.6223315161050064, 'Val Recall_macro': 0.6684628507854313, 'Val Recall_weighted': 0.5959302325581395}
Max val mean accuracy: 0.66%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [6]  [ 0/11]  eta: 0:00:43  lr: 0.000303  min_lr: 0.000000  loss: 1.4280 (1.4280)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7663 (1.7663)  time: 3.9917  data: 3.2109  max mem: 39406
Epoch: [6]  [10/11]  eta: 0:00:01  lr: 0.000349  min_lr: 0.000000  loss: 1.3841 (1.3709)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1205 (2.3355)  time: 1.0793  data: 0.2920  max mem: 39406
Epoch: [6] Total time: 0:00:12 (1.1014 s / it)
2025-04-28 18:27:14 Averaged stats: lr: 0.000349  min_lr: 0.000000  loss: 1.3841 (1.3709)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1205 (2.3355)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3852  data: 3.0100  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8416  data: 1.5051  max mem: 39406
Test: Total time: 0:00:03 (1.9574 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6538 Acc: 0.7035 Recall_macro: 0.6538 Recall_weighted: 0.7035 AUC-ROC: 0.9231 Weighted F1-score: 0.6980
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 6, 'Val Loss': 0.9955396056175232, 'Val BAcc': np.float64(0.653765098152195), 'Val Acc': 0.7034883720930233, 'Val ROC': np.float64(0.9231356171124183), 'Val W_F1': 0.6980351482685704, 'Val Recall_macro': 0.653765098152195, 'Val Recall_weighted': 0.7034883720930233}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [7]  [ 0/11]  eta: 0:00:43  lr: 0.000353  min_lr: 0.000000  loss: 1.3784 (1.3784)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0507 (2.0507)  time: 3.9587  data: 3.1746  max mem: 39406
Epoch: [7]  [10/11]  eta: 0:00:01  lr: 0.000399  min_lr: 0.000000  loss: 1.3784 (1.3263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0507 (2.0922)  time: 1.0784  data: 0.2887  max mem: 39406
Epoch: [7] Total time: 0:00:12 (1.0955 s / it)
2025-04-28 18:27:36 Averaged stats: lr: 0.000399  min_lr: 0.000000  loss: 1.3784 (1.3263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0507 (2.0922)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3842  data: 3.0073  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8431  data: 1.5037  max mem: 39406
Test: Total time: 0:00:03 (1.9299 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6506 Acc: 0.5233 Recall_macro: 0.6506 Recall_weighted: 0.5233 AUC-ROC: 0.9311 Weighted F1-score: 0.5547
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 7, 'Val Loss': 1.0241409540176392, 'Val BAcc': np.float64(0.6506393850909981), 'Val Acc': 0.5232558139534884, 'Val ROC': np.float64(0.9310714787347059), 'Val W_F1': 0.5546605494033601, 'Val Recall_macro': 0.6506393850909981, 'Val Recall_weighted': 0.5232558139534884}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [8]  [ 0/11]  eta: 0:00:45  lr: 0.000404  min_lr: 0.000000  loss: 1.3378 (1.3378)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6778 (2.6778)  time: 4.1089  data: 3.3205  max mem: 39406
Epoch: [8]  [10/11]  eta: 0:00:01  lr: 0.000450  min_lr: 0.000000  loss: 1.3429 (1.3703)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8792 (1.8556)  time: 1.0981  data: 0.3019  max mem: 39406
Epoch: [8] Total time: 0:00:12 (1.1201 s / it)
2025-04-28 18:27:52 Averaged stats: lr: 0.000450  min_lr: 0.000000  loss: 1.3429 (1.3703)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8792 (1.8556)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4406  data: 3.0623  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8717  data: 1.5312  max mem: 39406
Test: Total time: 0:00:03 (1.9714 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7056 Acc: 0.7006 Recall_macro: 0.7056 Recall_weighted: 0.7006 AUC-ROC: 0.9314 Weighted F1-score: 0.7147
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 8, 'Val Loss': 0.8987619280815125, 'Val BAcc': np.float64(0.7055605769154156), 'Val Acc': 0.7005813953488372, 'Val ROC': np.float64(0.9313582178055532), 'Val W_F1': 0.7147298661895573, 'Val Recall_macro': 0.7055605769154156, 'Val Recall_weighted': 0.7005813953488372}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [9]  [ 0/11]  eta: 0:00:44  lr: 0.000454  min_lr: 0.000000  loss: 1.0986 (1.0986)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9033 (1.9033)  time: 4.0300  data: 3.2398  max mem: 39406
Epoch: [9]  [10/11]  eta: 0:00:01  lr: 0.000500  min_lr: 0.000000  loss: 1.3878 (1.3658)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0659 (1.9823)  time: 1.0934  data: 0.2946  max mem: 39406
Epoch: [9] Total time: 0:00:12 (1.1165 s / it)
2025-04-28 18:28:09 Averaged stats: lr: 0.000500  min_lr: 0.000000  loss: 1.3878 (1.3658)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0659 (1.9823)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.6017  data: 3.2218  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9525  data: 1.6110  max mem: 39406
Test: Total time: 0:00:04 (2.0598 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6888 Acc: 0.6337 Recall_macro: 0.6888 Recall_weighted: 0.6337 AUC-ROC: 0.9250 Weighted F1-score: 0.6632
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 9, 'Val Loss': 0.9622971415519714, 'Val BAcc': np.float64(0.6888031055772991), 'Val Acc': 0.6337209302325582, 'Val ROC': np.float64(0.9250477914155603), 'Val W_F1': 0.663248867386957, 'Val Recall_macro': 0.6888031055772991, 'Val Recall_weighted': 0.6337209302325582}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [10]  [ 0/11]  eta: 0:00:45  lr: 0.000500  min_lr: 0.000000  loss: 1.1440 (1.1440)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5634 (1.5634)  time: 4.0965  data: 3.3052  max mem: 39406
Epoch: [10]  [10/11]  eta: 0:00:01  lr: 0.000499  min_lr: 0.000000  loss: 1.4338 (1.3995)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1147 (2.1905)  time: 1.1016  data: 0.3005  max mem: 39406
Epoch: [10] Total time: 0:00:12 (1.1220 s / it)
2025-04-28 18:28:25 Averaged stats: lr: 0.000499  min_lr: 0.000000  loss: 1.4338 (1.3995)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1147 (2.1905)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4619  data: 3.0774  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8832  data: 1.5388  max mem: 39406
Test: Total time: 0:00:03 (1.9735 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6943 Acc: 0.6744 Recall_macro: 0.6943 Recall_weighted: 0.6744 AUC-ROC: 0.9290 Weighted F1-score: 0.7018
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 10, 'Val Loss': 0.924159824848175, 'Val BAcc': np.float64(0.6943153190895126), 'Val Acc': 0.6744186046511628, 'Val ROC': np.float64(0.9290409925964176), 'Val W_F1': 0.701816811968726, 'Val Recall_macro': 0.6943153190895126, 'Val Recall_weighted': 0.6744186046511628}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [11]  [ 0/11]  eta: 0:00:45  lr: 0.000499  min_lr: 0.000000  loss: 1.4134 (1.4134)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5671 (1.5671)  time: 4.1535  data: 3.3568  max mem: 39406
Epoch: [11]  [10/11]  eta: 0:00:01  lr: 0.000497  min_lr: 0.000000  loss: 1.3955 (1.3253)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8032 (1.8095)  time: 1.1062  data: 0.3052  max mem: 39406
Epoch: [11] Total time: 0:00:12 (1.1279 s / it)
2025-04-28 18:28:41 Averaged stats: lr: 0.000497  min_lr: 0.000000  loss: 1.3955 (1.3253)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8032 (1.8095)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4120  data: 3.0276  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8596  data: 1.5139  max mem: 39406
Test: Total time: 0:00:03 (1.9510 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6719 Acc: 0.6831 Recall_macro: 0.6719 Recall_weighted: 0.6831 AUC-ROC: 0.9310 Weighted F1-score: 0.7085
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 11, 'Val Loss': 0.9239204525947571, 'Val BAcc': np.float64(0.6719041739686902), 'Val Acc': 0.6831395348837209, 'Val ROC': np.float64(0.930950787379995), 'Val W_F1': 0.7085497674534487, 'Val Recall_macro': 0.6719041739686902, 'Val Recall_weighted': 0.6831395348837209}
Max val mean accuracy: 0.70%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [12]  [ 0/11]  eta: 0:00:44  lr: 0.000497  min_lr: 0.000000  loss: 1.3502 (1.3502)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7790 (1.7790)  time: 4.0154  data: 3.2187  max mem: 39406
Epoch: [12]  [10/11]  eta: 0:00:01  lr: 0.000494  min_lr: 0.000000  loss: 1.1593 (1.1934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7790 (1.7291)  time: 1.0977  data: 0.2927  max mem: 39406
Epoch: [12] Total time: 0:00:12 (1.1182 s / it)
2025-04-28 18:28:58 Averaged stats: lr: 0.000494  min_lr: 0.000000  loss: 1.1593 (1.1934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7790 (1.7291)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.5440  data: 3.1595  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9252  data: 1.5798  max mem: 39406
Test: Total time: 0:00:04 (2.0414 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6625 Acc: 0.7209 Recall_macro: 0.6625 Recall_weighted: 0.7209 AUC-ROC: 0.9388 Weighted F1-score: 0.7366
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 12, 'Val Loss': 0.835507869720459, 'Val BAcc': np.float64(0.6624620565265728), 'Val Acc': 0.7209302325581395, 'Val ROC': np.float64(0.9388255475963021), 'Val W_F1': 0.7366263783434405, 'Val Recall_macro': 0.6624620565265728, 'Val Recall_weighted': 0.7209302325581395}
Max val mean accuracy: 0.72%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [13]  [ 0/11]  eta: 0:00:47  lr: 0.000493  min_lr: 0.000000  loss: 1.5605 (1.5605)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8822 (1.8822)  time: 4.3037  data: 3.5169  max mem: 39406
Epoch: [13]  [10/11]  eta: 0:00:01  lr: 0.000488  min_lr: 0.000000  loss: 1.3639 (1.2604)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9152 (1.8631)  time: 1.1179  data: 0.3198  max mem: 39406
Epoch: [13] Total time: 0:00:12 (1.1358 s / it)
2025-04-28 18:29:20 Averaged stats: lr: 0.000488  min_lr: 0.000000  loss: 1.3639 (1.2604)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9152 (1.8631)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3835  data: 3.0053  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8445  data: 1.5027  max mem: 39406
Test: Total time: 0:00:03 (1.9539 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6322 Acc: 0.7384 Recall_macro: 0.6322 Recall_weighted: 0.7384 AUC-ROC: 0.9365 Weighted F1-score: 0.7433
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 13, 'Val Loss': 0.7887508869171143, 'Val BAcc': np.float64(0.6322273408725022), 'Val Acc': 0.7383720930232558, 'Val ROC': np.float64(0.9365391549263555), 'Val W_F1': 0.7433490246517096, 'Val Recall_macro': 0.6322273408725022, 'Val Recall_weighted': 0.7383720930232558}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [14]  [ 0/11]  eta: 0:00:46  lr: 0.000488  min_lr: 0.000000  loss: 1.1635 (1.1635)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8561 (1.8561)  time: 4.2179  data: 3.4270  max mem: 39406
Epoch: [14]  [10/11]  eta: 0:00:01  lr: 0.000482  min_lr: 0.000000  loss: 1.1108 (1.1295)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9752 (2.1131)  time: 1.1100  data: 0.3116  max mem: 39406
Epoch: [14] Total time: 0:00:12 (1.1313 s / it)
2025-04-28 18:29:43 Averaged stats: lr: 0.000482  min_lr: 0.000000  loss: 1.1108 (1.1295)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9752 (2.1131)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3575  data: 2.9739  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8306  data: 1.4870  max mem: 39406
Test: Total time: 0:00:03 (1.9250 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6853 Acc: 0.6744 Recall_macro: 0.6853 Recall_weighted: 0.6744 AUC-ROC: 0.9387 Weighted F1-score: 0.7000
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 14, 'Val Loss': 0.8104360699653625, 'Val BAcc': np.float64(0.685323906227132), 'Val Acc': 0.6744186046511628, 'Val ROC': np.float64(0.9386737038252974), 'Val W_F1': 0.7000034559451316, 'Val Recall_macro': 0.685323906227132, 'Val Recall_weighted': 0.6744186046511628}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [15]  [ 0/11]  eta: 0:00:45  lr: 0.000481  min_lr: 0.000000  loss: 1.5588 (1.5588)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5062 (3.5062)  time: 4.1079  data: 3.3143  max mem: 39406
Epoch: [15]  [10/11]  eta: 0:00:01  lr: 0.000474  min_lr: 0.000000  loss: 1.5060 (1.4263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8664 (2.2226)  time: 1.1011  data: 0.3014  max mem: 39406
Epoch: [15] Total time: 0:00:12 (1.1220 s / it)
2025-04-28 18:29:59 Averaged stats: lr: 0.000474  min_lr: 0.000000  loss: 1.5060 (1.4263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8664 (2.2226)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4244  data: 3.0429  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8660  data: 1.5215  max mem: 39406
Test: Total time: 0:00:03 (1.9610 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7313 Acc: 0.6570 Recall_macro: 0.7313 Recall_weighted: 0.6570 AUC-ROC: 0.9397 Weighted F1-score: 0.6917
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 15, 'Val Loss': 0.8888662457466125, 'Val BAcc': np.float64(0.7313448110867466), 'Val Acc': 0.6569767441860465, 'Val ROC': np.float64(0.9396880354059344), 'Val W_F1': 0.6917080232738627, 'Val Recall_macro': 0.7313448110867466, 'Val Recall_weighted': 0.6569767441860465}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [16]  [ 0/11]  eta: 0:00:45  lr: 0.000473  min_lr: 0.000000  loss: 1.2632 (1.2632)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8135 (1.8135)  time: 4.1809  data: 3.3856  max mem: 39406
Epoch: [16]  [10/11]  eta: 0:00:01  lr: 0.000464  min_lr: 0.000000  loss: 1.3736 (1.3449)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7611 (1.8175)  time: 1.1128  data: 0.3079  max mem: 39406
Epoch: [16] Total time: 0:00:12 (1.1316 s / it)
2025-04-28 18:30:15 Averaged stats: lr: 0.000464  min_lr: 0.000000  loss: 1.3736 (1.3449)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7611 (1.8175)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4133  data: 3.0264  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8597  data: 1.5133  max mem: 39406
Test: Total time: 0:00:03 (1.9736 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6799 Acc: 0.6395 Recall_macro: 0.6799 Recall_weighted: 0.6395 AUC-ROC: 0.9391 Weighted F1-score: 0.6761
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 16, 'Val Loss': 0.88738614320755, 'Val BAcc': np.float64(0.6798857743373873), 'Val Acc': 0.6395348837209303, 'Val ROC': np.float64(0.9390851899969359), 'Val W_F1': 0.6761173159214264, 'Val Recall_macro': 0.6798857743373873, 'Val Recall_weighted': 0.6395348837209303}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [17]  [ 0/11]  eta: 0:00:45  lr: 0.000463  min_lr: 0.000000  loss: 1.3015 (1.3015)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8954 (1.8954)  time: 4.1607  data: 3.3648  max mem: 39406
Epoch: [17]  [10/11]  eta: 0:00:01  lr: 0.000453  min_lr: 0.000000  loss: 1.3224 (1.2776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8038 (1.7905)  time: 1.1097  data: 0.3060  max mem: 39406
Epoch: [17] Total time: 0:00:12 (1.1304 s / it)
2025-04-28 18:30:32 Averaged stats: lr: 0.000453  min_lr: 0.000000  loss: 1.3224 (1.2776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8038 (1.7905)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3913  data: 3.0056  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8488  data: 1.5029  max mem: 39406
Test: Total time: 0:00:03 (1.9599 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6744 Acc: 0.7238 Recall_macro: 0.6744 Recall_weighted: 0.7238 AUC-ROC: 0.9386 Weighted F1-score: 0.7358
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 17, 'Val Loss': 0.8087061643600464, 'Val BAcc': np.float64(0.6743970631712567), 'Val Acc': 0.7238372093023255, 'Val ROC': np.float64(0.9385927645832041), 'Val W_F1': 0.7358093357763097, 'Val Recall_macro': 0.6743970631712567, 'Val Recall_weighted': 0.7238372093023255}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [18]  [ 0/11]  eta: 0:00:47  lr: 0.000452  min_lr: 0.000000  loss: 1.4416 (1.4416)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3193 (2.3193)  time: 4.2736  data: 3.4762  max mem: 39406
Epoch: [18]  [10/11]  eta: 0:00:01  lr: 0.000441  min_lr: 0.000000  loss: 1.4136 (1.3456)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9698 (2.1563)  time: 1.1222  data: 0.3161  max mem: 39406
Epoch: [18] Total time: 0:00:12 (1.1438 s / it)
2025-04-28 18:30:48 Averaged stats: lr: 0.000441  min_lr: 0.000000  loss: 1.4136 (1.3456)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9698 (2.1563)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4244  data: 3.0390  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8662  data: 1.5196  max mem: 39406
Test: Total time: 0:00:03 (1.9742 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6275 Acc: 0.7442 Recall_macro: 0.6275 Recall_weighted: 0.7442 AUC-ROC: 0.9376 Weighted F1-score: 0.7352
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 18, 'Val Loss': 0.8054231405258179, 'Val BAcc': np.float64(0.6275151599022567), 'Val Acc': 0.7441860465116279, 'Val ROC': np.float64(0.93763740543713), 'Val W_F1': 0.7351785275774051, 'Val Recall_macro': 0.6275151599022567, 'Val Recall_weighted': 0.7441860465116279}
Max val mean accuracy: 0.74%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [19]  [ 0/11]  eta: 0:00:46  lr: 0.000440  min_lr: 0.000000  loss: 1.3818 (1.3818)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5830 (1.5830)  time: 4.2364  data: 3.4436  max mem: 39406
Epoch: [19]  [10/11]  eta: 0:00:01  lr: 0.000428  min_lr: 0.000000  loss: 1.3818 (1.3332)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8478 (1.8297)  time: 1.1140  data: 0.3131  max mem: 39406
Epoch: [19] Total time: 0:00:12 (1.1314 s / it)
2025-04-28 18:31:11 Averaged stats: lr: 0.000428  min_lr: 0.000000  loss: 1.3818 (1.3332)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8478 (1.8297)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3954  data: 3.0109  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8503  data: 1.5055  max mem: 39406
Test: Total time: 0:00:03 (1.9485 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6996 Acc: 0.7703 Recall_macro: 0.6996 Recall_weighted: 0.7703 AUC-ROC: 0.9374 Weighted F1-score: 0.7738
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 19, 'Val Loss': 0.7549920082092285, 'Val BAcc': np.float64(0.6995616362067976), 'Val Acc': 0.7703488372093024, 'Val ROC': np.float64(0.9373889869276826), 'Val W_F1': 0.7737630446913282, 'Val Recall_macro': 0.6995616362067976, 'Val Recall_weighted': 0.7703488372093024}
Max val mean accuracy: 0.77%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [20]  [ 0/11]  eta: 0:00:46  lr: 0.000427  min_lr: 0.000000  loss: 1.4756 (1.4756)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8850 (1.8850)  time: 4.2309  data: 3.4410  max mem: 39406
Epoch: [20]  [10/11]  eta: 0:00:01  lr: 0.000414  min_lr: 0.000000  loss: 1.2796 (1.2638)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8387 (1.9164)  time: 1.1108  data: 0.3129  max mem: 39406
Epoch: [20] Total time: 0:00:12 (1.1343 s / it)
2025-04-28 18:31:33 Averaged stats: lr: 0.000414  min_lr: 0.000000  loss: 1.2796 (1.2638)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8387 (1.9164)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4186  data: 3.0426  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8608  data: 1.5214  max mem: 39406
Test: Total time: 0:00:03 (1.9510 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7326 Acc: 0.7326 Recall_macro: 0.7326 Recall_weighted: 0.7326 AUC-ROC: 0.9338 Weighted F1-score: 0.7449
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 20, 'Val Loss': 0.7992039918899536, 'Val BAcc': np.float64(0.7325680468906276), 'Val Acc': 0.7325581395348837, 'Val ROC': np.float64(0.9337525065310094), 'Val W_F1': 0.74493990037605, 'Val Recall_macro': 0.7325680468906276, 'Val Recall_weighted': 0.7325581395348837}
Max val mean accuracy: 0.77%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [21]  [ 0/11]  eta: 0:00:44  lr: 0.000413  min_lr: 0.000000  loss: 1.3107 (1.3107)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5047 (1.5047)  time: 4.0839  data: 3.2902  max mem: 39406
Epoch: [21]  [10/11]  eta: 0:00:01  lr: 0.000399  min_lr: 0.000000  loss: 1.0313 (1.1168)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7237 (1.8153)  time: 1.1008  data: 0.2992  max mem: 39406
Epoch: [21] Total time: 0:00:12 (1.1221 s / it)
2025-04-28 18:31:50 Averaged stats: lr: 0.000399  min_lr: 0.000000  loss: 1.0313 (1.1168)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7237 (1.8153)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3748  data: 2.9878  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8400  data: 1.4940  max mem: 39406
Test: Total time: 0:00:03 (1.9541 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7153 Acc: 0.7587 Recall_macro: 0.7153 Recall_weighted: 0.7587 AUC-ROC: 0.9363 Weighted F1-score: 0.7656
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 21, 'Val Loss': 0.7212295532226562, 'Val BAcc': np.float64(0.7152841974777459), 'Val Acc': 0.7587209302325582, 'Val ROC': np.float64(0.9363279848658926), 'Val W_F1': 0.7656172933859684, 'Val Recall_macro': 0.7152841974777459, 'Val Recall_weighted': 0.7587209302325582}
Max val mean accuracy: 0.77%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [22]  [ 0/11]  eta: 0:00:44  lr: 0.000397  min_lr: 0.000000  loss: 1.1988 (1.1988)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0695 (2.0695)  time: 4.0766  data: 3.2814  max mem: 39406
Epoch: [22]  [10/11]  eta: 0:00:01  lr: 0.000382  min_lr: 0.000000  loss: 1.1983 (1.1596)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0695 (1.9769)  time: 1.1044  data: 0.2984  max mem: 39406
Epoch: [22] Total time: 0:00:12 (1.1269 s / it)
2025-04-28 18:32:06 Averaged stats: lr: 0.000382  min_lr: 0.000000  loss: 1.1983 (1.1596)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0695 (1.9769)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4058  data: 3.0167  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8572  data: 1.5084  max mem: 39406
Test: Total time: 0:00:03 (1.9558 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6863 Acc: 0.7849 Recall_macro: 0.6863 Recall_weighted: 0.7849 AUC-ROC: 0.9485 Weighted F1-score: 0.7774
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 22, 'Val Loss': 0.6851091980934143, 'Val BAcc': np.float64(0.6862663920728438), 'Val Acc': 0.7848837209302325, 'Val ROC': np.float64(0.9484976378320252), 'Val W_F1': 0.7774468717107201, 'Val Recall_macro': 0.6862663920728438, 'Val Recall_weighted': 0.7848837209302325}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [23]  [ 0/11]  eta: 0:00:45  lr: 0.000381  min_lr: 0.000000  loss: 1.4971 (1.4971)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3544 (2.3544)  time: 4.1394  data: 3.3487  max mem: 39406
Epoch: [23]  [10/11]  eta: 0:00:01  lr: 0.000365  min_lr: 0.000000  loss: 1.3837 (1.2844)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8268 (1.8887)  time: 1.1059  data: 0.3045  max mem: 39406
Epoch: [23] Total time: 0:00:12 (1.1280 s / it)
2025-04-28 18:32:28 Averaged stats: lr: 0.000365  min_lr: 0.000000  loss: 1.3837 (1.2844)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8268 (1.8887)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:08    time: 4.0431  data: 3.6537  max mem: 39406
Test:  [1/2]  eta: 0:00:02    time: 2.1736  data: 1.8269  max mem: 39406
Test: Total time: 0:00:04 (2.2789 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6835 Acc: 0.7762 Recall_macro: 0.6835 Recall_weighted: 0.7762 AUC-ROC: 0.9399 Weighted F1-score: 0.7711
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 23, 'Val Loss': 0.6952558755874634, 'Val BAcc': np.float64(0.683516265322717), 'Val Acc': 0.7761627906976745, 'Val ROC': np.float64(0.9398559944140779), 'Val W_F1': 0.7710503939542627, 'Val Recall_macro': 0.683516265322717, 'Val Recall_weighted': 0.7761627906976745}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [24]  [ 0/11]  eta: 0:00:44  lr: 0.000364  min_lr: 0.000000  loss: 0.9386 (0.9386)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7279 (1.7279)  time: 4.0131  data: 3.2138  max mem: 39406
Epoch: [24]  [10/11]  eta: 0:00:01  lr: 0.000348  min_lr: 0.000000  loss: 1.0771 (1.0887)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7160 (1.7608)  time: 1.0974  data: 0.2923  max mem: 39406
Epoch: [24] Total time: 0:00:12 (1.1207 s / it)
2025-04-28 18:32:45 Averaged stats: lr: 0.000348  min_lr: 0.000000  loss: 1.0771 (1.0887)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7160 (1.7608)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.6857  data: 3.2977  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9970  data: 1.6489  max mem: 39406
Test: Total time: 0:00:04 (2.1121 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7140 Acc: 0.7762 Recall_macro: 0.7140 Recall_weighted: 0.7762 AUC-ROC: 0.9397 Weighted F1-score: 0.7772
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 24, 'Val Loss': 0.7086189389228821, 'Val BAcc': np.float64(0.713969297324136), 'Val Acc': 0.7761627906976745, 'Val ROC': np.float64(0.9397282866444181), 'Val W_F1': 0.7771665714502558, 'Val Recall_macro': 0.713969297324136, 'Val Recall_weighted': 0.7761627906976745}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [25]  [ 0/11]  eta: 0:00:47  lr: 0.000346  min_lr: 0.000000  loss: 0.8182 (0.8182)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8972 (1.8972)  time: 4.3350  data: 3.5344  max mem: 39406
Epoch: [25]  [10/11]  eta: 0:00:01  lr: 0.000329  min_lr: 0.000000  loss: 1.0938 (1.0399)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6716 (1.7994)  time: 1.1287  data: 0.3214  max mem: 39406
Epoch: [25] Total time: 0:00:12 (1.1509 s / it)
2025-04-28 18:33:02 Averaged stats: lr: 0.000329  min_lr: 0.000000  loss: 1.0938 (1.0399)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6716 (1.7994)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.5755  data: 3.1883  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9423  data: 1.5942  max mem: 39406
Test: Total time: 0:00:04 (2.0422 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6739 Acc: 0.7558 Recall_macro: 0.6739 Recall_weighted: 0.7558 AUC-ROC: 0.9367 Weighted F1-score: 0.7572
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 25, 'Val Loss': 0.6925791501998901, 'Val BAcc': np.float64(0.6739151585603199), 'Val Acc': 0.7558139534883721, 'Val ROC': np.float64(0.9367436564245807), 'Val W_F1': 0.757245731468876, 'Val Recall_macro': 0.6739151585603199, 'Val Recall_weighted': 0.7558139534883721}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [26]  [ 0/11]  eta: 0:00:44  lr: 0.000328  min_lr: 0.000000  loss: 1.3804 (1.3804)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9095 (2.9095)  time: 4.0087  data: 3.2065  max mem: 39406
Epoch: [26]  [10/11]  eta: 0:00:01  lr: 0.000310  min_lr: 0.000000  loss: 1.2130 (1.1595)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0015 (2.0258)  time: 1.1022  data: 0.2916  max mem: 39406
Epoch: [26] Total time: 0:00:12 (1.1256 s / it)
2025-04-28 18:33:19 Averaged stats: lr: 0.000310  min_lr: 0.000000  loss: 1.2130 (1.1595)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0015 (2.0258)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.5247  data: 3.1348  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9159  data: 1.5675  max mem: 39406
Test: Total time: 0:00:04 (2.0282 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7383 Acc: 0.7645 Recall_macro: 0.7383 Recall_weighted: 0.7645 AUC-ROC: 0.9365 Weighted F1-score: 0.7791
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 26, 'Val Loss': 0.6904481649398804, 'Val BAcc': np.float64(0.7383114777953487), 'Val Acc': 0.7645348837209303, 'Val ROC': np.float64(0.9364933558189342), 'Val W_F1': 0.7790835898428301, 'Val Recall_macro': 0.7383114777953487, 'Val Recall_weighted': 0.7645348837209303}
Max val mean accuracy: 0.78%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [27]  [ 0/11]  eta: 0:00:46  lr: 0.000309  min_lr: 0.000000  loss: 1.1197 (1.1197)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6262 (1.6262)  time: 4.2129  data: 3.4118  max mem: 39406
Epoch: [27]  [10/11]  eta: 0:00:01  lr: 0.000291  min_lr: 0.000000  loss: 1.0335 (1.0943)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7086 (1.7738)  time: 1.1203  data: 0.3103  max mem: 39406
Epoch: [27] Total time: 0:00:12 (1.1415 s / it)
2025-04-28 18:33:35 Averaged stats: lr: 0.000291  min_lr: 0.000000  loss: 1.0335 (1.0943)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7086 (1.7738)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.8097  data: 3.4247  max mem: 39406
Test:  [1/2]  eta: 0:00:02    time: 2.0598  data: 1.7124  max mem: 39406
Test: Total time: 0:00:04 (2.1536 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6838 Acc: 0.7907 Recall_macro: 0.6838 Recall_weighted: 0.7907 AUC-ROC: 0.9426 Weighted F1-score: 0.7863
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 27, 'Val Loss': 0.6395235657691956, 'Val BAcc': np.float64(0.6838129256193772), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.942559748862303), 'Val W_F1': 0.7862672526406536, 'Val Recall_macro': 0.6838129256193772, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.79%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [28]  [ 0/11]  eta: 0:00:44  lr: 0.000290  min_lr: 0.000000  loss: 1.2026 (1.2026)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5280 (1.5280)  time: 4.0785  data: 3.2835  max mem: 39406
Epoch: [28]  [10/11]  eta: 0:00:01  lr: 0.000272  min_lr: 0.000000  loss: 1.1233 (1.1011)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8072 (1.9222)  time: 1.1012  data: 0.2986  max mem: 39406
Epoch: [28] Total time: 0:00:12 (1.1226 s / it)
2025-04-28 18:33:59 Averaged stats: lr: 0.000272  min_lr: 0.000000  loss: 1.1233 (1.1011)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8072 (1.9222)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.5107  data: 3.1258  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9071  data: 1.5630  max mem: 39406
Test: Total time: 0:00:04 (2.0001 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6890 Acc: 0.7849 Recall_macro: 0.6890 Recall_weighted: 0.7849 AUC-ROC: 0.9420 Weighted F1-score: 0.7732
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 28, 'Val Loss': 0.6554075479507446, 'Val BAcc': np.float64(0.6890234147008342), 'Val Acc': 0.7848837209302325, 'Val ROC': np.float64(0.9419585557987924), 'Val W_F1': 0.7731530243492658, 'Val Recall_macro': 0.6890234147008342, 'Val Recall_weighted': 0.7848837209302325}
Max val mean accuracy: 0.79%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [29]  [ 0/11]  eta: 0:00:43  lr: 0.000270  min_lr: 0.000000  loss: 0.6663 (0.6663)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9647 (1.9647)  time: 3.9875  data: 3.1898  max mem: 39406
Epoch: [29]  [10/11]  eta: 0:00:01  lr: 0.000252  min_lr: 0.000000  loss: 1.1741 (1.0846)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9989 (2.0350)  time: 1.0953  data: 0.2901  max mem: 39406
Epoch: [29] Total time: 0:00:12 (1.1163 s / it)
2025-04-28 18:34:15 Averaged stats: lr: 0.000252  min_lr: 0.000000  loss: 1.1741 (1.0846)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9989 (2.0350)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4132  data: 3.0291  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8593  data: 1.5146  max mem: 39406
Test: Total time: 0:00:03 (1.9485 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6795 Acc: 0.7907 Recall_macro: 0.6795 Recall_weighted: 0.7907 AUC-ROC: 0.9413 Weighted F1-score: 0.7809
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 29, 'Val Loss': 0.6254999041557312, 'Val BAcc': np.float64(0.6794614438485406), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9413280332114375), 'Val W_F1': 0.7808573073623966, 'Val Recall_macro': 0.6794614438485406, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.79%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [30]  [ 0/11]  eta: 0:00:44  lr: 0.000251  min_lr: 0.000000  loss: 1.1917 (1.1917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7446 (1.7446)  time: 4.0256  data: 3.2275  max mem: 39406
Epoch: [30]  [10/11]  eta: 0:00:01  lr: 0.000233  min_lr: 0.000000  loss: 1.3071 (1.1499)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0973 (2.0359)  time: 1.1005  data: 0.2935  max mem: 39406
Epoch: [30] Total time: 0:00:12 (1.1216 s / it)
2025-04-28 18:34:32 Averaged stats: lr: 0.000233  min_lr: 0.000000  loss: 1.3071 (1.1499)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0973 (2.0359)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4100  data: 3.0248  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8584  data: 1.5125  max mem: 39406
Test: Total time: 0:00:03 (1.9502 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7019 Acc: 0.7965 Recall_macro: 0.7019 Recall_weighted: 0.7965 AUC-ROC: 0.9372 Weighted F1-score: 0.7926
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 30, 'Val Loss': 0.6306794881820679, 'Val BAcc': np.float64(0.7018968931226995), 'Val Acc': 0.7965116279069767, 'Val ROC': np.float64(0.9371971804947762), 'Val W_F1': 0.7926429658097575, 'Val Recall_macro': 0.7018968931226995, 'Val Recall_weighted': 0.7965116279069767}
Max val mean accuracy: 0.80%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [31]  [ 0/11]  eta: 0:00:46  lr: 0.000231  min_lr: 0.000000  loss: 1.5113 (1.5113)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3730 (2.3730)  time: 4.1933  data: 3.4001  max mem: 39406
Epoch: [31]  [10/11]  eta: 0:00:01  lr: 0.000213  min_lr: 0.000000  loss: 1.3063 (1.2372)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7165 (1.7257)  time: 1.1099  data: 0.3092  max mem: 39406
Epoch: [31] Total time: 0:00:12 (1.1309 s / it)
2025-04-28 18:34:54 Averaged stats: lr: 0.000213  min_lr: 0.000000  loss: 1.3063 (1.2372)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7165 (1.7257)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4175  data: 3.0325  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8613  data: 1.5163  max mem: 39406
Test: Total time: 0:00:03 (1.9557 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7110 Acc: 0.8081 Recall_macro: 0.7110 Recall_weighted: 0.8081 AUC-ROC: 0.9337 Weighted F1-score: 0.7957
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 31, 'Val Loss': 0.6454062461853027, 'Val BAcc': np.float64(0.711009764687184), 'Val Acc': 0.8081395348837209, 'Val ROC': np.float64(0.9336513520586244), 'Val W_F1': 0.7957070953709217, 'Val Recall_macro': 0.711009764687184, 'Val Recall_weighted': 0.8081395348837209}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [32]  [ 0/11]  eta: 0:00:45  lr: 0.000211  min_lr: 0.000000  loss: 1.2541 (1.2541)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8465 (1.8465)  time: 4.1212  data: 3.3323  max mem: 39406
Epoch: [32]  [10/11]  eta: 0:00:01  lr: 0.000194  min_lr: 0.000000  loss: 1.2426 (1.2282)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7692 (1.8360)  time: 1.1010  data: 0.3030  max mem: 39406
Epoch: [32] Total time: 0:00:12 (1.1213 s / it)
2025-04-28 18:35:17 Averaged stats: lr: 0.000194  min_lr: 0.000000  loss: 1.2426 (1.2282)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7692 (1.8360)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4639  data: 3.0811  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8845  data: 1.5406  max mem: 39406
Test: Total time: 0:00:03 (1.9821 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6920 Acc: 0.7849 Recall_macro: 0.6920 Recall_weighted: 0.7849 AUC-ROC: 0.9304 Weighted F1-score: 0.7784
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 32, 'Val Loss': 0.6590784788131714, 'Val BAcc': np.float64(0.6920370391338134), 'Val Acc': 0.7848837209302325, 'Val ROC': np.float64(0.9304332587199081), 'Val W_F1': 0.7783552792816296, 'Val Recall_macro': 0.6920370391338134, 'Val Recall_weighted': 0.7848837209302325}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [33]  [ 0/11]  eta: 0:00:46  lr: 0.000192  min_lr: 0.000000  loss: 0.7418 (0.7418)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9422 (1.9422)  time: 4.1872  data: 3.3956  max mem: 39406
Epoch: [33]  [10/11]  eta: 0:00:01  lr: 0.000175  min_lr: 0.000000  loss: 1.0138 (1.0363)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8277 (1.8309)  time: 1.1116  data: 0.3088  max mem: 39406
Epoch: [33] Total time: 0:00:12 (1.1331 s / it)
2025-04-28 18:35:33 Averaged stats: lr: 0.000175  min_lr: 0.000000  loss: 1.0138 (1.0363)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8277 (1.8309)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4756  data: 3.0878  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8892  data: 1.5440  max mem: 39406
Test: Total time: 0:00:03 (1.9796 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7025 Acc: 0.7878 Recall_macro: 0.7025 Recall_weighted: 0.7878 AUC-ROC: 0.9332 Weighted F1-score: 0.7782
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 33, 'Val Loss': 0.665116548538208, 'Val BAcc': np.float64(0.7024590388461357), 'Val Acc': 0.7877906976744186, 'Val ROC': np.float64(0.9331863204438404), 'Val W_F1': 0.7782051806129772, 'Val Recall_macro': 0.7024590388461357, 'Val Recall_weighted': 0.7877906976744186}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [34]  [ 0/11]  eta: 0:00:44  lr: 0.000173  min_lr: 0.000000  loss: 1.0709 (1.0709)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9357 (1.9357)  time: 4.0664  data: 3.2685  max mem: 39406
Epoch: [34]  [10/11]  eta: 0:00:01  lr: 0.000157  min_lr: 0.000000  loss: 1.1707 (1.0967)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7858 (1.7715)  time: 1.1043  data: 0.2972  max mem: 39406
Epoch: [34] Total time: 0:00:12 (1.1251 s / it)
2025-04-28 18:35:50 Averaged stats: lr: 0.000157  min_lr: 0.000000  loss: 1.1707 (1.0967)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7858 (1.7715)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.6459  data: 3.2603  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9768  data: 1.6302  max mem: 39406
Test: Total time: 0:00:04 (2.0895 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7270 Acc: 0.7878 Recall_macro: 0.7270 Recall_weighted: 0.7878 AUC-ROC: 0.9349 Weighted F1-score: 0.7889
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 34, 'Val Loss': 0.6804496645927429, 'Val BAcc': np.float64(0.7270057036508649), 'Val Acc': 0.7877906976744186, 'Val ROC': np.float64(0.9349060569670399), 'Val W_F1': 0.7888657095351266, 'Val Recall_macro': 0.7270057036508649, 'Val Recall_weighted': 0.7877906976744186}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [35]  [ 0/11]  eta: 0:00:46  lr: 0.000155  min_lr: 0.000000  loss: 1.4333 (1.4333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5871 (2.5871)  time: 4.2386  data: 3.4393  max mem: 39406
Epoch: [35]  [10/11]  eta: 0:00:01  lr: 0.000139  min_lr: 0.000000  loss: 1.2324 (1.1625)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9368 (1.9269)  time: 1.1221  data: 0.3128  max mem: 39406
Epoch: [35] Total time: 0:00:12 (1.1434 s / it)
2025-04-28 18:36:07 Averaged stats: lr: 0.000139  min_lr: 0.000000  loss: 1.2324 (1.1625)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9368 (1.9269)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.5281  data: 3.1410  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9190  data: 1.5706  max mem: 39406
Test: Total time: 0:00:04 (2.0171 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7102 Acc: 0.7965 Recall_macro: 0.7102 Recall_weighted: 0.7965 AUC-ROC: 0.9407 Weighted F1-score: 0.7863
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 35, 'Val Loss': 0.6628646850585938, 'Val BAcc': np.float64(0.7101762105633074), 'Val Acc': 0.7965116279069767, 'Val ROC': np.float64(0.9407224253116068), 'Val W_F1': 0.786259183787854, 'Val Recall_macro': 0.7101762105633074, 'Val Recall_weighted': 0.7965116279069767}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [36]  [ 0/11]  eta: 0:00:46  lr: 0.000137  min_lr: 0.000000  loss: 0.9998 (0.9998)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6820 (1.6820)  time: 4.2679  data: 3.4670  max mem: 39406
Epoch: [36]  [10/11]  eta: 0:00:01  lr: 0.000122  min_lr: 0.000000  loss: 1.1427 (1.1174)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7449 (1.7173)  time: 1.1240  data: 0.3153  max mem: 39406
Epoch: [36] Total time: 0:00:12 (1.1458 s / it)
2025-04-28 18:36:23 Averaged stats: lr: 0.000122  min_lr: 0.000000  loss: 1.1427 (1.1174)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7449 (1.7173)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4146  data: 3.0255  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8619  data: 1.5128  max mem: 39406
Test: Total time: 0:00:03 (1.9677 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6924 Acc: 0.7791 Recall_macro: 0.6924 Recall_weighted: 0.7791 AUC-ROC: 0.9424 Weighted F1-score: 0.7717
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 36, 'Val Loss': 0.6589644551277161, 'Val BAcc': np.float64(0.6923845034812777), 'Val Acc': 0.7790697674418605, 'Val ROC': np.float64(0.9424005940861364), 'Val W_F1': 0.7716515500775206, 'Val Recall_macro': 0.6923845034812777, 'Val Recall_weighted': 0.7790697674418605}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [37]  [ 0/11]  eta: 0:00:47  lr: 0.000120  min_lr: 0.000000  loss: 1.1688 (1.1688)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4788 (1.4788)  time: 4.2811  data: 3.4803  max mem: 39406
Epoch: [37]  [10/11]  eta: 0:00:01  lr: 0.000105  min_lr: 0.000000  loss: 0.9836 (1.0113)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6483 (1.7023)  time: 1.1264  data: 0.3165  max mem: 39406
Epoch: [37] Total time: 0:00:12 (1.1477 s / it)
2025-04-28 18:36:40 Averaged stats: lr: 0.000105  min_lr: 0.000000  loss: 0.9836 (1.0113)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6483 (1.7023)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4549  data: 3.0666  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8818  data: 1.5334  max mem: 39406
Test: Total time: 0:00:03 (1.9669 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6931 Acc: 0.7907 Recall_macro: 0.6931 Recall_weighted: 0.7907 AUC-ROC: 0.9426 Weighted F1-score: 0.7774
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 37, 'Val Loss': 0.6582894325256348, 'Val BAcc': np.float64(0.6930603867378061), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9425737196059844), 'Val W_F1': 0.7773722661309649, 'Val Recall_macro': 0.6930603867378061, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [38]  [ 0/11]  eta: 0:00:44  lr: 0.000104  min_lr: 0.000000  loss: 0.8574 (0.8574)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4298 (1.4298)  time: 4.0527  data: 3.2523  max mem: 39406
Epoch: [38]  [10/11]  eta: 0:00:01  lr: 0.000090  min_lr: 0.000000  loss: 0.9924 (1.0155)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0342 (1.9061)  time: 1.1062  data: 0.2957  max mem: 39406
Epoch: [38] Total time: 0:00:12 (1.1278 s / it)
2025-04-28 18:36:56 Averaged stats: lr: 0.000090  min_lr: 0.000000  loss: 0.9924 (1.0155)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0342 (1.9061)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4258  data: 3.0404  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8677  data: 1.5203  max mem: 39406
Test: Total time: 0:00:03 (1.9705 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7255 Acc: 0.8023 Recall_macro: 0.7255 Recall_weighted: 0.8023 AUC-ROC: 0.9417 Weighted F1-score: 0.7973
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 38, 'Val Loss': 0.64202481508255, 'Val BAcc': np.float64(0.7254659166917232), 'Val Acc': 0.8023255813953488, 'Val ROC': np.float64(0.9416638898987312), 'Val W_F1': 0.7973015227855039, 'Val Recall_macro': 0.7254659166917232, 'Val Recall_weighted': 0.8023255813953488}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [39]  [ 0/11]  eta: 0:00:45  lr: 0.000088  min_lr: 0.000000  loss: 1.4084 (1.4084)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9562 (1.9562)  time: 4.1573  data: 3.3557  max mem: 39406
Epoch: [39]  [10/11]  eta: 0:00:01  lr: 0.000075  min_lr: 0.000000  loss: 1.1395 (1.0914)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9562 (1.8503)  time: 1.1162  data: 0.3051  max mem: 39406
Epoch: [39] Total time: 0:00:12 (1.1379 s / it)
2025-04-28 18:37:13 Averaged stats: lr: 0.000075  min_lr: 0.000000  loss: 1.1395 (1.0914)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9562 (1.8503)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.3861  data: 2.9983  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8483  data: 1.4992  max mem: 39406
Test: Total time: 0:00:03 (1.9364 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7042 Acc: 0.7994 Recall_macro: 0.7042 Recall_weighted: 0.7994 AUC-ROC: 0.9392 Weighted F1-score: 0.7856
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 39, 'Val Loss': 0.6393416523933411, 'Val BAcc': np.float64(0.7042061298835492), 'Val Acc': 0.7994186046511628, 'Val ROC': np.float64(0.9392110295578183), 'Val W_F1': 0.7855531741797268, 'Val Recall_macro': 0.7042061298835492, 'Val Recall_weighted': 0.7994186046511628}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [40]  [ 0/11]  eta: 0:00:43  lr: 0.000074  min_lr: 0.000000  loss: 0.8726 (0.8726)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5767 (1.5767)  time: 3.9923  data: 3.1885  max mem: 39406
Epoch: [40]  [10/11]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000000  loss: 1.0969 (1.0304)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7249 (1.7090)  time: 1.1010  data: 0.2900  max mem: 39406
Epoch: [40] Total time: 0:00:12 (1.1211 s / it)
2025-04-28 18:37:29 Averaged stats: lr: 0.000062  min_lr: 0.000000  loss: 1.0969 (1.0304)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7249 (1.7090)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4285  data: 3.0389  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8689  data: 1.5195  max mem: 39406
Test: Total time: 0:00:03 (1.9600 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.6996 Acc: 0.7965 Recall_macro: 0.6996 Recall_weighted: 0.7965 AUC-ROC: 0.9406 Weighted F1-score: 0.7847
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 40, 'Val Loss': 0.6319226622581482, 'Val BAcc': np.float64(0.6996496320367288), 'Val Acc': 0.7965116279069767, 'Val ROC': np.float64(0.9405711290741143), 'Val W_F1': 0.7847114896327035, 'Val Recall_macro': 0.6996496320367288, 'Val Recall_weighted': 0.7965116279069767}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [41]  [ 0/11]  eta: 0:00:47  lr: 0.000061  min_lr: 0.000000  loss: 0.6079 (0.6079)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5968 (1.5968)  time: 4.3000  data: 3.4951  max mem: 39406
Epoch: [41]  [10/11]  eta: 0:00:01  lr: 0.000050  min_lr: 0.000000  loss: 1.1605 (1.0702)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8855 (1.8216)  time: 1.1291  data: 0.3178  max mem: 39406
Epoch: [41] Total time: 0:00:12 (1.1485 s / it)
2025-04-28 18:37:46 Averaged stats: lr: 0.000050  min_lr: 0.000000  loss: 1.1605 (1.0702)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8855 (1.8216)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.6089  data: 3.2239  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9589  data: 1.6120  max mem: 39406
Test: Total time: 0:00:04 (2.0572 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7081 Acc: 0.7907 Recall_macro: 0.7081 Recall_weighted: 0.7907 AUC-ROC: 0.9392 Weighted F1-score: 0.7817
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 41, 'Val Loss': 0.6290315389633179, 'Val BAcc': np.float64(0.7081239832207574), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9391709701816882), 'Val W_F1': 0.7817234803112265, 'Val Recall_macro': 0.7081239832207574, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [42]  [ 0/11]  eta: 0:00:46  lr: 0.000049  min_lr: 0.000000  loss: 0.9199 (0.9199)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7097 (1.7097)  time: 4.2400  data: 3.4396  max mem: 39406
Epoch: [42]  [10/11]  eta: 0:00:01  lr: 0.000039  min_lr: 0.000000  loss: 0.9759 (1.0206)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7270 (1.7942)  time: 1.1236  data: 0.3128  max mem: 39406
Epoch: [42] Total time: 0:00:12 (1.1467 s / it)
2025-04-28 18:38:02 Averaged stats: lr: 0.000039  min_lr: 0.000000  loss: 0.9759 (1.0206)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7270 (1.7942)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4083  data: 3.0191  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8584  data: 1.5096  max mem: 39406
Test: Total time: 0:00:03 (1.9670 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7193 Acc: 0.7994 Recall_macro: 0.7193 Recall_weighted: 0.7994 AUC-ROC: 0.9394 Weighted F1-score: 0.7917
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 42, 'Val Loss': 0.6275032162666321, 'Val BAcc': np.float64(0.7192592290656807), 'Val Acc': 0.7994186046511628, 'Val ROC': np.float64(0.9393533329943528), 'Val W_F1': 0.7917245561633094, 'Val Recall_macro': 0.7192592290656807, 'Val Recall_weighted': 0.7994186046511628}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [43]  [ 0/11]  eta: 0:00:45  lr: 0.000038  min_lr: 0.000000  loss: 1.1768 (1.1768)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6457 (1.6457)  time: 4.1443  data: 3.3436  max mem: 39406
Epoch: [43]  [10/11]  eta: 0:00:01  lr: 0.000029  min_lr: 0.000000  loss: 1.0993 (1.0680)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7685 (1.7277)  time: 1.1153  data: 0.3040  max mem: 39406
Epoch: [43] Total time: 0:00:12 (1.1353 s / it)
2025-04-28 18:38:19 Averaged stats: lr: 0.000029  min_lr: 0.000000  loss: 1.0993 (1.0680)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7685 (1.7277)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.5001  data: 3.1095  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9058  data: 1.5548  max mem: 39406
Test: Total time: 0:00:04 (2.0154 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7086 Acc: 0.8023 Recall_macro: 0.7086 Recall_weighted: 0.8023 AUC-ROC: 0.9401 Weighted F1-score: 0.7896
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 43, 'Val Loss': 0.6342954039573669, 'Val BAcc': np.float64(0.7086316983091177), 'Val Acc': 0.8023255813953488, 'Val ROC': np.float64(0.9401405337830591), 'Val W_F1': 0.7896187540562378, 'Val Recall_macro': 0.7086316983091177, 'Val Recall_weighted': 0.8023255813953488}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [44]  [ 0/11]  eta: 0:00:45  lr: 0.000028  min_lr: 0.000000  loss: 1.3108 (1.3108)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9754 (1.9754)  time: 4.1117  data: 3.3114  max mem: 39406
Epoch: [44]  [10/11]  eta: 0:00:01  lr: 0.000021  min_lr: 0.000000  loss: 1.1867 (1.0934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9754 (1.8097)  time: 1.1109  data: 0.3011  max mem: 39406
Epoch: [44] Total time: 0:00:12 (1.1323 s / it)
2025-04-28 18:38:35 Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.1867 (1.0934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9754 (1.8097)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4539  data: 3.0638  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8823  data: 1.5320  max mem: 39406
Test: Total time: 0:00:03 (1.9864 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7087 Acc: 0.7936 Recall_macro: 0.7087 Recall_weighted: 0.7936 AUC-ROC: 0.9397 Weighted F1-score: 0.7837
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 44, 'Val Loss': 0.6327357888221741, 'Val BAcc': np.float64(0.708674709061806), 'Val Acc': 0.7936046511627907, 'Val ROC': np.float64(0.9396790402412711), 'Val W_F1': 0.7836556733159449, 'Val Recall_macro': 0.708674709061806, 'Val Recall_weighted': 0.7936046511627907}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [45]  [ 0/11]  eta: 0:00:45  lr: 0.000020  min_lr: 0.000000  loss: 1.1470 (1.1470)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0175 (2.0175)  time: 4.1076  data: 3.3072  max mem: 39406
Epoch: [45]  [10/11]  eta: 0:00:01  lr: 0.000014  min_lr: 0.000000  loss: 1.1470 (1.0964)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9914 (1.8428)  time: 1.1119  data: 0.3007  max mem: 39406
Epoch: [45] Total time: 0:00:12 (1.1332 s / it)
2025-04-28 18:38:52 Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.1470 (1.0964)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9914 (1.8428)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:07    time: 3.6710  data: 3.2815  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9899  data: 1.6409  max mem: 39406
Test: Total time: 0:00:04 (2.1071 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7026 Acc: 0.7878 Recall_macro: 0.7026 Recall_weighted: 0.7878 AUC-ROC: 0.9398 Weighted F1-score: 0.7801
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 45, 'Val Loss': 0.6304678916931152, 'Val BAcc': np.float64(0.702616709713484), 'Val Acc': 0.7877906976744186, 'Val ROC': np.float64(0.9398346069918647), 'Val W_F1': 0.7800953601668545, 'Val Recall_macro': 0.702616709713484, 'Val Recall_weighted': 0.7877906976744186}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [46]  [ 0/11]  eta: 0:00:47  lr: 0.000013  min_lr: 0.000000  loss: 1.1724 (1.1724)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6740 (1.6740)  time: 4.3406  data: 3.5415  max mem: 39406
Epoch: [46]  [10/11]  eta: 0:00:01  lr: 0.000008  min_lr: 0.000000  loss: 0.9893 (1.0608)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4617 (1.6661)  time: 1.1330  data: 0.3221  max mem: 39406
Epoch: [46] Total time: 0:00:12 (1.1538 s / it)
2025-04-28 18:39:09 Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 0.9893 (1.0608)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4617 (1.6661)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4905  data: 3.1023  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8996  data: 1.5512  max mem: 39406
Test: Total time: 0:00:04 (2.0025 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7073 Acc: 0.7907 Recall_macro: 0.7073 Recall_weighted: 0.7907 AUC-ROC: 0.9389 Weighted F1-score: 0.7813
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 46, 'Val Loss': 0.6316371560096741, 'Val BAcc': np.float64(0.7073413757284727), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9388743385100375), 'Val W_F1': 0.7812940862914406, 'Val Recall_macro': 0.7073413757284727, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [47]  [ 0/11]  eta: 0:00:45  lr: 0.000008  min_lr: 0.000000  loss: 0.7636 (0.7636)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6650 (1.6650)  time: 4.1428  data: 3.3450  max mem: 39406
Epoch: [47]  [10/11]  eta: 0:00:01  lr: 0.000004  min_lr: 0.000000  loss: 0.8599 (0.9121)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6053 (1.6573)  time: 1.1130  data: 0.3042  max mem: 39406
Epoch: [47] Total time: 0:00:12 (1.1354 s / it)
2025-04-28 18:39:25 Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 0.8599 (0.9121)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6053 (1.6573)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4225  data: 3.0393  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8653  data: 1.5197  max mem: 39406
Test: Total time: 0:00:03 (1.9582 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7073 Acc: 0.7907 Recall_macro: 0.7073 Recall_weighted: 0.7907 AUC-ROC: 0.9386 Weighted F1-score: 0.7813
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 47, 'Val Loss': 0.6331915855407715, 'Val BAcc': np.float64(0.7073413757284727), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9385748915610238), 'Val W_F1': 0.7812940862914406, 'Val Recall_macro': 0.7073413757284727, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [48]  [ 0/11]  eta: 0:00:43  lr: 0.000004  min_lr: 0.000000  loss: 0.9848 (0.9848)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5119 (1.5119)  time: 3.9748  data: 3.1747  max mem: 39406
Epoch: [48]  [10/11]  eta: 0:00:01  lr: 0.000002  min_lr: 0.000000  loss: 1.1123 (1.0809)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5765 (1.6300)  time: 1.0978  data: 0.2887  max mem: 39406
Epoch: [48] Total time: 0:00:12 (1.1201 s / it)
2025-04-28 18:39:42 Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.1123 (1.0809)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5765 (1.6300)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4259  data: 3.0400  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.8678  data: 1.5201  max mem: 39406
Test: Total time: 0:00:03 (1.9618 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7073 Acc: 0.7907 Recall_macro: 0.7073 Recall_weighted: 0.7907 AUC-ROC: 0.9385 Weighted F1-score: 0.7813
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 48, 'Val Loss': 0.6338801980018616, 'Val BAcc': np.float64(0.7073413757284727), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9385153632338227), 'Val W_F1': 0.7812940862914406, 'Val Recall_macro': 0.7073413757284727, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Epoch: [49]  [ 0/11]  eta: 0:00:46  lr: 0.000002  min_lr: 0.000000  loss: 1.1490 (1.1490)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7729 (1.7729)  time: 4.2607  data: 3.4607  max mem: 39406
Epoch: [49]  [10/11]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000000  loss: 1.2677 (1.2278)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9940 (1.9583)  time: 1.1236  data: 0.3147  max mem: 39406
Epoch: [49] Total time: 0:00:12 (1.1452 s / it)
2025-04-28 18:39:58 Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.2677 (1.2278)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9940 (1.9583)
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/2]  eta: 0:00:06    time: 3.4936  data: 3.1051  max mem: 39406
Test:  [1/2]  eta: 0:00:01    time: 1.9013  data: 1.5526  max mem: 39406
Test: Total time: 0:00:04 (2.0005 s / it)
------------- val -------------
Sklearn Metrics - BAcc: 0.7073 Acc: 0.7907 Recall_macro: 0.7073 Recall_weighted: 0.7907 AUC-ROC: 0.9385 Weighted F1-score: 0.7813
Predictions for val saved to /home/share/FM_Code/PanDerm/PAD_Res/val.csv
-------------------------- {'Epoch': 49, 'Val Loss': 0.633878231048584, 'Val BAcc': np.float64(0.7073413757284727), 'Val Acc': 0.7906976744186046, 'Val ROC': np.float64(0.9385225622576153), 'Val W_F1': 0.7812940862914406, 'Val Recall_macro': 0.7073413757284727, 'Val Recall_weighted': 0.7906976744186046}
Max val mean accuracy: 0.81%
/home/share/FM_Code/PanDerm/classification/run_class_finetuning.py:724: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_dict = torch.load(model_weight)
Starting test without tta
/home/share/FM_Code/PanDerm/classification/furnace/engine_for_finetuning.py:280: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Test:  [0/4]  eta: 0:00:11    time: 2.9156  data: 2.6660  max mem: 39406
Test:  [3/4]  eta: 0:00:00    time: 0.9641  data: 0.6666  max mem: 39406
Test: Total time: 0:00:04 (1.0335 s / it)
------------- test -------------
Sklearn Metrics - BAcc: 0.7397 Acc: 0.7874 Recall_macro: 0.7397 Recall_weighted: 0.7874 AUC-ROC: 0.9564 Weighted F1-score: 0.7816
{'balanced_accuracy': np.float64(0.7397185216183075), 'accuracy': 0.7874186550976139, 'top3 accuracy': np.float64(0.9783080260303688), 'top5 accuracy': np.float64(0.9978308026030369), 'sensitivity': np.float64(0.7397185216183075), 'specificity': np.float64(0.9488856365949797), 'auc_roc': np.float64(0.9563607993296396), 'weighted_f1': 0.7816121182937246, 'recall_macro': 0.7397185216183075, 'recall_weighted': 0.7874186550976139}
Predictions for test saved to /home/share/FM_Code/PanDerm/PAD_Res/test.csv
Training time 0:15:01
